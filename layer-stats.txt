Log of miniGPT forward pass:
	Random seed: 3620553521
	Time elapsed: 22.789s (22.713s spent logging)
	Structure:
		- recurrent (at most 2 loops)
		- with branching
		- no conditional (if-then) branching
		- contains 48 buffer layers
		- 317 total modules
	Tensor info:
		- 698 total tensors (14.4 MB) computed in forward pass.
		- 698 tensors (14.4 MB) with saved activations.
	Parameters: 178 parameter operations (10788929 params total; 41.2 MB)


Node details:
--------------------------------------------
Layer input_1, operation 0/648:
	Output tensor: shape=1x11(88 bytes), dype=torch.int64, Tensor size=216 bytes, Tensor storage size=0
		tensor([[42]])...
	Related Layers:
		- no parent layers
		- child layers: embedding_1_1
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Output of modules: none
	Lookup keys: -698, 0, input.idx, input_1, input_1, input_1:1, input_1:1
--------------------------------------------
Layer embedding_1_1, operation 1/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-0.5682,  0.6073, -0.4393, -0.6129,  0.2118,  1.9125,  0.1985,  1.2209],
        [ 0.4752, -1.3044,  0.2877,  2.3898, -1.2345, -1.1072, -1.3594, -1.7027],
        [-0.7326, -1.2240, -0.2339,  0.4898, -0.0960, -0.0279,  0.7931,  0.1592],
        [-0.7326, -1.2240, -0.2339,  0.4898, -0.0960, -0.0279,  0.7931,  0.1592],
        [ 1.0141,  0.4149, -0.5647, -1.0683,  2.1059, -0.0822,  0.8860,  0.1120],
        [ 1.2421,  0.9075,  0.1316,  1.4356, -0.0212,  0.3730, -1.3040, -0.7241],
        [-0.9557, -0.0117,  1.4950,  0.5488, -0.0374,  2.3920, -0.6318,  0.0792],
        [-0.8917,  1.7229,  0.5939, -0.6319,  0.0240, -0.0343, -1.6418, -1.0043]])...
	Related Layers:
		- parent layers: input_1
		- child layers: add_1_4
		- shares parents with no other layers
		- shares children with layers: embedding_2_3
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (65, 384); 24960 params total (97.6 KB)
	Function: embedding (grad_fn: EmbeddingBackward0) 
	Computed inside module: token_embedding_table:1
	Time elapsed:  2.251E-04s
	Output of modules: token_embedding_table
	Output of bottom-level module: token_embedding_table:1
	Lookup keys: -697, 1, embedding_1, embedding_1:1, embedding_1_1, embedding_1_1:1, token_embedding_table, token_embedding_table:1
--------------------------------------------
Layer arange_1_2, operation 2/648:
	Output tensor: shape=11(88 bytes), dype=torch.int64, Tensor size=200 bytes, Tensor storage size=152
		tensor([0, 1, 2, 3, 4, 5, 6, 7])...
	Related Layers:
		- no parent layers
		- child layers: embedding_2_3
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: arange (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  1.965E-04s
	Output of modules: none
	Lookup keys: -696, 2, arange_1, arange_1:1, arange_1_2, arange_1_2:1
--------------------------------------------
Layer embedding_2_3, operation 3/648:
	Output tensor: shape=11x384(16896 bytes), dype=torch.float32, Tensor size=17024 bytes, Tensor storage size=16960
		tensor([[ 0.3007, -0.1916,  0.2411,  0.9030, -1.2097,  0.0306,  1.2770, -0.2606],
        [-0.2395, -0.5329,  0.9710, -0.5946, -0.6381,  0.6068,  1.5018,  0.4344],
        [ 0.7109,  0.2245, -1.0280,  0.7702, -0.4917,  0.6438, -0.8439, -1.3233],
        [ 1.1891,  0.8964,  0.7996, -0.3278,  0.2265,  0.8040,  0.1987,  0.1316],
        [ 1.0299, -0.0204, -0.4053, -0.1829,  0.0104, -1.4258,  0.0661,  0.7017],
        [-0.1331,  0.7235, -2.1202, -0.8567, -1.2131,  0.1519, -0.7219,  1.2688],
        [-0.3180, -0.1523, -0.7307, -1.1994, -0.1217,  0.7153, -0.2349, -1.3885],
        [-1.6191, -0.4535, -0.1166, -0.5731, -1.2998,  0.0504, -2.3936, -0.1405]])...
	Related Layers:
		- parent layers: arange_1_2
		- child layers: add_1_4
		- shares parents with no other layers
		- shares children with layers: embedding_1_1
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: Computed from params with shape (256, 384); 98304 params total (384.1 KB)
	Function: embedding (grad_fn: EmbeddingBackward0) 
	Computed inside module: position_embedding_table:1
	Time elapsed:  7.558E-05s
	Output of modules: position_embedding_table
	Output of bottom-level module: position_embedding_table:1
	Lookup keys: -695, 3, embedding_2, embedding_2:1, embedding_2_3, embedding_2_3:1, position_embedding_table, position_embedding_table:1
--------------------------------------------
Layer add_1_4, operation 4/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-0.2675,  0.4157, -0.1982,  0.2901, -0.9979,  1.9431,  1.4755,  0.9603],
        [ 0.2356, -1.8373,  1.2587,  1.7951, -1.8726, -0.5004,  0.1424, -1.2683],
        [-0.0217, -0.9995, -1.2618,  1.2600, -0.5877,  0.6159, -0.0508, -1.1641],
        [ 0.4565, -0.3276,  0.5657,  0.1620,  0.1305,  0.7760,  0.9918,  0.2908],
        [ 2.0440,  0.3945, -0.9700, -1.2512,  2.1163, -1.5079,  0.9520,  0.8137],
        [ 1.1090,  1.6310, -1.9885,  0.5790, -1.2342,  0.5249, -2.0259,  0.5447],
        [-1.2737, -0.1640,  0.7643, -0.6506, -0.1591,  3.1073, -0.8666, -1.3094],
        [-2.5108,  1.2695,  0.4773, -1.2050, -1.2758,  0.0161, -4.0355, -1.1448]])...
	Related Layers:
		- parent layers: embedding_1_1, embedding_2_3
		- child layers: layernorm_1_5, add_2_105:1
		- shares parents with no other layers
		- shares children with layers: dropout_9_104
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __add__ (grad_fn: AddBackward0) 
	Computed inside module: not computed inside a module
	Time elapsed:  2.270E-04s
	Output of modules: none
	Lookup keys: -694, 4, add_1, add_1:1, add_1_4, add_1_4:1
--------------------------------------------
Layer layernorm_1_5, operation 5/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-0.2762,  0.1879, -0.2304,  0.0917, -0.7431,  1.2254,  0.9269,  0.5653],
        [ 0.1715, -1.2898,  0.8812,  1.2458, -1.2734, -0.3866,  0.1156, -0.8545],
        [ 0.0301, -0.6763, -0.8636,  0.9287, -0.3481,  0.4555,  0.0223, -0.7584],
        [ 0.3119, -0.2300,  0.3752,  0.0891,  0.1095,  0.4929,  0.6727,  0.1987],
        [ 1.5488,  0.3395, -0.6293, -0.8450,  1.5841, -1.0528,  0.7505,  0.6399],
        [ 0.8591,  1.2239, -1.4796,  0.4307, -0.8758,  0.3779, -1.4669,  0.4210],
        [-0.8850, -0.0934,  0.5682, -0.4503, -0.0572,  2.2242, -0.5615, -0.8652],
        [-1.7253,  0.8270,  0.3013, -0.8440, -0.8381, -0.0382, -2.6725, -0.7534]])...
	Related Layers:
		- parent layers: add_1_4
		- child layers: linear_1_6, linear_2_7, linear_3_16, linear_4_18, linear_5_19, linear_6_28, linear_7_30, linear_8_31, linear_9_40, linear_10_42, linear_11_43, linear_12_52, linear_13_54, linear_14_55, linear_15_64, linear_16_66, linear_17_67, linear_18_76, linear_19_78, linear_20_79, linear_21_88, linear_22_90, linear_23_91, linear_24_100
		- shares parents with layers: add_2_105:1
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (384,), (384,); 768 params total (3.2 KB)
	Function: layer_norm (grad_fn: NativeLayerNormBackward0) 
	Computed inside module: blocks.0.ln1:1
	Time elapsed:  2.121E-03s
	Output of modules: blocks.0.ln1
	Output of bottom-level module: blocks.0.ln1:1
	Lookup keys: -693, 5, blocks.0.ln1, blocks.0.ln1:1, layernorm_1, layernorm_1:1, layernorm_1_5, layernorm_1_5:1
--------------------------------------------
Layer linear_1_6, operation 6/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.0592, -0.2643, -0.4999, -2.5263, -2.2876,  1.5732,  2.0375,  0.1636],
        [-1.1963, -0.2808,  0.3116, -1.6370, -1.1617,  0.3508,  0.8795,  1.1641],
        [ 0.6627,  0.2638,  1.6938,  0.5048, -0.8589,  1.7712,  0.1537,  0.2849],
        [ 1.1928, -1.1809,  0.9211,  0.7787,  0.2857,  0.1211,  0.7069,  0.7513],
        [ 1.2835,  0.6706, -1.5994,  0.1226,  1.1441, -1.5586, -1.4937, -0.4194],
        [-3.9176, -3.7041, -2.7502, -2.1240, -1.3696,  2.4169,  1.1582, -0.2389],
        [-1.4371,  0.0065, -0.8166,  0.2175, -0.4559,  2.4779,  0.6830, -2.5782],
        [-1.1387,  0.7114, -0.2380,  0.5000, -2.4940,  1.7858, -0.3477, -2.4105]])...
	Related Layers:
		- parent layers: layernorm_1_5
		- child layers: transpose_1_8
		- shares parents with layers: linear_2_7, linear_3_16, linear_4_18, linear_5_19, linear_6_28, linear_7_30, linear_8_31, linear_9_40, linear_10_42, linear_11_43, linear_12_52, linear_13_54, linear_14_55, linear_15_64, linear_16_66, linear_17_67, linear_18_76, linear_19_78, linear_20_79, linear_21_88, linear_22_90, linear_23_91, linear_24_100
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.0.key:1
	Time elapsed:  2.397E-03s
	Output of modules: blocks.0.sa.heads.0.key
	Output of bottom-level module: blocks.0.sa.heads.0.key:1
	Lookup keys: -692, 6, blocks.0.sa.heads.0.key, blocks.0.sa.heads.0.key:1, linear_1, linear_1:1, linear_1_6, linear_1_6:1
--------------------------------------------
Layer linear_2_7, operation 7/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.3383,  0.3953,  0.1926, -1.0558, -0.3057,  0.4732, -0.4212, -0.0041],
        [ 1.1393, -0.3857,  0.7504,  1.0370,  0.7473, -0.4326,  0.7625,  2.3072],
        [ 0.7583, -0.0225,  1.4747, -0.3221,  0.8399, -1.1496, -1.6560, -0.4024],
        [ 0.6659,  0.2801,  2.2827,  0.2842,  1.5116, -0.1577, -1.9884, -0.6797],
        [-0.3333,  0.6803,  0.7824,  1.9521,  0.9257,  1.0973, -2.2145, -0.9388],
        [ 1.0930,  0.3807, -0.7288,  0.2670, -0.0778, -1.6064, -2.1795, -1.3355],
        [ 2.4429, -0.0054,  2.6454,  1.1072,  1.3872, -1.5269, -1.0462, -0.1629],
        [ 0.1726, -1.2678, -1.0167,  0.8438,  1.5523, -1.2087, -2.3912, -1.9072]])...
	Related Layers:
		- parent layers: layernorm_1_5
		- child layers: matmul_1_9
		- shares parents with layers: linear_1_6, linear_3_16, linear_4_18, linear_5_19, linear_6_28, linear_7_30, linear_8_31, linear_9_40, linear_10_42, linear_11_43, linear_12_52, linear_13_54, linear_14_55, linear_15_64, linear_16_66, linear_17_67, linear_18_76, linear_19_78, linear_20_79, linear_21_88, linear_22_90, linear_23_91, linear_24_100
		- shares children with layers: transpose_1_8
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.0.query:1
	Time elapsed:  1.049E-04s
	Output of modules: blocks.0.sa.heads.0.query
	Output of bottom-level module: blocks.0.sa.heads.0.query:1
	Lookup keys: -691, 7, blocks.0.sa.heads.0.query, blocks.0.sa.heads.0.query:1, linear_2, linear_2:1, linear_2_7, linear_2_7:1
--------------------------------------------
Layer transpose_1_8, operation 8/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.0592, -1.1963,  0.6627,  1.1928,  1.2835, -3.9176, -1.4371, -1.1387],
        [-0.2643, -0.2808,  0.2638, -1.1809,  0.6706, -3.7041,  0.0065,  0.7114],
        [-0.4999,  0.3116,  1.6938,  0.9211, -1.5994, -2.7502, -0.8166, -0.2380],
        [-2.5263, -1.6370,  0.5048,  0.7787,  0.1226, -2.1240,  0.2175,  0.5000],
        [-2.2876, -1.1617, -0.8589,  0.2857,  1.1441, -1.3696, -0.4559, -2.4940],
        [ 1.5732,  0.3508,  1.7712,  0.1211, -1.5586,  2.4169,  2.4779,  1.7858],
        [ 2.0375,  0.8795,  0.1537,  0.7069, -1.4937,  1.1582,  0.6830, -0.3477],
        [ 0.1636,  1.1641,  0.2849,  0.7513, -0.4194, -0.2389, -2.5782, -2.4105]])...
	Related Layers:
		- parent layers: linear_1_6
		- child layers: matmul_1_9
		- shares parents with no other layers
		- shares children with layers: linear_2_7
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.0.sa.heads.0:1
	Time elapsed:  6.509E-05s
	Output of modules: none
	Lookup keys: -690, 8, transpose_1, transpose_1:1, transpose_1_8, transpose_1_8:1
--------------------------------------------
Layer matmul_1_9, operation 9/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ 15.5477,  21.4082,  16.8673,  10.6187, -13.5087,  14.3695,   4.8481,
         -20.6077],
        [-14.8328,   7.3664,  -3.4618,   9.7869,   4.5173, -15.5804, -33.1131,
         -27.2701],
        [-38.0965, -13.2179, -10.1693,  -5.0810,   6.7253, -31.7674, -14.6013,
           7.1550],
        [-35.7524, -26.7474,   2.8388,  -7.9067,   9.4304, -39.4280,  -1.4555,
          15.8903],
        [-38.7009, -27.2612,   0.5712,  -7.0700,   3.9914, -36.7333,  10.3137,
          16.1914],
        [-34.5520, -30.9612, -30.5221, -17.9972,  34.0487, -36.1808, -16.4794,
          10.9569],
        [-44.7819, -10.7485, -27.1267,  -2.1807,  20.4502, -49.9321, -43.9717,
          -1.3077],
        [-63.3000, -22.2892, -26.5376, -11.1323,  11.8339, -28.5582, -14.5107,
           1.7180]])...
	Related Layers:
		- parent layers: linear_2_7, transpose_1_8
		- child layers: mul_1_10
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.0:1
	Time elapsed:  2.425E-04s
	Output of modules: none
	Lookup keys: -689, 9, matmul_1, matmul_1:1, matmul_1_9, matmul_1_9:1
--------------------------------------------
Layer mul_1_10, operation 10/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ 0.7934,  1.0925,  0.8608,  0.5419, -0.6894,  0.7333,  0.2474, -1.0516],
        [-0.7569,  0.3759, -0.1767,  0.4994,  0.2305, -0.7951, -1.6898, -1.3916],
        [-1.9441, -0.6745, -0.5190, -0.2593,  0.3432, -1.6211, -0.7451,  0.3651],
        [-1.8245, -1.3649,  0.1449, -0.4035,  0.4812, -2.0121, -0.0743,  0.8109],
        [-1.9749, -1.3912,  0.0291, -0.3608,  0.2037, -1.8745,  0.5263,  0.8263],
        [-1.7632, -1.5800, -1.5576, -0.9184,  1.7375, -1.8463, -0.8410,  0.5591],
        [-2.2853, -0.5485, -1.3843, -0.1113,  1.0436, -2.5481, -2.2439, -0.0667],
        [-3.2303, -1.1374, -1.3542, -0.5681,  0.6039, -1.4574, -0.7405,  0.0877]])...
	Related Layers:
		- parent layers: matmul_1_9
		- child layers: maskedfill_1_13
		- shares parents with no other layers
		- shares children with layers: eq_1_12
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.0.sa.heads.0:1
	Time elapsed:  1.564E-04s
	Output of modules: none
	Lookup keys: -688, 10, mul_1, mul_1:1, mul_1_10, mul_1_10:1
--------------------------------------------
Layer getitem_1_11, operation 11/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_1
		- child layers: eq_1_12
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  1.476E-04s
	Output of modules: none
	Lookup keys: -686, 12, getitem_1, getitem_1:1, getitem_1_11, getitem_1_11:1
--------------------------------------------
Layer eq_1_12, operation 12/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_1_11
		- child layers: maskedfill_1_13
		- shares parents with no other layers
		- shares children with layers: mul_1_10
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  1.428E-04s
	Output of modules: none
	Lookup keys: -685, 13, eq_1, eq_1:1, eq_1_12, eq_1_12:1
--------------------------------------------
Layer maskedfill_1_13, operation 13/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ 0.7934,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-0.7569,  0.3759,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-1.9441, -0.6745, -0.5190,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-1.8245, -1.3649,  0.1449, -0.4035,    -inf,    -inf,    -inf,    -inf],
        [-1.9749, -1.3912,  0.0291, -0.3608,  0.2037,    -inf,    -inf,    -inf],
        [-1.7632, -1.5800, -1.5576, -0.9184,  1.7375, -1.8463,    -inf,    -inf],
        [-2.2853, -0.5485, -1.3843, -0.1113,  1.0436, -2.5481, -2.2439,    -inf],
        [-3.2303, -1.1374, -1.3542, -0.5681,  0.6039, -1.4574, -0.7405,  0.0877]])...
	Related Layers:
		- parent layers: mul_1_10, eq_1_12
		- child layers: softmax_1_14
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.0.sa.heads.0:1
	Time elapsed:  1.888E-04s
	Output of modules: none
	Lookup keys: -684, 14, maskedfill_1, maskedfill_1:1, maskedfill_1_13, maskedfill_1_13:1
--------------------------------------------
Layer softmax_1_14, operation 14/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2436, 0.7564, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1147, 0.4083, 0.4770, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0720, 0.1140, 0.5159, 0.2981, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0415, 0.0745, 0.3082, 0.2087, 0.3670, 0.0000, 0.0000, 0.0000],
        [0.0251, 0.0302, 0.0308, 0.0585, 0.8323, 0.0231, 0.0000, 0.0000],
        [0.0210, 0.1192, 0.0517, 0.1845, 0.5856, 0.0161, 0.0219, 0.0000],
        [0.0082, 0.0666, 0.0536, 0.1177, 0.3799, 0.0484, 0.0990, 0.2267]])...
	Related Layers:
		- parent layers: maskedfill_1_13
		- child layers: dropout_1_15
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.0.sa.heads.0:1
	Time elapsed:  1.557E-04s
	Output of modules: none
	Lookup keys: -683, 15, softmax_1, softmax_1:1, softmax_1_14, softmax_1_14:1
--------------------------------------------
Layer dropout_1_15, operation 15/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2436, 0.7564, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1147, 0.4083, 0.4770, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0720, 0.1140, 0.5159, 0.2981, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0415, 0.0745, 0.3082, 0.2087, 0.3670, 0.0000, 0.0000, 0.0000],
        [0.0251, 0.0302, 0.0308, 0.0585, 0.8323, 0.0231, 0.0000, 0.0000],
        [0.0210, 0.1192, 0.0517, 0.1845, 0.5856, 0.0161, 0.0219, 0.0000],
        [0.0082, 0.0666, 0.0536, 0.1177, 0.3799, 0.0484, 0.0990, 0.2267]])...
	Related Layers:
		- parent layers: softmax_1_14
		- child layers: matmul_2_17
		- shares parents with no other layers
		- shares children with layers: linear_3_16
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.0.sa.heads.0.dropout:1
	Time elapsed:  1.194E-04s
	Output of modules: blocks.0.sa.heads.0.dropout
	Output of bottom-level module: blocks.0.sa.heads.0.dropout:1
	Lookup keys: -682, 16, blocks.0.sa.heads.0.dropout, blocks.0.sa.heads.0.dropout:1, dropout_1, dropout_1:1, dropout_1_15, dropout_1_15:1
--------------------------------------------
Layer linear_3_16, operation 16/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-7.4854e-01,  1.7668e+00,  2.7114e+00, -3.2724e-01, -4.4953e-01,
         -1.4062e+00, -5.2633e-01, -2.0887e-01],
        [ 3.0568e-01,  2.1512e-01,  1.5741e+00,  6.0972e-01, -5.9942e-01,
         -1.5246e+00,  1.0707e-01,  1.1069e+00],
        [-5.2264e-01,  4.4750e-01,  5.7607e-01, -1.0766e+00, -9.5259e-01,
         -1.9292e-01, -8.8370e-01,  8.1518e-02],
        [-5.4254e-01,  7.0452e-01,  1.9017e+00,  9.2078e-01, -1.2988e+00,
         -2.0633e-03, -6.2786e-01, -9.6862e-01],
        [-7.7352e-01,  6.5274e-01,  1.6357e+00, -2.7293e-02, -2.9817e-01,
         -6.5322e-02, -4.0153e-01,  2.2258e-01],
        [-1.0334e+00,  4.5721e-01,  4.1852e-01,  8.3570e-01, -1.6004e+00,
         -7.2439e-02, -1.5702e-02,  4.9277e-01],
        [-1.5659e-01,  1.5264e-01,  9.0247e-01, -2.2404e-01, -8.0376e-01,
         -1.0650e-01,  1.5713e+00, -4.2731e-01],
        [-6.8074e-01, -1.1391e-02,  2.4477e-01, -3.4755e-01, -1.2062e+00,
          6.2059e-01,  1.4187e-01, -5.8647e-01]])...
	Related Layers:
		- parent layers: layernorm_1_5
		- child layers: matmul_2_17
		- shares parents with layers: linear_1_6, linear_2_7, linear_4_18, linear_5_19, linear_6_28, linear_7_30, linear_8_31, linear_9_40, linear_10_42, linear_11_43, linear_12_52, linear_13_54, linear_14_55, linear_15_64, linear_16_66, linear_17_67, linear_18_76, linear_19_78, linear_20_79, linear_21_88, linear_22_90, linear_23_91, linear_24_100
		- shares children with layers: dropout_1_15
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.0.value:1
	Time elapsed:  1.135E-04s
	Output of modules: blocks.0.sa.heads.0.value
	Output of bottom-level module: blocks.0.sa.heads.0.value:1
	Lookup keys: -681, 17, blocks.0.sa.heads.0.value, blocks.0.sa.heads.0.value:1, linear_3, linear_3:1, linear_3_16, linear_3_16:1
--------------------------------------------
Layer matmul_2_17, operation 17/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.7485,  1.7668,  2.7114, -0.3272, -0.4495, -1.4062, -0.5263, -0.2089],
        [ 0.0488,  0.5932,  1.8512,  0.3814, -0.5629, -1.4958, -0.0473,  0.7863],
        [-0.2104,  0.5040,  1.2285, -0.3021, -0.7507, -0.8758, -0.4382,  0.4668],
        [-0.4504,  0.5926,  1.2388, -0.2349, -0.9793, -0.3752, -0.6688, -0.1356],
        [-0.5666,  0.6140,  1.4047, -0.1179, -0.7375, -0.2558, -0.5647, -0.0216],
        [-0.7251,  0.6597,  1.6156,  0.0274, -0.4198, -0.1434, -0.4085,  0.1707],
        [-0.5795,  0.6088,  1.6096,  0.1727, -0.5878, -0.2633, -0.3608,  0.0820],
        [-0.5913,  0.4183,  1.1682,  0.0176, -0.7912, -0.0218, -0.0840, -0.1045]])...
	Related Layers:
		- parent layers: dropout_1_15, linear_3_16
		- child layers: cat_1_102
		- shares parents with no other layers
		- shares children with layers: matmul_4_29, matmul_6_41, matmul_8_53, matmul_10_65, matmul_12_77, matmul_14_89, matmul_16_101
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.0:1
	Time elapsed:  1.154E-04s
	Output of modules: blocks.0.sa.heads.0
	Lookup keys: -680, 18, blocks.0.sa.heads.0, blocks.0.sa.heads.0:1, matmul_2, matmul_2:1, matmul_2_17, matmul_2_17:1
--------------------------------------------
Layer linear_4_18, operation 18/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-3.0121,  2.7930, -0.1471, -0.6426, -0.0166, -1.9548,  0.4633,  0.6794],
        [ 0.6033,  0.3664,  0.9575,  0.5402, -0.8614, -1.5979, -1.4298,  0.9843],
        [-1.4888,  1.4524, -1.0331,  0.1524,  2.4763, -1.9010,  0.4223, -1.0930],
        [-0.9677,  2.7373, -0.2799,  0.7853,  1.3985, -1.2424,  0.4376, -1.1569],
        [-3.8473,  0.5740, -1.5947, -0.5214,  0.7990, -2.0734, -1.1843, -1.5015],
        [ 0.2527,  0.8557, -1.5435,  2.0430, -0.2718,  2.4741,  0.0604, -1.3930],
        [ 0.8523,  1.0509,  1.8382,  1.3721,  0.4307,  2.1791,  1.4940,  0.5465],
        [-0.0715, -0.5768, -1.6107, -0.3135,  0.1726,  0.2745,  1.4840, -1.0995]])...
	Related Layers:
		- parent layers: layernorm_1_5
		- child layers: transpose_2_20
		- shares parents with layers: linear_1_6, linear_2_7, linear_3_16, linear_5_19, linear_6_28, linear_7_30, linear_8_31, linear_9_40, linear_10_42, linear_11_43, linear_12_52, linear_13_54, linear_14_55, linear_15_64, linear_16_66, linear_17_67, linear_18_76, linear_19_78, linear_20_79, linear_21_88, linear_22_90, linear_23_91, linear_24_100
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.1.key:1
	Time elapsed:  1.225E-04s
	Output of modules: blocks.0.sa.heads.1.key
	Output of bottom-level module: blocks.0.sa.heads.1.key:1
	Lookup keys: -679, 19, blocks.0.sa.heads.1.key, blocks.0.sa.heads.1.key:1, linear_4, linear_4:1, linear_4_18, linear_4_18:1
--------------------------------------------
Layer linear_5_19, operation 19/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.0120, -1.0498,  0.2061, -0.2391, -1.4873, -0.7215,  0.2445, -0.0547],
        [-0.1359, -0.7281,  0.4124, -0.1348,  0.5677,  1.7322,  1.2829, -0.1028],
        [ 1.4544, -0.5691,  2.6821,  0.4019, -0.5033,  0.5378, -1.4170,  0.0657],
        [ 2.3267, -1.0402,  2.1407, -0.2695,  0.8676,  1.9633,  0.5447,  0.9522],
        [-0.0092,  1.3960,  1.3730,  0.6387,  1.0016, -0.3942, -0.4826, -0.2793],
        [-0.9705, -1.5920,  0.6924,  1.2673,  1.2817,  1.0234,  0.5210,  1.1265],
        [-1.2808, -0.0292, -0.7092,  1.4191,  1.2332,  1.2946, -1.0822, -1.4677],
        [ 0.5112,  1.4264,  2.2781,  1.5743, -0.1529,  2.3817, -0.8705,  1.3610]])...
	Related Layers:
		- parent layers: layernorm_1_5
		- child layers: matmul_3_21
		- shares parents with layers: linear_1_6, linear_2_7, linear_3_16, linear_4_18, linear_6_28, linear_7_30, linear_8_31, linear_9_40, linear_10_42, linear_11_43, linear_12_52, linear_13_54, linear_14_55, linear_15_64, linear_16_66, linear_17_67, linear_18_76, linear_19_78, linear_20_79, linear_21_88, linear_22_90, linear_23_91, linear_24_100
		- shares children with layers: transpose_2_20
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.1.query:1
	Time elapsed:  1.163E-04s
	Output of modules: blocks.0.sa.heads.1.query
	Output of bottom-level module: blocks.0.sa.heads.1.query:1
	Lookup keys: -678, 20, blocks.0.sa.heads.1.query, blocks.0.sa.heads.1.query:1, linear_5, linear_5:1, linear_5_19, linear_5_19:1
--------------------------------------------
Layer transpose_2_20, operation 20/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-3.0121,  0.6033, -1.4888, -0.9677, -3.8473,  0.2527,  0.8523, -0.0715],
        [ 2.7930,  0.3664,  1.4524,  2.7373,  0.5740,  0.8557,  1.0509, -0.5768],
        [-0.1471,  0.9575, -1.0331, -0.2799, -1.5947, -1.5435,  1.8382, -1.6107],
        [-0.6426,  0.5402,  0.1524,  0.7853, -0.5214,  2.0430,  1.3721, -0.3135],
        [-0.0166, -0.8614,  2.4763,  1.3985,  0.7990, -0.2718,  0.4307,  0.1726],
        [-1.9548, -1.5979, -1.9010, -1.2424, -2.0734,  2.4741,  2.1791,  0.2745],
        [ 0.4633, -1.4298,  0.4223,  0.4376, -1.1843,  0.0604,  1.4940,  1.4840],
        [ 0.6794,  0.9843, -1.0930, -1.1569, -1.5015, -1.3930,  0.5465, -1.0995]])...
	Related Layers:
		- parent layers: linear_4_18
		- child layers: matmul_3_21
		- shares parents with no other layers
		- shares children with layers: linear_5_19
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.0.sa.heads.1:1
	Time elapsed:  6.104E-05s
	Output of modules: none
	Lookup keys: -677, 21, transpose_2, transpose_2:1, transpose_2_20, transpose_2_20:1
--------------------------------------------
Layer matmul_3_21, operation 21/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-20.1665,  -8.2682, -25.1718, -22.8222, -18.2995, -13.4541, -12.8340,
           3.7783],
        [ 30.8340,  21.7667,  -4.7702,  -8.9392,  11.6380,  12.9462,   9.1247,
          -4.3887],
        [-32.3807,   4.6842, -19.5728, -10.5456, -21.0025,  -5.1899,  17.3743,
         -18.8229],
        [-58.6156, -11.2196, -12.2116, -17.3452, -36.2652,  -6.8115,  31.9498,
         -13.9121],
        [-27.5001, -24.1467,  11.8637,  34.5666, -19.5213, -28.3958,   0.5574,
         -29.4492],
        [ -1.7448,  -0.1839,  -2.3812,   0.6145,  16.3657, -12.2130,  14.8540,
          -4.0861],
        [  4.2267,   5.3532,  -8.4424,  -4.6112,  15.1281,  17.2641,  -3.1558,
           0.5235],
        [-44.3093,  -6.0217, -32.5287, -24.6265, -40.8029,  -9.7724,  30.9777,
         -31.3865]])...
	Related Layers:
		- parent layers: linear_5_19, transpose_2_20
		- child layers: mul_2_22
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.1:1
	Time elapsed:  9.298E-05s
	Output of modules: none
	Lookup keys: -676, 22, matmul_3, matmul_3:1, matmul_3_21, matmul_3_21:1
--------------------------------------------
Layer mul_2_22, operation 22/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-1.0291, -0.4219, -1.2845, -1.1646, -0.9338, -0.6866, -0.6549,  0.1928],
        [ 1.5735,  1.1108, -0.2434, -0.4562,  0.5939,  0.6607,  0.4656, -0.2240],
        [-1.6524,  0.2390, -0.9988, -0.5382, -1.0718, -0.2648,  0.8866, -0.9606],
        [-2.9912, -0.5725, -0.6232, -0.8851, -1.8507, -0.3476,  1.6304, -0.7099],
        [-1.4034, -1.2322,  0.6054,  1.7640, -0.9962, -1.4491,  0.0284, -1.5028],
        [-0.0890, -0.0094, -0.1215,  0.0314,  0.8352, -0.6232,  0.7580, -0.2085],
        [ 0.2157,  0.2732, -0.4308, -0.2353,  0.7720,  0.8810, -0.1610,  0.0267],
        [-2.2612, -0.3073, -1.6600, -1.2567, -2.0822, -0.4987,  1.5808, -1.6017]])...
	Related Layers:
		- parent layers: matmul_3_21
		- child layers: maskedfill_2_25
		- shares parents with no other layers
		- shares children with layers: eq_2_24
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.0.sa.heads.1:1
	Time elapsed:  7.534E-05s
	Output of modules: none
	Lookup keys: -675, 23, mul_2, mul_2:1, mul_2_22, mul_2_22:1
--------------------------------------------
Layer getitem_2_23, operation 23/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_2
		- child layers: eq_2_24
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  1.271E-04s
	Output of modules: none
	Lookup keys: -673, 25, getitem_2, getitem_2:1, getitem_2_23, getitem_2_23:1
--------------------------------------------
Layer eq_2_24, operation 24/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_2_23
		- child layers: maskedfill_2_25
		- shares parents with no other layers
		- shares children with layers: mul_2_22
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.345E-05s
	Output of modules: none
	Lookup keys: -672, 26, eq_2, eq_2:1, eq_2_24, eq_2_24:1
--------------------------------------------
Layer maskedfill_2_25, operation 25/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-1.0291,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [ 1.5735,  1.1108,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-1.6524,  0.2390, -0.9988,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-2.9912, -0.5725, -0.6232, -0.8851,    -inf,    -inf,    -inf,    -inf],
        [-1.4034, -1.2322,  0.6054,  1.7640, -0.9962,    -inf,    -inf,    -inf],
        [-0.0890, -0.0094, -0.1215,  0.0314,  0.8352, -0.6232,    -inf,    -inf],
        [ 0.2157,  0.2732, -0.4308, -0.2353,  0.7720,  0.8810, -0.1610,    -inf],
        [-2.2612, -0.3073, -1.6600, -1.2567, -2.0822, -0.4987,  1.5808, -1.6017]])...
	Related Layers:
		- parent layers: mul_2_22, eq_2_24
		- child layers: softmax_2_26
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.0.sa.heads.1:1
	Time elapsed:  8.535E-05s
	Output of modules: none
	Lookup keys: -671, 27, maskedfill_2, maskedfill_2:1, maskedfill_2_25, maskedfill_2_25:1
--------------------------------------------
Layer softmax_2_26, operation 26/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.6137, 0.3863, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1047, 0.6940, 0.2013, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0321, 0.3609, 0.3430, 0.2640, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0287, 0.0340, 0.2137, 0.6806, 0.0431, 0.0000, 0.0000, 0.0000],
        [0.1373, 0.1487, 0.1329, 0.1548, 0.3459, 0.0805, 0.0000, 0.0000],
        [0.1317, 0.1394, 0.0690, 0.0839, 0.2296, 0.2561, 0.0903, 0.0000],
        [0.0147, 0.1035, 0.0268, 0.0400, 0.0175, 0.0855, 0.6837, 0.0284]])...
	Related Layers:
		- parent layers: maskedfill_2_25
		- child layers: dropout_2_27
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.0.sa.heads.1:1
	Time elapsed:  7.486E-05s
	Output of modules: none
	Lookup keys: -670, 28, softmax_2, softmax_2:1, softmax_2_26, softmax_2_26:1
--------------------------------------------
Layer dropout_2_27, operation 27/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.6137, 0.3863, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1047, 0.6940, 0.2013, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0321, 0.3609, 0.3430, 0.2640, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0287, 0.0340, 0.2137, 0.6806, 0.0431, 0.0000, 0.0000, 0.0000],
        [0.1373, 0.1487, 0.1329, 0.1548, 0.3459, 0.0805, 0.0000, 0.0000],
        [0.1317, 0.1394, 0.0690, 0.0839, 0.2296, 0.2561, 0.0903, 0.0000],
        [0.0147, 0.1035, 0.0268, 0.0400, 0.0175, 0.0855, 0.6837, 0.0284]])...
	Related Layers:
		- parent layers: softmax_2_26
		- child layers: matmul_4_29
		- shares parents with no other layers
		- shares children with layers: linear_6_28
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.0.sa.heads.1.dropout:1
	Time elapsed:  5.841E-05s
	Output of modules: blocks.0.sa.heads.1.dropout
	Output of bottom-level module: blocks.0.sa.heads.1.dropout:1
	Lookup keys: -669, 29, blocks.0.sa.heads.1.dropout, blocks.0.sa.heads.1.dropout:1, dropout_2, dropout_2:1, dropout_2_27, dropout_2_27:1
--------------------------------------------
Layer linear_6_28, operation 28/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 3.9424e-02,  1.1634e+00,  1.5117e+00,  9.7719e-02, -1.8017e+00,
          6.0146e-01, -1.3487e+00,  7.5498e-01],
        [ 9.7148e-03,  7.2943e-01,  1.2182e+00,  2.1938e+00, -2.1485e+00,
          5.5029e-01,  1.5499e+00,  8.7547e-01],
        [-1.4232e-01,  2.2252e+00,  5.8802e-01,  1.7600e+00,  1.3297e-03,
          2.3042e+00,  2.0692e+00, -4.6043e-01],
        [ 1.4049e+00,  1.3827e+00, -2.2393e-01,  1.3620e+00,  1.0188e+00,
          1.2932e+00,  1.0579e+00, -3.4163e-01],
        [ 6.2874e-01, -1.0956e+00, -2.8758e+00, -1.1109e+00,  6.0602e-01,
         -1.6922e+00, -1.3154e+00, -2.6869e+00],
        [-3.3063e-01,  1.9788e+00,  2.8635e-01, -2.4749e+00, -1.2313e+00,
         -3.8313e+00, -4.9375e+00, -1.3263e+00],
        [-6.1817e-02,  2.6934e+00,  2.5786e+00,  1.1421e+00, -2.2550e+00,
          6.8997e-01,  3.1638e+00, -1.1005e+00],
        [-8.0821e-01, -3.8177e-01, -5.7922e-02,  2.4522e+00,  9.3497e-01,
          3.1750e-02,  2.3667e+00,  5.6394e-01]])...
	Related Layers:
		- parent layers: layernorm_1_5
		- child layers: matmul_4_29
		- shares parents with layers: linear_1_6, linear_2_7, linear_3_16, linear_4_18, linear_5_19, linear_7_30, linear_8_31, linear_9_40, linear_10_42, linear_11_43, linear_12_52, linear_13_54, linear_14_55, linear_15_64, linear_16_66, linear_17_67, linear_18_76, linear_19_78, linear_20_79, linear_21_88, linear_22_90, linear_23_91, linear_24_100
		- shares children with layers: dropout_2_27
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.1.value:1
	Time elapsed:  1.175E-04s
	Output of modules: blocks.0.sa.heads.1.value
	Output of bottom-level module: blocks.0.sa.heads.1.value:1
	Lookup keys: -668, 30, blocks.0.sa.heads.1.value, blocks.0.sa.heads.1.value:1, linear_6, linear_6:1, linear_6_28, linear_6_28:1
--------------------------------------------
Layer matmul_4_29, operation 29/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.0394,  1.1634,  1.5117,  0.0977, -1.8017,  0.6015, -1.3487,  0.7550],
        [ 0.0279,  0.9958,  1.3983,  0.9075, -1.9357,  0.5817, -0.2288,  0.8015],
        [-0.0178,  1.0759,  1.1221,  1.8871, -1.6795,  0.9087,  1.3510,  0.5940],
        [ 0.3268,  1.4289,  0.6308,  1.7581, -0.5638,  1.3497,  1.5050,  0.0920],
        [ 0.9543,  1.4275, -0.0659,  1.3326,  0.5950,  1.3356,  1.1195, -0.3952],
        [ 0.3963,  0.5582, -0.5396,  0.2009, -0.2982, -0.2228, -0.3682, -0.9164],
        [ 0.1687,  1.0228,  0.0365, -0.2313, -0.8311, -0.8841, -1.0107, -0.8950],
        [-0.0284,  2.1880,  1.8904,  0.9495, -1.8178,  0.2947,  2.0235, -0.8212]])...
	Related Layers:
		- parent layers: dropout_2_27, linear_6_28
		- child layers: cat_1_102
		- shares parents with no other layers
		- shares children with layers: matmul_2_17, matmul_6_41, matmul_8_53, matmul_10_65, matmul_12_77, matmul_14_89, matmul_16_101
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.1:1
	Time elapsed:  8.273E-05s
	Output of modules: blocks.0.sa.heads.1
	Lookup keys: -667, 31, blocks.0.sa.heads.1, blocks.0.sa.heads.1:1, matmul_4, matmul_4:1, matmul_4_29, matmul_4_29:1
--------------------------------------------
Layer linear_7_30, operation 30/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.8269, -1.2867, -0.5518,  1.2673,  1.5355, -0.3843, -2.0766,  0.2617],
        [ 0.2850, -1.3512,  1.6852, -0.9484,  1.0643, -0.0980,  0.7073,  0.4081],
        [ 1.7835,  0.5005,  0.6302,  1.4362,  0.8374,  0.0362, -0.1824,  0.3805],
        [ 1.1011,  0.1972, -0.6965,  1.5912,  2.3052,  0.8777, -0.7636,  1.0367],
        [-0.9336,  2.2033, -0.0208, -1.6088,  1.2095,  2.5971, -0.7501,  0.6135],
        [ 0.0124, -0.8406,  1.2593,  0.5600,  2.3586,  2.2064, -2.2789, -0.9263],
        [ 1.0080, -0.7855, -0.2213,  1.6126,  0.2548, -0.4056, -0.6587, -0.0759],
        [ 2.7488, -2.0400, -2.4605,  2.0243,  0.7330,  0.3941, -0.0355,  0.1060]])...
	Related Layers:
		- parent layers: layernorm_1_5
		- child layers: transpose_3_32
		- shares parents with layers: linear_1_6, linear_2_7, linear_3_16, linear_4_18, linear_5_19, linear_6_28, linear_8_31, linear_9_40, linear_10_42, linear_11_43, linear_12_52, linear_13_54, linear_14_55, linear_15_64, linear_16_66, linear_17_67, linear_18_76, linear_19_78, linear_20_79, linear_21_88, linear_22_90, linear_23_91, linear_24_100
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.2.key:1
	Time elapsed:  1.140E-04s
	Output of modules: blocks.0.sa.heads.2.key
	Output of bottom-level module: blocks.0.sa.heads.2.key:1
	Lookup keys: -666, 32, blocks.0.sa.heads.2.key, blocks.0.sa.heads.2.key:1, linear_7, linear_7:1, linear_7_30, linear_7_30:1
--------------------------------------------
Layer linear_8_31, operation 31/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.1743, -0.9282, -0.3082,  0.2519, -0.7424,  0.4181, -1.1181, -0.2287],
        [-0.3432,  0.3278,  1.2749, -0.0389, -0.6607,  1.1757, -1.6492,  0.1890],
        [-0.1105,  2.3266,  0.1230, -0.7120, -1.9409,  0.8122,  1.4694,  2.0419],
        [-1.0400,  2.6088, -0.0322, -0.9025, -3.1315,  0.5025,  2.6300,  0.6978],
        [-0.2579, -0.3915,  0.4737, -0.0707,  0.5933,  0.7979,  0.7387, -0.2212],
        [-1.0582,  1.7202, -1.6172,  0.6397, -0.1387, -1.2371,  0.1093,  1.1723],
        [-1.1609,  1.5260, -1.5822,  0.5773, -0.2972,  0.0631, -0.0847,  2.4303],
        [-0.1091,  0.7410, -1.3117,  0.6898,  0.4929, -0.0488, -1.0299, -1.2057]])...
	Related Layers:
		- parent layers: layernorm_1_5
		- child layers: matmul_5_33
		- shares parents with layers: linear_1_6, linear_2_7, linear_3_16, linear_4_18, linear_5_19, linear_6_28, linear_7_30, linear_9_40, linear_10_42, linear_11_43, linear_12_52, linear_13_54, linear_14_55, linear_15_64, linear_16_66, linear_17_67, linear_18_76, linear_19_78, linear_20_79, linear_21_88, linear_22_90, linear_23_91, linear_24_100
		- shares children with layers: transpose_3_32
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.2.query:1
	Time elapsed:  1.113E-04s
	Output of modules: blocks.0.sa.heads.2.query
	Output of bottom-level module: blocks.0.sa.heads.2.query:1
	Lookup keys: -665, 33, blocks.0.sa.heads.2.query, blocks.0.sa.heads.2.query:1, linear_8, linear_8:1, linear_8_31, linear_8_31:1
--------------------------------------------
Layer transpose_3_32, operation 32/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.8269,  0.2850,  1.7835,  1.1011, -0.9336,  0.0124,  1.0080,  2.7488],
        [-1.2867, -1.3512,  0.5005,  0.1972,  2.2033, -0.8406, -0.7855, -2.0400],
        [-0.5518,  1.6852,  0.6302, -0.6965, -0.0208,  1.2593, -0.2213, -2.4605],
        [ 1.2673, -0.9484,  1.4362,  1.5912, -1.6088,  0.5600,  1.6126,  2.0243],
        [ 1.5355,  1.0643,  0.8374,  2.3052,  1.2095,  2.3586,  0.2548,  0.7330],
        [-0.3843, -0.0980,  0.0362,  0.8777,  2.5971,  2.2064, -0.4056,  0.3941],
        [-2.0766,  0.7073, -0.1824, -0.7636, -0.7501, -2.2789, -0.6587, -0.0355],
        [ 0.2617,  0.4081,  0.3805,  1.0367,  0.6135, -0.9263, -0.0759,  0.1060]])...
	Related Layers:
		- parent layers: linear_7_30
		- child layers: matmul_5_33
		- shares parents with no other layers
		- shares children with layers: linear_8_31
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.0.sa.heads.2:1
	Time elapsed:  6.342E-05s
	Output of modules: none
	Lookup keys: -664, 34, transpose_3, transpose_3:1, transpose_3_32, transpose_3_32:1
--------------------------------------------
Layer matmul_5_33, operation 33/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ 3.6723e+00,  2.6908e+00, -2.8153e+00,  1.1892e+00, -3.7536e+00,
          4.9307e+00,  6.2192e+00,  1.5918e+00],
        [-2.6433e+01, -1.4090e+01, -9.2880e+00, -1.0733e+01,  1.4385e+00,
         -2.7836e+00, -1.3227e+01, -4.5933e+00],
        [-4.6421e+01,  1.3943e+00, -1.7647e+01, -1.1162e+01, -1.5711e+00,
         -1.9220e+01, -3.2098e+01, -3.2419e+01],
        [-7.5602e+01, -9.2252e+00, -2.9553e+01, -4.3956e+01,  3.2904e+00,
         -5.0432e+01, -4.9075e+01, -3.5805e+01],
        [-2.1413e+01, -3.7995e+00, -1.5992e+01, -1.0818e+01,  1.1932e+01,
         -2.7012e+01, -2.6101e+01, -4.8155e+00],
        [-1.4789e+00, -3.3236e+01,  1.6920e+01, -2.4221e+00,  1.1675e+01,
         -2.8884e+01, -3.6360e+00,  3.0873e+01],
        [-2.0274e+01, -1.4013e+01, -1.3579e+01,  9.0401e+00, -7.0497e+00,
         -1.3385e+01, -2.6342e+01, -2.6167e+00],
        [-1.0194e+01, -3.8815e-02, -1.0064e+01, -1.2364e+01,  6.4866e-01,
          1.5415e+01,  1.4262e+01, -8.9309e+00]])...
	Related Layers:
		- parent layers: linear_8_31, transpose_3_32
		- child layers: mul_3_34
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.2:1
	Time elapsed:  8.702E-05s
	Output of modules: none
	Lookup keys: -663, 35, matmul_5, matmul_5:1, matmul_5_33, matmul_5_33:1
--------------------------------------------
Layer mul_3_34, operation 34/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ 1.8740e-01,  1.3731e-01, -1.4367e-01,  6.0687e-02, -1.9155e-01,
          2.5162e-01,  3.1737e-01,  8.1232e-02],
        [-1.3489e+00, -7.1904e-01, -4.7398e-01, -5.4773e-01,  7.3406e-02,
         -1.4205e-01, -6.7501e-01, -2.3440e-01],
        [-2.3689e+00,  7.1154e-02, -9.0055e-01, -5.6960e-01, -8.0177e-02,
         -9.8083e-01, -1.6380e+00, -1.6544e+00],
        [-3.8580e+00, -4.7077e-01, -1.5081e+00, -2.2431e+00,  1.6791e-01,
         -2.5736e+00, -2.5044e+00, -1.8272e+00],
        [-1.0927e+00, -1.9389e-01, -8.1607e-01, -5.5205e-01,  6.0891e-01,
         -1.3785e+00, -1.3320e+00, -2.4574e-01],
        [-7.5472e-02, -1.6961e+00,  8.6345e-01, -1.2360e-01,  5.9579e-01,
         -1.4740e+00, -1.8555e-01,  1.5755e+00],
        [-1.0346e+00, -7.1507e-01, -6.9294e-01,  4.6133e-01, -3.5975e-01,
         -6.8305e-01, -1.3442e+00, -1.3353e-01],
        [-5.2019e-01, -1.9808e-03, -5.1356e-01, -6.3096e-01,  3.3102e-02,
          7.8663e-01,  7.2780e-01, -4.5575e-01]])...
	Related Layers:
		- parent layers: matmul_5_33
		- child layers: maskedfill_3_37
		- shares parents with no other layers
		- shares children with layers: eq_3_36
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.0.sa.heads.2:1
	Time elapsed:  6.866E-05s
	Output of modules: none
	Lookup keys: -662, 36, mul_3, mul_3:1, mul_3_34, mul_3_34:1
--------------------------------------------
Layer getitem_3_35, operation 35/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_3
		- child layers: eq_3_36
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  1.273E-04s
	Output of modules: none
	Lookup keys: -660, 38, getitem_3, getitem_3:1, getitem_3_35, getitem_3_35:1
--------------------------------------------
Layer eq_3_36, operation 36/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_3_35
		- child layers: maskedfill_3_37
		- shares parents with no other layers
		- shares children with layers: mul_3_34
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  7.939E-05s
	Output of modules: none
	Lookup keys: -659, 39, eq_3, eq_3:1, eq_3_36, eq_3_36:1
--------------------------------------------
Layer maskedfill_3_37, operation 37/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ 1.8740e-01,        -inf,        -inf,        -inf,        -inf,
                -inf,        -inf,        -inf],
        [-1.3489e+00, -7.1904e-01,        -inf,        -inf,        -inf,
                -inf,        -inf,        -inf],
        [-2.3689e+00,  7.1154e-02, -9.0055e-01,        -inf,        -inf,
                -inf,        -inf,        -inf],
        [-3.8580e+00, -4.7077e-01, -1.5081e+00, -2.2431e+00,        -inf,
                -inf,        -inf,        -inf],
        [-1.0927e+00, -1.9389e-01, -8.1607e-01, -5.5205e-01,  6.0891e-01,
                -inf,        -inf,        -inf],
        [-7.5472e-02, -1.6961e+00,  8.6345e-01, -1.2360e-01,  5.9579e-01,
         -1.4740e+00,        -inf,        -inf],
        [-1.0346e+00, -7.1507e-01, -6.9294e-01,  4.6133e-01, -3.5975e-01,
         -6.8305e-01, -1.3442e+00,        -inf],
        [-5.2019e-01, -1.9808e-03, -5.1356e-01, -6.3096e-01,  3.3102e-02,
          7.8663e-01,  7.2780e-01, -4.5575e-01]])...
	Related Layers:
		- parent layers: mul_3_34, eq_3_36
		- child layers: softmax_3_38
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.0.sa.heads.2:1
	Time elapsed:  8.583E-05s
	Output of modules: none
	Lookup keys: -658, 40, maskedfill_3, maskedfill_3:1, maskedfill_3_37, maskedfill_3_37:1
--------------------------------------------
Layer softmax_3_38, operation 38/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3475, 0.6525, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0595, 0.6823, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0217, 0.6418, 0.2274, 0.1091, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0835, 0.2051, 0.1101, 0.1434, 0.4578, 0.0000, 0.0000, 0.0000],
        [0.1447, 0.0286, 0.3700, 0.1379, 0.2831, 0.0357, 0.0000, 0.0000],
        [0.0809, 0.1113, 0.1138, 0.3609, 0.1588, 0.1149, 0.0593, 0.0000],
        [0.0687, 0.1153, 0.0691, 0.0615, 0.1194, 0.2537, 0.2392, 0.0732]])...
	Related Layers:
		- parent layers: maskedfill_3_37
		- child layers: dropout_3_39
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.0.sa.heads.2:1
	Time elapsed:  7.153E-05s
	Output of modules: none
	Lookup keys: -657, 41, softmax_3, softmax_3:1, softmax_3_38, softmax_3_38:1
--------------------------------------------
Layer dropout_3_39, operation 39/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3475, 0.6525, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0595, 0.6823, 0.2582, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0217, 0.6418, 0.2274, 0.1091, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0835, 0.2051, 0.1101, 0.1434, 0.4578, 0.0000, 0.0000, 0.0000],
        [0.1447, 0.0286, 0.3700, 0.1379, 0.2831, 0.0357, 0.0000, 0.0000],
        [0.0809, 0.1113, 0.1138, 0.3609, 0.1588, 0.1149, 0.0593, 0.0000],
        [0.0687, 0.1153, 0.0691, 0.0615, 0.1194, 0.2537, 0.2392, 0.0732]])...
	Related Layers:
		- parent layers: softmax_3_38
		- child layers: matmul_6_41
		- shares parents with no other layers
		- shares children with layers: linear_9_40
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.0.sa.heads.2.dropout:1
	Time elapsed:  1.249E-04s
	Output of modules: blocks.0.sa.heads.2.dropout
	Output of bottom-level module: blocks.0.sa.heads.2.dropout:1
	Lookup keys: -656, 42, blocks.0.sa.heads.2.dropout, blocks.0.sa.heads.2.dropout:1, dropout_3, dropout_3:1, dropout_3_39, dropout_3_39:1
--------------------------------------------
Layer linear_9_40, operation 40/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.3491,  0.8444, -0.5354, -0.4723, -0.5182,  0.0422, -0.6934,  1.4686],
        [ 0.3434,  0.8323,  1.7436, -1.8016, -1.8712,  0.7338,  0.5457, -0.3330],
        [-0.3510,  0.9970, -0.2129,  0.6447, -0.1048,  0.0790, -0.1263,  1.0868],
        [-0.5085,  1.7145,  1.5251,  0.2784, -0.7130, -0.5821, -1.4032,  0.2071],
        [ 0.8943,  0.6291,  0.7058, -0.8749, -0.0493,  0.0628,  0.0268, -0.0238],
        [-0.1626,  0.6109, -1.2388, -0.7553,  0.0807, -0.9534, -1.7846,  0.0076],
        [ 0.3509,  0.4863,  1.2446,  0.0699, -0.0408, -0.1445, -0.0199,  1.2433],
        [-0.0700, -0.1278, -0.2990, -0.7796, -0.5822, -0.5603,  0.1722, -1.5673]])...
	Related Layers:
		- parent layers: layernorm_1_5
		- child layers: matmul_6_41
		- shares parents with layers: linear_1_6, linear_2_7, linear_3_16, linear_4_18, linear_5_19, linear_6_28, linear_7_30, linear_8_31, linear_10_42, linear_11_43, linear_12_52, linear_13_54, linear_14_55, linear_15_64, linear_16_66, linear_17_67, linear_18_76, linear_19_78, linear_20_79, linear_21_88, linear_22_90, linear_23_91, linear_24_100
		- shares children with layers: dropout_3_39
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.2.value:1
	Time elapsed:  1.192E-04s
	Output of modules: blocks.0.sa.heads.2.value
	Output of bottom-level module: blocks.0.sa.heads.2.value:1
	Lookup keys: -655, 43, blocks.0.sa.heads.2.value, blocks.0.sa.heads.2.value:1, linear_9, linear_9:1, linear_9_40, linear_9_40:1
--------------------------------------------
Layer matmul_6_41, operation 41/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.3491,  0.8444, -0.5354, -0.4723, -0.5182,  0.0422, -0.6934,  1.4686],
        [ 0.3454,  0.8365,  0.9515, -1.3396, -1.4010,  0.4935,  0.1151,  0.2931],
        [ 0.1644,  0.8755,  1.1029, -1.0909, -1.3346,  0.5236,  0.2985,  0.1407],
        [ 0.0927,  0.9662,  1.2253, -0.9895, -1.3138,  0.4264,  0.1534,  0.0879],
        [ 0.3975,  0.8849,  0.8314, -0.6987, -0.5635,  0.1080, -0.1488,  0.1928],
        [ 0.1077,  0.9512,  0.2595, -0.1177, -0.2767, -0.0402, -0.3811,  0.6272],
        [-0.0129,  1.0922,  0.7206, -0.2865, -0.5205, -0.2242, -0.7182,  0.3510],
        [ 0.1524,  0.6653,  0.2891, -0.5149, -0.3402, -0.2528, -0.5213,  0.3320]])...
	Related Layers:
		- parent layers: dropout_3_39, linear_9_40
		- child layers: cat_1_102
		- shares parents with no other layers
		- shares children with layers: matmul_2_17, matmul_4_29, matmul_8_53, matmul_10_65, matmul_12_77, matmul_14_89, matmul_16_101
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.2:1
	Time elapsed:  8.774E-05s
	Output of modules: blocks.0.sa.heads.2
	Lookup keys: -654, 44, blocks.0.sa.heads.2, blocks.0.sa.heads.2:1, matmul_6, matmul_6:1, matmul_6_41, matmul_6_41:1
--------------------------------------------
Layer linear_10_42, operation 42/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.2337,  0.0794, -1.7802,  1.0619,  0.2012, -1.3427, -0.1589,  1.9701],
        [ 1.0200, -1.0562, -1.0061, -2.2724,  1.0381, -0.8726, -1.6242,  0.6269],
        [-0.7458,  0.9567,  0.2847,  0.8569, -0.9566, -1.1240, -1.9315,  0.7813],
        [-0.7806,  0.8416, -1.5666,  1.1571, -0.0519, -0.3678, -2.6662,  0.9316],
        [ 0.1455, -0.3947, -1.2788, -0.1489, -0.5663,  0.8419,  0.1662, -1.5483],
        [ 1.5172,  0.9812, -0.5225, -0.3304, -0.3882,  0.4031, -0.6368, -2.0687],
        [ 0.5120, -2.8522, -1.0647,  0.9249, -0.2722,  0.3339, -0.9475, -0.3598],
        [ 0.5493,  0.3257,  3.6412,  3.1380, -1.3256, -1.1328, -2.2225,  0.8177]])...
	Related Layers:
		- parent layers: layernorm_1_5
		- child layers: transpose_4_44
		- shares parents with layers: linear_1_6, linear_2_7, linear_3_16, linear_4_18, linear_5_19, linear_6_28, linear_7_30, linear_8_31, linear_9_40, linear_11_43, linear_12_52, linear_13_54, linear_14_55, linear_15_64, linear_16_66, linear_17_67, linear_18_76, linear_19_78, linear_20_79, linear_21_88, linear_22_90, linear_23_91, linear_24_100
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.3.key:1
	Time elapsed:  1.140E-04s
	Output of modules: blocks.0.sa.heads.3.key
	Output of bottom-level module: blocks.0.sa.heads.3.key:1
	Lookup keys: -653, 45, blocks.0.sa.heads.3.key, blocks.0.sa.heads.3.key:1, linear_10, linear_10:1, linear_10_42, linear_10_42:1
--------------------------------------------
Layer linear_11_43, operation 43/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.0416,  0.9826,  0.4436, -0.6529,  0.5482, -1.0651,  0.5014, -0.2990],
        [ 0.7232,  0.2272, -0.4701, -0.9684, -0.5566,  1.8964,  1.9190, -0.6639],
        [ 0.4417,  0.1414,  0.4712,  0.1481,  1.0427,  0.9413,  0.1428, -0.5292],
        [-1.4068,  0.9817,  1.5883,  0.5848,  0.3864,  0.4159,  0.9184, -0.7142],
        [-0.5476, -0.3643,  1.3998, -0.7814,  0.0815,  1.7108,  0.5412, -0.6033],
        [-0.7102, -0.6136, -1.8849,  0.2773, -0.6400, -0.1336, -0.0445, -0.0033],
        [-0.2027, -2.2870,  1.5186,  0.8655, -1.3172, -1.5573, -2.3853, -1.9420],
        [ 0.7843, -0.6177, -0.1298, -0.2669,  1.2032,  0.1784,  0.3066, -0.8058]])...
	Related Layers:
		- parent layers: layernorm_1_5
		- child layers: matmul_7_45
		- shares parents with layers: linear_1_6, linear_2_7, linear_3_16, linear_4_18, linear_5_19, linear_6_28, linear_7_30, linear_8_31, linear_9_40, linear_10_42, linear_12_52, linear_13_54, linear_14_55, linear_15_64, linear_16_66, linear_17_67, linear_18_76, linear_19_78, linear_20_79, linear_21_88, linear_22_90, linear_23_91, linear_24_100
		- shares children with layers: transpose_4_44
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.3.query:1
	Time elapsed:  1.132E-04s
	Output of modules: blocks.0.sa.heads.3.query
	Output of bottom-level module: blocks.0.sa.heads.3.query:1
	Lookup keys: -652, 46, blocks.0.sa.heads.3.query, blocks.0.sa.heads.3.query:1, linear_11, linear_11:1, linear_11_43, linear_11_43:1
--------------------------------------------
Layer transpose_4_44, operation 44/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.2337,  1.0200, -0.7458, -0.7806,  0.1455,  1.5172,  0.5120,  0.5493],
        [ 0.0794, -1.0562,  0.9567,  0.8416, -0.3947,  0.9812, -2.8522,  0.3257],
        [-1.7802, -1.0061,  0.2847, -1.5666, -1.2788, -0.5225, -1.0647,  3.6412],
        [ 1.0619, -2.2724,  0.8569,  1.1571, -0.1489, -0.3304,  0.9249,  3.1380],
        [ 0.2012,  1.0381, -0.9566, -0.0519, -0.5663, -0.3882, -0.2722, -1.3256],
        [-1.3427, -0.8726, -1.1240, -0.3678,  0.8419,  0.4031,  0.3339, -1.1328],
        [-0.1589, -1.6242, -1.9315, -2.6662,  0.1662, -0.6368, -0.9475, -2.2225],
        [ 1.9701,  0.6269,  0.7813,  0.9316, -1.5483, -2.0687, -0.3598,  0.8177]])...
	Related Layers:
		- parent layers: linear_10_42
		- child layers: matmul_7_45
		- shares parents with no other layers
		- shares children with layers: linear_11_43
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.0.sa.heads.3:1
	Time elapsed:  5.913E-05s
	Output of modules: none
	Lookup keys: -651, 47, transpose_4, transpose_4:1, transpose_4_44, transpose_4_44:1
--------------------------------------------
Layer matmul_7_45, operation 45/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-27.5413, -15.8960, -16.2241, -11.0799, -12.7926,  10.9659, -13.7878,
          -6.9910],
        [-36.5220, -42.2618, -34.6934, -40.4414, -18.5016,   4.6140, -13.2536,
         -22.9046],
        [-39.6554,  -4.5202, -22.5611,  -9.7574,   2.7567,   2.6583,   4.6242,
         -11.7273],
        [-38.2255, -35.1849,  -9.8534, -21.9461,  -4.3188,  -8.1712, -18.9343,
         -17.1587],
        [-44.2340, -25.6247, -15.9801,  -2.7994, -11.5595,   0.2720,  -5.3381,
         -11.3120],
        [  3.6991,  -0.5714,  -5.9691,   1.4464,  -6.8216,  -9.4362,  12.8340,
         -18.0727],
        [  3.7841,  31.3665,  18.6675,  34.5375,  15.5052,  49.2453,   4.9804,
          50.2447],
        [-58.2480, -38.7748, -40.9200, -50.7587,  -9.0611,   5.8286,  34.6122,
         -33.0867]])...
	Related Layers:
		- parent layers: linear_11_43, transpose_4_44
		- child layers: mul_4_46
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.3:1
	Time elapsed:  8.678E-05s
	Output of modules: none
	Lookup keys: -650, 48, matmul_7, matmul_7:1, matmul_7_45, matmul_7_45:1
--------------------------------------------
Layer mul_4_46, operation 46/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-1.4055, -0.8112, -0.8279, -0.5654, -0.6528,  0.5596, -0.7036, -0.3568],
        [-1.8638, -2.1567, -1.7704, -2.0638, -0.9442,  0.2355, -0.6763, -1.1688],
        [-2.0237, -0.2307, -1.1513, -0.4979,  0.1407,  0.1357,  0.2360, -0.5985],
        [-1.9507, -1.7955, -0.5028, -1.1199, -0.2204, -0.4170, -0.9662, -0.8756],
        [-2.2573, -1.3077, -0.8155, -0.1429, -0.5899,  0.0139, -0.2724, -0.5773],
        [ 0.1888, -0.0292, -0.3046,  0.0738, -0.3481, -0.4815,  0.6549, -0.9223],
        [ 0.1931,  1.6007,  0.9526,  1.7625,  0.7912,  2.5130,  0.2542,  2.5640],
        [-2.9725, -1.9787, -2.0882, -2.5903, -0.4624,  0.2974,  1.7663, -1.6884]])...
	Related Layers:
		- parent layers: matmul_7_45
		- child layers: maskedfill_4_49
		- shares parents with no other layers
		- shares children with layers: eq_4_48
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.0.sa.heads.3:1
	Time elapsed:  6.747E-05s
	Output of modules: none
	Lookup keys: -649, 49, mul_4, mul_4:1, mul_4_46, mul_4_46:1
--------------------------------------------
Layer getitem_4_47, operation 47/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_4
		- child layers: eq_4_48
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  1.254E-04s
	Output of modules: none
	Lookup keys: -647, 51, getitem_4, getitem_4:1, getitem_4_47, getitem_4_47:1
--------------------------------------------
Layer eq_4_48, operation 48/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_4_47
		- child layers: maskedfill_4_49
		- shares parents with no other layers
		- shares children with layers: mul_4_46
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  7.963E-05s
	Output of modules: none
	Lookup keys: -646, 52, eq_4, eq_4:1, eq_4_48, eq_4_48:1
--------------------------------------------
Layer maskedfill_4_49, operation 49/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-1.4055,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-1.8638, -2.1567,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-2.0237, -0.2307, -1.1513,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-1.9507, -1.7955, -0.5028, -1.1199,    -inf,    -inf,    -inf,    -inf],
        [-2.2573, -1.3077, -0.8155, -0.1429, -0.5899,    -inf,    -inf,    -inf],
        [ 0.1888, -0.0292, -0.3046,  0.0738, -0.3481, -0.4815,    -inf,    -inf],
        [ 0.1931,  1.6007,  0.9526,  1.7625,  0.7912,  2.5130,  0.2542,    -inf],
        [-2.9725, -1.9787, -2.0882, -2.5903, -0.4624,  0.2974,  1.7663, -1.6884]])...
	Related Layers:
		- parent layers: mul_4_46, eq_4_48
		- child layers: softmax_4_50
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.0.sa.heads.3:1
	Time elapsed:  8.583E-05s
	Output of modules: none
	Lookup keys: -645, 53, maskedfill_4, maskedfill_4:1, maskedfill_4_49, maskedfill_4_49:1
--------------------------------------------
Layer softmax_4_50, operation 50/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5727, 0.4273, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1064, 0.6391, 0.2545, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1147, 0.1340, 0.4880, 0.2633, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0467, 0.1208, 0.1976, 0.3872, 0.2476, 0.0000, 0.0000, 0.0000],
        [0.2272, 0.1827, 0.1387, 0.2025, 0.1328, 0.1162, 0.0000, 0.0000],
        [0.0399, 0.1629, 0.0852, 0.1915, 0.0725, 0.4056, 0.0424, 0.0000],
        [0.0061, 0.0165, 0.0148, 0.0089, 0.0750, 0.1603, 0.6965, 0.0220]])...
	Related Layers:
		- parent layers: maskedfill_4_49
		- child layers: dropout_4_51
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.0.sa.heads.3:1
	Time elapsed:  7.224E-05s
	Output of modules: none
	Lookup keys: -644, 54, softmax_4, softmax_4:1, softmax_4_50, softmax_4_50:1
--------------------------------------------
Layer dropout_4_51, operation 51/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5727, 0.4273, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1064, 0.6391, 0.2545, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1147, 0.1340, 0.4880, 0.2633, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0467, 0.1208, 0.1976, 0.3872, 0.2476, 0.0000, 0.0000, 0.0000],
        [0.2272, 0.1827, 0.1387, 0.2025, 0.1328, 0.1162, 0.0000, 0.0000],
        [0.0399, 0.1629, 0.0852, 0.1915, 0.0725, 0.4056, 0.0424, 0.0000],
        [0.0061, 0.0165, 0.0148, 0.0089, 0.0750, 0.1603, 0.6965, 0.0220]])...
	Related Layers:
		- parent layers: softmax_4_50
		- child layers: matmul_8_53
		- shares parents with no other layers
		- shares children with layers: linear_12_52
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.0.sa.heads.3.dropout:1
	Time elapsed:  5.436E-05s
	Output of modules: blocks.0.sa.heads.3.dropout
	Output of bottom-level module: blocks.0.sa.heads.3.dropout:1
	Lookup keys: -643, 55, blocks.0.sa.heads.3.dropout, blocks.0.sa.heads.3.dropout:1, dropout_4, dropout_4:1, dropout_4_51, dropout_4_51:1
--------------------------------------------
Layer linear_12_52, operation 52/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.5642,  0.5486, -2.0499, -0.5557, -0.5879, -0.4679,  0.2557,  0.9771],
        [-0.3653,  0.1409, -2.1030, -0.9356, -0.3261,  0.0154,  0.3381,  1.4482],
        [-0.4226, -0.3005, -0.5492, -0.3345,  0.3078,  0.4690,  0.0055, -0.0773],
        [-0.4603,  0.0489,  0.2181, -0.7925, -0.1473,  1.5935, -0.0340, -0.6026],
        [-0.6619,  0.1497, -0.1655, -0.1230,  0.8470, -0.2489,  1.4297,  0.5031],
        [ 0.2608,  0.6876,  1.5869, -1.8781, -2.7717,  1.7350,  1.4993,  0.5189],
        [ 0.1967,  0.7391, -0.5545,  0.2232,  0.5872,  0.6036, -2.0433,  0.0956],
        [-0.4341,  0.4876, -0.9224,  1.9118,  0.8372, -0.2997,  0.5342, -0.2535]])...
	Related Layers:
		- parent layers: layernorm_1_5
		- child layers: matmul_8_53
		- shares parents with layers: linear_1_6, linear_2_7, linear_3_16, linear_4_18, linear_5_19, linear_6_28, linear_7_30, linear_8_31, linear_9_40, linear_10_42, linear_11_43, linear_13_54, linear_14_55, linear_15_64, linear_16_66, linear_17_67, linear_18_76, linear_19_78, linear_20_79, linear_21_88, linear_22_90, linear_23_91, linear_24_100
		- shares children with layers: dropout_4_51
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.3.value:1
	Time elapsed:  1.194E-04s
	Output of modules: blocks.0.sa.heads.3.value
	Output of bottom-level module: blocks.0.sa.heads.3.value:1
	Lookup keys: -642, 56, blocks.0.sa.heads.3.value, blocks.0.sa.heads.3.value:1, linear_12, linear_12:1, linear_12_52, linear_12_52:1
--------------------------------------------
Layer matmul_8_53, operation 53/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 5.6420e-01,  5.4865e-01, -2.0499e+00, -5.5575e-01, -5.8789e-01,
         -4.6792e-01,  2.5575e-01,  9.7710e-01],
        [ 1.6703e-01,  3.7443e-01, -2.0726e+00, -7.1807e-01, -4.7604e-01,
         -2.6141e-01,  2.9092e-01,  1.1784e+00],
        [-2.8100e-01,  7.1925e-02, -1.7018e+00, -7.4221e-01, -1.9262e-01,
          7.9432e-02,  2.4466e-01,  1.0098e+00],
        [-3.1163e-01, -5.1989e-02, -7.2749e-01, -5.6098e-01,  2.7995e-04,
          5.9684e-01,  6.8369e-02,  1.0976e-01],
        [-4.4340e-01,  3.9254e-02, -4.1491e-01, -5.4242e-01,  1.4665e-01,
          6.2808e-01,  3.9473e-01,  9.6612e-02],
        [-1.4797e-01,  2.1836e-01, -7.1939e-01, -7.3859e-01, -3.8986e-01,
          4.5281e-01,  4.7778e-01,  4.8088e-01],
        [-9.5043e-02,  3.4968e-01,  1.7890e-01, -1.1161e+00, -1.1166e+00,
          1.0403e+00,  6.8446e-01,  4.0389e-01],
        [ 1.0668e-01,  6.4863e-01, -2.1773e-01, -1.4363e-01,  4.0860e-02,
          6.9185e-01, -1.0569e+00,  2.0516e-01]])...
	Related Layers:
		- parent layers: dropout_4_51, linear_12_52
		- child layers: cat_1_102
		- shares parents with no other layers
		- shares children with layers: matmul_2_17, matmul_4_29, matmul_6_41, matmul_10_65, matmul_12_77, matmul_14_89, matmul_16_101
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.3:1
	Time elapsed:  8.345E-05s
	Output of modules: blocks.0.sa.heads.3
	Lookup keys: -641, 57, blocks.0.sa.heads.3, blocks.0.sa.heads.3:1, matmul_8, matmul_8:1, matmul_8_53, matmul_8_53:1
--------------------------------------------
Layer linear_13_54, operation 54/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.4614, -0.9331, -0.9403,  0.8395,  0.8970,  1.4867, -2.1891,  0.0056],
        [ 0.6100,  0.8310, -0.6352, -0.9016,  0.3524,  0.5024, -0.9612, -0.9163],
        [ 0.3239,  0.2020, -0.2689, -1.2792,  0.1240, -1.0812, -0.6752, -1.2455],
        [ 0.6054, -1.2879,  0.1483, -0.6422,  1.1865, -0.7535, -0.8208, -1.4185],
        [ 0.4709, -1.4793, -0.7191, -0.2805,  0.2184,  1.3679, -0.7773,  0.8427],
        [ 1.8213,  0.7136,  0.6012, -1.0046, -2.2050,  1.3026, -0.7529,  0.3779],
        [ 1.6202,  0.4123, -0.2786, -1.8545, -2.2627,  0.8388,  0.7909,  0.1794],
        [-0.0431,  0.9184, -0.3445, -1.0878, -1.1543,  1.1235,  0.6168,  0.2234]])...
	Related Layers:
		- parent layers: layernorm_1_5
		- child layers: transpose_5_56
		- shares parents with layers: linear_1_6, linear_2_7, linear_3_16, linear_4_18, linear_5_19, linear_6_28, linear_7_30, linear_8_31, linear_9_40, linear_10_42, linear_11_43, linear_12_52, linear_14_55, linear_15_64, linear_16_66, linear_17_67, linear_18_76, linear_19_78, linear_20_79, linear_21_88, linear_22_90, linear_23_91, linear_24_100
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.4.key:1
	Time elapsed:  1.118E-04s
	Output of modules: blocks.0.sa.heads.4.key
	Output of bottom-level module: blocks.0.sa.heads.4.key:1
	Lookup keys: -640, 58, blocks.0.sa.heads.4.key, blocks.0.sa.heads.4.key:1, linear_13, linear_13:1, linear_13_54, linear_13_54:1
--------------------------------------------
Layer linear_14_55, operation 55/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.5831,  0.3683, -1.5304, -0.4046, -0.7594,  0.0198, -0.8944,  0.9941],
        [-1.0533, -2.8751, -1.4441, -0.0635,  1.4423, -1.3546, -1.5423,  0.5974],
        [-0.8650, -0.0461,  0.9651,  0.9248,  0.7965, -2.1293,  0.5408, -1.5620],
        [-1.2083,  0.1645,  1.4225,  0.2437,  0.6623, -2.1936,  0.8867, -1.8809],
        [ 0.8323, -0.3882,  0.4649, -0.3672,  1.6020,  0.0703, -0.4283,  1.5666],
        [-1.6662, -0.4071,  2.7034,  0.7770, -0.2038,  0.2577,  0.9207, -0.5691],
        [ 0.7249,  1.4412,  1.0186,  1.2587,  1.1820, -0.0519,  1.6258,  0.0549],
        [-0.3795, -0.1394, -0.3439,  0.4175, -3.4032,  0.0878,  1.0997, -0.0556]])...
	Related Layers:
		- parent layers: layernorm_1_5
		- child layers: matmul_9_57
		- shares parents with layers: linear_1_6, linear_2_7, linear_3_16, linear_4_18, linear_5_19, linear_6_28, linear_7_30, linear_8_31, linear_9_40, linear_10_42, linear_11_43, linear_12_52, linear_13_54, linear_15_64, linear_16_66, linear_17_67, linear_18_76, linear_19_78, linear_20_79, linear_21_88, linear_22_90, linear_23_91, linear_24_100
		- shares children with layers: transpose_5_56
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.4.query:1
	Time elapsed:  1.166E-04s
	Output of modules: blocks.0.sa.heads.4.query
	Output of bottom-level module: blocks.0.sa.heads.4.query:1
	Lookup keys: -639, 59, blocks.0.sa.heads.4.query, blocks.0.sa.heads.4.query:1, linear_14, linear_14:1, linear_14_55, linear_14_55:1
--------------------------------------------
Layer transpose_5_56, operation 56/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.4614,  0.6100,  0.3239,  0.6054,  0.4709,  1.8213,  1.6202, -0.0431],
        [-0.9331,  0.8310,  0.2020, -1.2879, -1.4793,  0.7136,  0.4123,  0.9184],
        [-0.9403, -0.6352, -0.2689,  0.1483, -0.7191,  0.6012, -0.2786, -0.3445],
        [ 0.8395, -0.9016, -1.2792, -0.6422, -0.2805, -1.0046, -1.8545, -1.0878],
        [ 0.8970,  0.3524,  0.1240,  1.1865,  0.2184, -2.2050, -2.2627, -1.1543],
        [ 1.4867,  0.5024, -1.0812, -0.7535,  1.3679,  1.3026,  0.8388,  1.1235],
        [-2.1891, -0.9612, -0.6752, -0.8208, -0.7773, -0.7529,  0.7909,  0.6168],
        [ 0.0056, -0.9163, -1.2455, -1.4185,  0.8427,  0.3779,  0.1794,  0.2234]])...
	Related Layers:
		- parent layers: linear_13_54
		- child layers: matmul_9_57
		- shares parents with no other layers
		- shares children with layers: linear_14_55
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.0.sa.heads.4:1
	Time elapsed:  5.865E-05s
	Output of modules: none
	Lookup keys: -638, 60, transpose_5, transpose_5:1, transpose_5_56, transpose_5_56:1
--------------------------------------------
Layer matmul_9_57, operation 57/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[  8.6183,  13.0451,  -3.0245,  -2.0518,  22.6797,   6.7306,   5.3632,
          -8.7085],
        [  3.9785, -14.9015,  -8.8766,   3.3483,  55.1968, -11.4199, -20.3589,
         -24.9215],
        [-20.3656, -14.2462,   6.1035,  -0.5777,  -4.7107, -19.0977, -25.6400,
          -7.3758],
        [-21.4183,  -6.8684,  11.6704,   3.1048,  -3.3100, -13.5797, -17.0711,
         -14.5769],
        [ -0.9968,  -7.8972,   4.1119,   8.8610,   0.6094,  -1.9277,  -9.7531,
          -8.9544],
        [-32.1490, -40.7051,   7.6530,  -3.3875,   6.7485, -23.4715, -15.5017,
          -8.3571],
        [-21.4553, -17.7719, -14.5259,  -2.4863, -31.6259,   0.6180, -19.8395,
          22.7301],
        [-16.9219, -25.7343,  -3.4295, -10.9486, -10.1560,   6.7106,  14.9411,
          -0.9897]])...
	Related Layers:
		- parent layers: linear_14_55, transpose_5_56
		- child layers: mul_5_58
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.4:1
	Time elapsed:  8.631E-05s
	Output of modules: none
	Lookup keys: -637, 61, matmul_9, matmul_9:1, matmul_9_57, matmul_9_57:1
--------------------------------------------
Layer mul_5_58, operation 58/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ 0.4398,  0.6657, -0.1543, -0.1047,  1.1574,  0.3435,  0.2737, -0.4444],
        [ 0.2030, -0.7604, -0.4530,  0.1709,  2.8168, -0.5828, -1.0389, -1.2718],
        [-1.0393, -0.7270,  0.3115, -0.0295, -0.2404, -0.9746, -1.3084, -0.3764],
        [-1.0930, -0.3505,  0.5956,  0.1584, -0.1689, -0.6930, -0.8712, -0.7439],
        [-0.0509, -0.4030,  0.2098,  0.4522,  0.0311, -0.0984, -0.4977, -0.4570],
        [-1.6406, -2.0772,  0.3905, -0.1729,  0.3444, -1.1978, -0.7911, -0.4265],
        [-1.0949, -0.9069, -0.7413, -0.1269, -1.6139,  0.0315, -1.0124,  1.1599],
        [-0.8635, -1.3132, -0.1750, -0.5587, -0.5183,  0.3425,  0.7625, -0.0505]])...
	Related Layers:
		- parent layers: matmul_9_57
		- child layers: maskedfill_5_61
		- shares parents with no other layers
		- shares children with layers: eq_5_60
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.0.sa.heads.4:1
	Time elapsed:  6.866E-05s
	Output of modules: none
	Lookup keys: -636, 62, mul_5, mul_5:1, mul_5_58, mul_5_58:1
--------------------------------------------
Layer getitem_5_59, operation 59/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_5
		- child layers: eq_5_60
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  1.254E-04s
	Output of modules: none
	Lookup keys: -634, 64, getitem_5, getitem_5:1, getitem_5_59, getitem_5_59:1
--------------------------------------------
Layer eq_5_60, operation 60/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_5_59
		- child layers: maskedfill_5_61
		- shares parents with no other layers
		- shares children with layers: mul_5_58
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.059E-05s
	Output of modules: none
	Lookup keys: -633, 65, eq_5, eq_5:1, eq_5_60, eq_5_60:1
--------------------------------------------
Layer maskedfill_5_61, operation 61/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ 0.4398,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [ 0.2030, -0.7604,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-1.0393, -0.7270,  0.3115,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-1.0930, -0.3505,  0.5956,  0.1584,    -inf,    -inf,    -inf,    -inf],
        [-0.0509, -0.4030,  0.2098,  0.4522,  0.0311,    -inf,    -inf,    -inf],
        [-1.6406, -2.0772,  0.3905, -0.1729,  0.3444, -1.1978,    -inf,    -inf],
        [-1.0949, -0.9069, -0.7413, -0.1269, -1.6139,  0.0315, -1.0124,    -inf],
        [-0.8635, -1.3132, -0.1750, -0.5587, -0.5183,  0.3425,  0.7625, -0.0505]])...
	Related Layers:
		- parent layers: mul_5_58, eq_5_60
		- child layers: softmax_5_62
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.0.sa.heads.4:1
	Time elapsed:  8.392E-05s
	Output of modules: none
	Lookup keys: -632, 66, maskedfill_5, maskedfill_5:1, maskedfill_5_61, maskedfill_5_61:1
--------------------------------------------
Layer softmax_5_62, operation 62/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.7238, 0.2762, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1606, 0.2195, 0.6199, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0833, 0.1750, 0.4507, 0.2911, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1742, 0.1225, 0.2261, 0.2881, 0.1891, 0.0000, 0.0000, 0.0000],
        [0.0446, 0.0288, 0.3396, 0.1933, 0.3243, 0.0694, 0.0000, 0.0000],
        [0.0907, 0.1094, 0.1291, 0.2387, 0.0540, 0.2797, 0.0985, 0.0000],
        [0.0586, 0.0374, 0.1166, 0.0794, 0.0827, 0.1956, 0.2977, 0.1320]])...
	Related Layers:
		- parent layers: maskedfill_5_61
		- child layers: dropout_5_63
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.0.sa.heads.4:1
	Time elapsed:  7.033E-05s
	Output of modules: none
	Lookup keys: -631, 67, softmax_5, softmax_5:1, softmax_5_62, softmax_5_62:1
--------------------------------------------
Layer dropout_5_63, operation 63/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.7238, 0.2762, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1606, 0.2195, 0.6199, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0833, 0.1750, 0.4507, 0.2911, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1742, 0.1225, 0.2261, 0.2881, 0.1891, 0.0000, 0.0000, 0.0000],
        [0.0446, 0.0288, 0.3396, 0.1933, 0.3243, 0.0694, 0.0000, 0.0000],
        [0.0907, 0.1094, 0.1291, 0.2387, 0.0540, 0.2797, 0.0985, 0.0000],
        [0.0586, 0.0374, 0.1166, 0.0794, 0.0827, 0.1956, 0.2977, 0.1320]])...
	Related Layers:
		- parent layers: softmax_5_62
		- child layers: matmul_10_65
		- shares parents with no other layers
		- shares children with layers: linear_15_64
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.0.sa.heads.4.dropout:1
	Time elapsed:  5.722E-05s
	Output of modules: blocks.0.sa.heads.4.dropout
	Output of bottom-level module: blocks.0.sa.heads.4.dropout:1
	Lookup keys: -630, 68, blocks.0.sa.heads.4.dropout, blocks.0.sa.heads.4.dropout:1, dropout_5, dropout_5:1, dropout_5_63, dropout_5_63:1
--------------------------------------------
Layer linear_15_64, operation 64/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 3.5300e-01,  8.0403e-01,  4.9218e-01, -7.8778e-01, -4.2026e-01,
         -1.4931e+00,  1.6751e+00,  1.4500e-01],
        [ 7.7097e-02,  3.2970e-01,  4.8231e-01, -1.5966e+00,  5.7613e-01,
         -1.2957e+00,  1.3515e+00, -7.6241e-02],
        [-2.0172e+00,  4.5734e-01, -6.8841e-01,  2.0509e-01,  8.6415e-01,
         -1.0460e+00,  9.9177e-01,  8.1037e-01],
        [-7.4275e-01,  9.7082e-01, -1.4775e-01, -1.0499e+00,  7.8782e-01,
          1.8839e-01,  2.3816e-01,  9.7956e-01],
        [-1.8292e+00,  1.2865e+00, -8.6509e-01, -6.7555e-01,  1.5997e+00,
          7.9433e-01,  1.5394e+00, -6.8714e-01],
        [-9.6422e-02,  1.6183e+00, -2.8942e-01, -1.7123e+00,  1.8610e+00,
         -1.4966e+00, -5.4046e-01,  8.7694e-01],
        [-7.7254e-04,  8.5337e-01,  5.8789e-01, -2.5685e-01,  7.1182e-01,
         -6.5489e-01,  5.7779e-01,  5.1874e-02],
        [-7.5572e-01,  5.9493e-01, -8.6104e-01,  1.1907e-01,  7.6023e-01,
          9.0321e-01,  2.9800e-01,  5.1173e-01]])...
	Related Layers:
		- parent layers: layernorm_1_5
		- child layers: matmul_10_65
		- shares parents with layers: linear_1_6, linear_2_7, linear_3_16, linear_4_18, linear_5_19, linear_6_28, linear_7_30, linear_8_31, linear_9_40, linear_10_42, linear_11_43, linear_12_52, linear_13_54, linear_14_55, linear_16_66, linear_17_67, linear_18_76, linear_19_78, linear_20_79, linear_21_88, linear_22_90, linear_23_91, linear_24_100
		- shares children with layers: dropout_5_63
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.4.value:1
	Time elapsed:  1.180E-04s
	Output of modules: blocks.0.sa.heads.4.value
	Output of bottom-level module: blocks.0.sa.heads.4.value:1
	Lookup keys: -629, 69, blocks.0.sa.heads.4.value, blocks.0.sa.heads.4.value:1, linear_15, linear_15:1, linear_15_64, linear_15_64:1
--------------------------------------------
Layer matmul_10_65, operation 65/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.3530,  0.8040,  0.4922, -0.7878, -0.4203, -1.4931,  1.6751,  0.1450],
        [ 0.2768,  0.6730,  0.4895, -1.0112, -0.1451, -1.4386,  1.5857,  0.0839],
        [-1.1769,  0.4850, -0.2419, -0.3498,  0.5947, -1.1726,  1.1804,  0.5089],
        [-1.0824,  0.6133, -0.2279, -0.5582,  0.6846, -0.7676,  0.8923,  0.6491],
        [-0.9450,  0.8068, -0.2170, -0.7167,  0.7222, -0.4509,  1.0413,  0.3514],
        [-1.4107,  0.9178, -0.5272, -0.5523,  1.0916, -0.2689,  0.9582,  0.3069],
        [-0.5231,  1.0058, -0.0965, -1.0109,  1.0014, -0.8074,  0.4735,  0.5566],
        [-0.5408,  0.9454, -0.1120, -0.6168,  0.9689, -0.5456,  0.5161,  0.3756]])...
	Related Layers:
		- parent layers: dropout_5_63, linear_15_64
		- child layers: cat_1_102
		- shares parents with no other layers
		- shares children with layers: matmul_2_17, matmul_4_29, matmul_6_41, matmul_8_53, matmul_12_77, matmul_14_89, matmul_16_101
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.4:1
	Time elapsed:  8.321E-05s
	Output of modules: blocks.0.sa.heads.4
	Lookup keys: -628, 70, blocks.0.sa.heads.4, blocks.0.sa.heads.4:1, matmul_10, matmul_10:1, matmul_10_65, matmul_10_65:1
--------------------------------------------
Layer linear_16_66, operation 66/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-1.2530, -1.2724, -1.6778, -0.3595, -0.3410, -1.4350, -0.8172,  0.7267],
        [-0.8843, -0.0734, -0.7471,  0.5492,  1.5965,  0.4694, -0.2665, -0.3184],
        [-1.1086, -0.2601, -0.4872, -0.3963,  0.3095,  0.2474, -1.0081, -0.1056],
        [-0.6826,  0.3720, -1.1690, -0.3375,  0.5128,  1.7727, -0.3769,  0.9886],
        [-0.3154,  0.6600, -0.1751,  1.2623,  0.7383, -0.1458, -0.0721,  0.6556],
        [-1.8603,  1.6462,  1.5219,  2.1544,  2.0307, -0.0648, -1.9453,  1.1927],
        [-0.1085,  0.2747, -0.4815, -0.8287,  0.0816,  0.2011, -0.1404, -0.2501],
        [-0.9637,  1.0623,  1.0600, -0.6698,  1.7293,  1.0944,  0.0983, -0.6678]])...
	Related Layers:
		- parent layers: layernorm_1_5
		- child layers: transpose_6_68
		- shares parents with layers: linear_1_6, linear_2_7, linear_3_16, linear_4_18, linear_5_19, linear_6_28, linear_7_30, linear_8_31, linear_9_40, linear_10_42, linear_11_43, linear_12_52, linear_13_54, linear_14_55, linear_15_64, linear_17_67, linear_18_76, linear_19_78, linear_20_79, linear_21_88, linear_22_90, linear_23_91, linear_24_100
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.5.key:1
	Time elapsed:  1.140E-04s
	Output of modules: blocks.0.sa.heads.5.key
	Output of bottom-level module: blocks.0.sa.heads.5.key:1
	Lookup keys: -627, 71, blocks.0.sa.heads.5.key, blocks.0.sa.heads.5.key:1, linear_16, linear_16:1, linear_16_66, linear_16_66:1
--------------------------------------------
Layer linear_17_67, operation 67/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.5812,  0.2605, -0.5176,  1.1044,  0.4133,  0.0170,  1.3065, -0.4716],
        [ 0.0516, -0.2721,  0.9107,  1.1746,  0.7844, -0.2205,  0.6075, -2.2162],
        [ 2.2508, -0.7943,  1.4143, -0.8848,  0.4647,  0.1202,  0.6771, -0.8077],
        [ 1.0997, -1.2111, -0.0289, -0.1510,  0.2977,  1.4170,  0.9190, -0.2663],
        [-0.8358, -1.2741,  1.4036,  1.8923,  0.8615,  1.3736,  0.9314, -0.4738],
        [ 0.5888, -1.3689, -0.5228,  1.3760, -1.3751,  0.9046,  2.7480, -1.6703],
        [-1.8879, -0.5597, -1.4024,  0.0610, -1.9083, -0.9758,  1.0575, -0.0239],
        [ 0.3011, -0.4510,  2.5394,  0.4474,  1.8838,  2.0112, -1.6218,  0.8299]])...
	Related Layers:
		- parent layers: layernorm_1_5
		- child layers: matmul_11_69
		- shares parents with layers: linear_1_6, linear_2_7, linear_3_16, linear_4_18, linear_5_19, linear_6_28, linear_7_30, linear_8_31, linear_9_40, linear_10_42, linear_11_43, linear_12_52, linear_13_54, linear_14_55, linear_15_64, linear_16_66, linear_18_76, linear_19_78, linear_20_79, linear_21_88, linear_22_90, linear_23_91, linear_24_100
		- shares children with layers: transpose_6_68
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.5.query:1
	Time elapsed:  1.137E-04s
	Output of modules: blocks.0.sa.heads.5.query
	Output of bottom-level module: blocks.0.sa.heads.5.query:1
	Lookup keys: -626, 72, blocks.0.sa.heads.5.query, blocks.0.sa.heads.5.query:1, linear_17, linear_17:1, linear_17_67, linear_17_67:1
--------------------------------------------
Layer transpose_6_68, operation 68/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-1.2530, -0.8843, -1.1086, -0.6826, -0.3154, -1.8603, -0.1085, -0.9637],
        [-1.2724, -0.0734, -0.2601,  0.3720,  0.6600,  1.6462,  0.2747,  1.0623],
        [-1.6778, -0.7471, -0.4872, -1.1690, -0.1751,  1.5219, -0.4815,  1.0600],
        [-0.3595,  0.5492, -0.3963, -0.3375,  1.2623,  2.1544, -0.8287, -0.6698],
        [-0.3410,  1.5965,  0.3095,  0.5128,  0.7383,  2.0307,  0.0816,  1.7293],
        [-1.4350,  0.4694,  0.2474,  1.7727, -0.1458, -0.0648,  0.2011,  1.0944],
        [-0.8172, -0.2665, -1.0081, -0.3769, -0.0721, -1.9453, -0.1404,  0.0983],
        [ 0.7267, -0.3184, -0.1056,  0.9886,  0.6556,  1.1927, -0.2501, -0.6678]])...
	Related Layers:
		- parent layers: linear_16_66
		- child layers: matmul_11_69
		- shares parents with no other layers
		- shares children with layers: linear_17_67
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.0.sa.heads.5:1
	Time elapsed:  5.889E-05s
	Output of modules: none
	Lookup keys: -625, 73, transpose_6, transpose_6:1, transpose_6_68, transpose_6_68:1
--------------------------------------------
Layer matmul_11_69, operation 69/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -4.2099,   7.4027,   1.0030,   3.8834,   4.9578,  16.4774,  -9.2099,
          10.7846],
        [-32.8060,  -8.5414,  29.1584,  20.4763,  40.8581, -17.1809, -33.7135,
           9.0885],
        [-16.4232,  -0.8841, -20.2130, -21.9511,   5.2692, -20.2233, -17.9312,
          -6.6143],
        [-19.1533,  -2.2720, -19.4420,  -5.6016,   6.7030, -34.4936, -17.4294,
          -6.8027],
        [-17.6226,   3.7728,  -7.2329,  -0.2704,  10.2872, -11.0301,  -5.8987,
          10.1903],
        [ -7.3252, -21.9842,  10.1952,  19.9003,  19.0390, -46.3277,  -5.5823,
         -24.0743],
        [ 25.9382, -12.7947,   3.1039,   5.8515,   5.7587, -16.1690,  11.6551,
         -26.2419],
        [-11.0072,   2.7621,  -6.7980,   6.6596,  15.6633,  27.1198,   8.7194,
          10.0140]])...
	Related Layers:
		- parent layers: linear_17_67, transpose_6_68
		- child layers: mul_6_70
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.5:1
	Time elapsed:  8.655E-05s
	Output of modules: none
	Lookup keys: -624, 74, matmul_11, matmul_11:1, matmul_11_69, matmul_11_69:1
--------------------------------------------
Layer mul_6_70, operation 70/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-0.2148,  0.3778,  0.0512,  0.1982,  0.2530,  0.8409, -0.4700,  0.5504],
        [-1.6741, -0.4359,  1.4880,  1.0449,  2.0850, -0.8768, -1.7204,  0.4638],
        [-0.8381, -0.0451, -1.0315, -1.1202,  0.2689, -1.0320, -0.9150, -0.3375],
        [-0.9774, -0.1159, -0.9921, -0.2859,  0.3421, -1.7602, -0.8894, -0.3472],
        [-0.8993,  0.1925, -0.3691, -0.0138,  0.5250, -0.5629, -0.3010,  0.5200],
        [-0.3738, -1.1219,  0.5203,  1.0155,  0.9716, -2.3641, -0.2849, -1.2285],
        [ 1.3237, -0.6529,  0.1584,  0.2986,  0.2939, -0.8251,  0.5948, -1.3392],
        [-0.5617,  0.1410, -0.3469,  0.3398,  0.7993,  1.3839,  0.4450,  0.5110]])...
	Related Layers:
		- parent layers: matmul_11_69
		- child layers: maskedfill_6_73
		- shares parents with no other layers
		- shares children with layers: eq_6_72
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.0.sa.heads.5:1
	Time elapsed:  6.938E-05s
	Output of modules: none
	Lookup keys: -623, 75, mul_6, mul_6:1, mul_6_70, mul_6_70:1
--------------------------------------------
Layer getitem_6_71, operation 71/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_6
		- child layers: eq_6_72
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  1.302E-04s
	Output of modules: none
	Lookup keys: -621, 77, getitem_6, getitem_6:1, getitem_6_71, getitem_6_71:1
--------------------------------------------
Layer eq_6_72, operation 72/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_6_71
		- child layers: maskedfill_6_73
		- shares parents with no other layers
		- shares children with layers: mul_6_70
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  7.844E-05s
	Output of modules: none
	Lookup keys: -620, 78, eq_6, eq_6:1, eq_6_72, eq_6_72:1
--------------------------------------------
Layer maskedfill_6_73, operation 73/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-0.2148,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-1.6741, -0.4359,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-0.8381, -0.0451, -1.0315,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-0.9774, -0.1159, -0.9921, -0.2859,    -inf,    -inf,    -inf,    -inf],
        [-0.8993,  0.1925, -0.3691, -0.0138,  0.5250,    -inf,    -inf,    -inf],
        [-0.3738, -1.1219,  0.5203,  1.0155,  0.9716, -2.3641,    -inf,    -inf],
        [ 1.3237, -0.6529,  0.1584,  0.2986,  0.2939, -0.8251,  0.5948,    -inf],
        [-0.5617,  0.1410, -0.3469,  0.3398,  0.7993,  1.3839,  0.4450,  0.5110]])...
	Related Layers:
		- parent layers: mul_6_70, eq_6_72
		- child layers: softmax_6_74
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.0.sa.heads.5:1
	Time elapsed:  8.440E-05s
	Output of modules: none
	Lookup keys: -619, 79, maskedfill_6, maskedfill_6:1, maskedfill_6_73, maskedfill_6_73:1
--------------------------------------------
Layer softmax_6_74, operation 74/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2247, 0.7753, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2479, 0.5478, 0.2043, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1575, 0.3728, 0.1552, 0.3145, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0816, 0.2431, 0.1386, 0.1978, 0.3389, 0.0000, 0.0000, 0.0000],
        [0.0840, 0.0397, 0.2054, 0.3370, 0.3225, 0.0115, 0.0000, 0.0000],
        [0.3616, 0.0501, 0.1128, 0.1297, 0.1291, 0.0422, 0.1745, 0.0000],
        [0.0430, 0.0867, 0.0532, 0.1058, 0.1675, 0.3006, 0.1175, 0.1256]])...
	Related Layers:
		- parent layers: maskedfill_6_73
		- child layers: dropout_6_75
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.0.sa.heads.5:1
	Time elapsed:  7.010E-05s
	Output of modules: none
	Lookup keys: -618, 80, softmax_6, softmax_6:1, softmax_6_74, softmax_6_74:1
--------------------------------------------
Layer dropout_6_75, operation 75/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2247, 0.7753, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2479, 0.5478, 0.2043, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1575, 0.3728, 0.1552, 0.3145, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0816, 0.2431, 0.1386, 0.1978, 0.3389, 0.0000, 0.0000, 0.0000],
        [0.0840, 0.0397, 0.2054, 0.3370, 0.3225, 0.0115, 0.0000, 0.0000],
        [0.3616, 0.0501, 0.1128, 0.1297, 0.1291, 0.0422, 0.1745, 0.0000],
        [0.0430, 0.0867, 0.0532, 0.1058, 0.1675, 0.3006, 0.1175, 0.1256]])...
	Related Layers:
		- parent layers: softmax_6_74
		- child layers: matmul_12_77
		- shares parents with no other layers
		- shares children with layers: linear_18_76
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.0.sa.heads.5.dropout:1
	Time elapsed:  5.412E-05s
	Output of modules: blocks.0.sa.heads.5.dropout
	Output of bottom-level module: blocks.0.sa.heads.5.dropout:1
	Lookup keys: -617, 81, blocks.0.sa.heads.5.dropout, blocks.0.sa.heads.5.dropout:1, dropout_6, dropout_6:1, dropout_6_75, dropout_6_75:1
--------------------------------------------
Layer linear_18_76, operation 76/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.1026, -0.9155, -0.6318, -0.1614,  1.8513,  1.3002, -1.1730, -1.0399],
        [ 0.9461, -0.6324,  0.4466,  0.1093,  1.1192,  1.3175, -0.8478,  0.1335],
        [ 0.7539, -1.4144, -0.4066, -0.7515,  0.5983,  0.3156,  0.0692, -0.2468],
        [-0.1453, -1.5279, -0.7916, -1.4094,  0.1411,  0.3135, -0.8124, -1.0700],
        [ 1.3264,  1.4158,  0.5751,  0.7976, -0.0189,  0.3296, -1.4533, -0.1401],
        [-0.0327, -1.5422, -0.0300, -0.7866,  2.0444, -0.1139, -1.5482, -0.5740],
        [ 0.6449, -0.0577, -1.2854, -0.2650,  1.0749, -0.1599, -1.2029, -0.9069],
        [ 0.6062,  0.3120, -1.0167,  0.3869,  0.4915, -0.9333, -0.5529, -0.8354]])...
	Related Layers:
		- parent layers: layernorm_1_5
		- child layers: matmul_12_77
		- shares parents with layers: linear_1_6, linear_2_7, linear_3_16, linear_4_18, linear_5_19, linear_6_28, linear_7_30, linear_8_31, linear_9_40, linear_10_42, linear_11_43, linear_12_52, linear_13_54, linear_14_55, linear_15_64, linear_16_66, linear_17_67, linear_19_78, linear_20_79, linear_21_88, linear_22_90, linear_23_91, linear_24_100
		- shares children with layers: dropout_6_75
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.5.value:1
	Time elapsed:  1.471E-04s
	Output of modules: blocks.0.sa.heads.5.value
	Output of bottom-level module: blocks.0.sa.heads.5.value:1
	Lookup keys: -616, 82, blocks.0.sa.heads.5.value, blocks.0.sa.heads.5.value:1, linear_18, linear_18:1, linear_18_76, linear_18_76:1
--------------------------------------------
Layer matmul_12_77, operation 77/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.1026, -0.9155, -0.6318, -0.1614,  1.8513,  1.3002, -1.1730, -1.0399],
        [ 0.7566, -0.6960,  0.2043,  0.0485,  1.2838,  1.3136, -0.9209, -0.1302],
        [ 0.6978, -0.8623,  0.0050, -0.1336,  1.1943,  1.1085, -0.7411, -0.2351],
        [ 0.4402, -1.0800, -0.2451, -0.5446,  0.8461,  0.8435, -0.7456, -0.4889],
        [ 0.7637, -0.2468,  0.0390, -0.0992,  0.5275,  0.6438, -0.9454, -0.3457],
        [ 0.5794, -0.4685, -0.2004, -0.3903,  0.3877,  0.4370, -0.8782, -0.5450],
        [ 0.4331, -0.6128, -0.5059, -0.2969,  1.0826,  0.6223, -1.0271, -0.7365],
        [ 0.4756, -0.5252, -0.2853, -0.2720,  1.0228,  0.1051, -1.1259, -0.5670]])...
	Related Layers:
		- parent layers: dropout_6_75, linear_18_76
		- child layers: cat_1_102
		- shares parents with no other layers
		- shares children with layers: matmul_2_17, matmul_4_29, matmul_6_41, matmul_8_53, matmul_10_65, matmul_14_89, matmul_16_101
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.5:1
	Time elapsed:  1.099E-04s
	Output of modules: blocks.0.sa.heads.5
	Lookup keys: -615, 83, blocks.0.sa.heads.5, blocks.0.sa.heads.5:1, matmul_12, matmul_12:1, matmul_12_77, matmul_12_77:1
--------------------------------------------
Layer linear_19_78, operation 78/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.9630, -0.5980,  2.2882,  0.3395,  0.2552,  2.0256, -0.9178, -0.0430],
        [-0.7276, -0.0326, -0.7855,  0.3830, -0.4478,  0.3113,  0.1189,  0.6031],
        [-1.3114, -1.0833,  1.6714,  0.9873,  1.9303, -1.4291, -1.6722, -1.6020],
        [-1.5664, -0.8005,  1.6894,  1.7989,  1.7525, -1.0749, -1.5475,  0.5411],
        [-0.4390, -0.0837, -0.8532, -0.9576,  0.5249,  0.1109, -1.3763, -0.3601],
        [ 1.1373, -0.9884,  0.4590,  1.9912, -0.6561, -0.5463, -2.0246,  2.0956],
        [ 0.5241, -0.0911, -0.0584,  1.8511, -0.3449,  0.2108, -0.7431,  2.0487],
        [-1.3062, -1.7621,  2.0738,  1.5866,  0.6887, -0.4558,  0.1196, -1.9547]])...
	Related Layers:
		- parent layers: layernorm_1_5
		- child layers: transpose_7_80
		- shares parents with layers: linear_1_6, linear_2_7, linear_3_16, linear_4_18, linear_5_19, linear_6_28, linear_7_30, linear_8_31, linear_9_40, linear_10_42, linear_11_43, linear_12_52, linear_13_54, linear_14_55, linear_15_64, linear_16_66, linear_17_67, linear_18_76, linear_20_79, linear_21_88, linear_22_90, linear_23_91, linear_24_100
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.6.key:1
	Time elapsed:  1.342E-04s
	Output of modules: blocks.0.sa.heads.6.key
	Output of bottom-level module: blocks.0.sa.heads.6.key:1
	Lookup keys: -614, 84, blocks.0.sa.heads.6.key, blocks.0.sa.heads.6.key:1, linear_19, linear_19:1, linear_19_78, linear_19_78:1
--------------------------------------------
Layer linear_20_79, operation 79/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.3184, -1.5550,  0.6444, -0.3489, -0.2706, -1.2516, -0.1393, -0.6652],
        [ 0.5807,  0.0364, -1.7195,  0.1250, -0.5522, -1.4007, -0.5237, -0.8165],
        [ 0.0913,  0.7526, -0.6757, -0.4211,  0.3214,  0.4474,  0.5477,  0.8810],
        [-0.7081,  1.3370, -1.4279, -0.7180, -0.4571, -0.6607,  0.6622,  0.4959],
        [-1.2361, -0.0225,  2.3757,  2.2902,  0.6188, -1.8436,  0.0505, -0.4032],
        [ 0.4914,  1.0614, -0.3288, -1.1921, -0.6271,  0.9852,  1.2003, -0.4241],
        [ 0.9954,  0.3895, -0.5345, -1.1848, -0.9915,  0.9802, -1.0622,  1.5402],
        [ 1.6693,  1.3684, -1.0003,  1.3300, -0.4814, -1.0713,  0.4277,  2.6377]])...
	Related Layers:
		- parent layers: layernorm_1_5
		- child layers: matmul_13_81
		- shares parents with layers: linear_1_6, linear_2_7, linear_3_16, linear_4_18, linear_5_19, linear_6_28, linear_7_30, linear_8_31, linear_9_40, linear_10_42, linear_11_43, linear_12_52, linear_13_54, linear_14_55, linear_15_64, linear_16_66, linear_17_67, linear_18_76, linear_19_78, linear_21_88, linear_22_90, linear_23_91, linear_24_100
		- shares children with layers: transpose_7_80
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.6.query:1
	Time elapsed:  1.335E-04s
	Output of modules: blocks.0.sa.heads.6.query
	Output of bottom-level module: blocks.0.sa.heads.6.query:1
	Lookup keys: -613, 85, blocks.0.sa.heads.6.query, blocks.0.sa.heads.6.query:1, linear_20, linear_20:1, linear_20_79, linear_20_79:1
--------------------------------------------
Layer transpose_7_80, operation 80/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.9630, -0.7276, -1.3114, -1.5664, -0.4390,  1.1373,  0.5241, -1.3062],
        [-0.5980, -0.0326, -1.0833, -0.8005, -0.0837, -0.9884, -0.0911, -1.7621],
        [ 2.2882, -0.7855,  1.6714,  1.6894, -0.8532,  0.4590, -0.0584,  2.0738],
        [ 0.3395,  0.3830,  0.9873,  1.7989, -0.9576,  1.9912,  1.8511,  1.5866],
        [ 0.2552, -0.4478,  1.9303,  1.7525,  0.5249, -0.6561, -0.3449,  0.6887],
        [ 2.0256,  0.3113, -1.4291, -1.0749,  0.1109, -0.5463,  0.2108, -0.4558],
        [-0.9178,  0.1189, -1.6722, -1.5475, -1.3763, -2.0246, -0.7431,  0.1196],
        [-0.0430,  0.6031, -1.6020,  0.5411, -0.3601,  2.0956,  2.0487, -1.9547]])...
	Related Layers:
		- parent layers: linear_19_78
		- child layers: matmul_13_81
		- shares parents with no other layers
		- shares children with layers: linear_20_79
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.0.sa.heads.6:1
	Time elapsed:  6.056E-05s
	Output of modules: none
	Lookup keys: -612, 86, transpose_7, transpose_7:1, transpose_7_80, transpose_7_80:1
--------------------------------------------
Layer matmul_13_81, operation 81/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-13.7491,   6.8538,  32.4654,  34.1304,  -3.6636,  25.4072,   4.3345,
          12.3106],
        [-28.2388,   9.6566,   0.7337,   2.6971,  -6.1782,  10.7809, -19.1444,
          -2.1569],
        [-15.1916, -12.3742, -16.3611,  -7.0736,   4.2261,  -9.8492,  -7.1562,
          -5.9910],
        [ -7.8542,  -7.2993, -22.0694, -20.9875,  -2.2780, -16.6015,   0.8377,
          -7.0855],
        [ -9.6666, -10.3519,  50.9151,  60.2363, -14.1039,   7.1264,  -3.9165,
          22.4286],
        [  6.4491, -22.5313, -24.7271, -34.9254,   3.2622, -45.1513, -18.8573,
         -13.0710],
        [ 23.3822, -12.8240, -33.6139, -37.2216,  20.6114, -19.6907,   6.8271,
          -7.0246],
        [-28.4973,  -3.9369, -26.2737,  -8.1639, -11.9490,  19.6154,  28.9203,
         -10.8604]])...
	Related Layers:
		- parent layers: linear_20_79, transpose_7_80
		- child layers: mul_7_82
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.6:1
	Time elapsed:  1.197E-04s
	Output of modules: none
	Lookup keys: -611, 87, matmul_13, matmul_13:1, matmul_13_81, matmul_13_81:1
--------------------------------------------
Layer mul_7_82, operation 82/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-0.7016,  0.3498,  1.6567,  1.7417, -0.1870,  1.2966,  0.2212,  0.6282],
        [-1.4411,  0.4928,  0.0374,  0.1376, -0.3153,  0.5502, -0.9770, -0.1101],
        [-0.7752, -0.6315, -0.8349, -0.3610,  0.2157, -0.5026, -0.3652, -0.3057],
        [-0.4008, -0.3725, -1.1262, -1.0710, -0.1163, -0.8472,  0.0427, -0.3616],
        [-0.4933, -0.5283,  2.5983,  3.0739, -0.7197,  0.3637, -0.1999,  1.1446],
        [ 0.3291, -1.1498, -1.2618, -1.7823,  0.1665, -2.3041, -0.9623, -0.6670],
        [ 1.1932, -0.6544, -1.7153, -1.8995,  1.0518, -1.0048,  0.3484, -0.3585],
        [-1.4542, -0.2009, -1.3408, -0.4166, -0.6098,  1.0010,  1.4758, -0.5542]])...
	Related Layers:
		- parent layers: matmul_13_81
		- child layers: maskedfill_7_85
		- shares parents with no other layers
		- shares children with layers: eq_7_84
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.0.sa.heads.6:1
	Time elapsed:  8.488E-05s
	Output of modules: none
	Lookup keys: -610, 88, mul_7, mul_7:1, mul_7_82, mul_7_82:1
--------------------------------------------
Layer getitem_7_83, operation 83/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_7
		- child layers: eq_7_84
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  1.400E-04s
	Output of modules: none
	Lookup keys: -608, 90, getitem_7, getitem_7:1, getitem_7_83, getitem_7_83:1
--------------------------------------------
Layer eq_7_84, operation 84/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_7_83
		- child layers: maskedfill_7_85
		- shares parents with no other layers
		- shares children with layers: mul_7_82
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.965E-05s
	Output of modules: none
	Lookup keys: -607, 91, eq_7, eq_7:1, eq_7_84, eq_7_84:1
--------------------------------------------
Layer maskedfill_7_85, operation 85/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-0.7016,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-1.4411,  0.4928,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-0.7752, -0.6315, -0.8349,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-0.4008, -0.3725, -1.1262, -1.0710,    -inf,    -inf,    -inf,    -inf],
        [-0.4933, -0.5283,  2.5983,  3.0739, -0.7197,    -inf,    -inf,    -inf],
        [ 0.3291, -1.1498, -1.2618, -1.7823,  0.1665, -2.3041,    -inf,    -inf],
        [ 1.1932, -0.6544, -1.7153, -1.8995,  1.0518, -1.0048,  0.3484,    -inf],
        [-1.4542, -0.2009, -1.3408, -0.4166, -0.6098,  1.0010,  1.4758, -0.5542]])...
	Related Layers:
		- parent layers: mul_7_82, eq_7_84
		- child layers: softmax_7_86
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.0.sa.heads.6:1
	Time elapsed:  9.584E-05s
	Output of modules: none
	Lookup keys: -606, 92, maskedfill_7, maskedfill_7:1, maskedfill_7_85, maskedfill_7_85:1
--------------------------------------------
Layer softmax_7_86, operation 86/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1263, 0.8737, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3229, 0.3729, 0.3042, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3306, 0.3401, 0.1601, 0.1692, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0166, 0.0160, 0.3657, 0.5884, 0.0132, 0.0000, 0.0000, 0.0000],
        [0.4041, 0.0921, 0.0823, 0.0489, 0.3435, 0.0290, 0.0000, 0.0000],
        [0.3750, 0.0591, 0.0205, 0.0170, 0.3256, 0.0416, 0.1611, 0.0000],
        [0.0229, 0.0803, 0.0257, 0.0647, 0.0534, 0.2671, 0.4295, 0.0564]])...
	Related Layers:
		- parent layers: maskedfill_7_85
		- child layers: dropout_7_87
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.0.sa.heads.6:1
	Time elapsed:  8.059E-05s
	Output of modules: none
	Lookup keys: -605, 93, softmax_7, softmax_7:1, softmax_7_86, softmax_7_86:1
--------------------------------------------
Layer dropout_7_87, operation 87/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1263, 0.8737, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3229, 0.3729, 0.3042, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3306, 0.3401, 0.1601, 0.1692, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0166, 0.0160, 0.3657, 0.5884, 0.0132, 0.0000, 0.0000, 0.0000],
        [0.4041, 0.0921, 0.0823, 0.0489, 0.3435, 0.0290, 0.0000, 0.0000],
        [0.3750, 0.0591, 0.0205, 0.0170, 0.3256, 0.0416, 0.1611, 0.0000],
        [0.0229, 0.0803, 0.0257, 0.0647, 0.0534, 0.2671, 0.4295, 0.0564]])...
	Related Layers:
		- parent layers: softmax_7_86
		- child layers: matmul_14_89
		- shares parents with no other layers
		- shares children with layers: linear_21_88
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.0.sa.heads.6.dropout:1
	Time elapsed:  5.770E-05s
	Output of modules: blocks.0.sa.heads.6.dropout
	Output of bottom-level module: blocks.0.sa.heads.6.dropout:1
	Lookup keys: -604, 94, blocks.0.sa.heads.6.dropout, blocks.0.sa.heads.6.dropout:1, dropout_7, dropout_7:1, dropout_7_87, dropout_7_87:1
--------------------------------------------
Layer linear_21_88, operation 88/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.2918,  1.0052, -0.7437, -0.0259,  1.0106,  0.4445, -2.5767, -0.7241],
        [-0.8648,  0.4561,  0.3473, -0.1681, -0.0756,  0.5019, -0.6089,  1.0799],
        [ 1.4897,  0.2112, -0.7040,  1.3644,  0.8159,  0.5683, -0.4163, -0.3717],
        [ 0.5671,  0.9324, -0.2159, -0.3031,  0.4632, -0.0758, -1.0674, -1.6619],
        [ 0.4459, -0.7170,  0.3315, -0.2883,  0.9215, -0.1105, -1.5509, -0.4095],
        [-0.1817,  1.3725,  0.0128, -0.6168,  0.9235, -1.4327, -1.0726, -1.0581],
        [ 0.4624, -0.0563,  0.1903,  0.8945,  0.4540,  0.5467, -1.8332, -0.5095],
        [ 0.0659, -0.0084, -0.8849,  0.3032,  0.3487, -0.5016, -0.4372, -0.7291]])...
	Related Layers:
		- parent layers: layernorm_1_5
		- child layers: matmul_14_89
		- shares parents with layers: linear_1_6, linear_2_7, linear_3_16, linear_4_18, linear_5_19, linear_6_28, linear_7_30, linear_8_31, linear_9_40, linear_10_42, linear_11_43, linear_12_52, linear_13_54, linear_14_55, linear_15_64, linear_16_66, linear_17_67, linear_18_76, linear_19_78, linear_20_79, linear_22_90, linear_23_91, linear_24_100
		- shares children with layers: dropout_7_87
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.6.value:1
	Time elapsed:  1.578E-04s
	Output of modules: blocks.0.sa.heads.6.value
	Output of bottom-level module: blocks.0.sa.heads.6.value:1
	Lookup keys: -603, 95, blocks.0.sa.heads.6.value, blocks.0.sa.heads.6.value:1, linear_21, linear_21:1, linear_21_88, linear_21_88:1
--------------------------------------------
Layer matmul_14_89, operation 89/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.2918,  1.0052, -0.7437, -0.0259,  1.0106,  0.4445, -2.5767, -0.7241],
        [-0.7925,  0.5255,  0.2095, -0.1502,  0.0617,  0.4947, -0.8575,  0.8520],
        [ 0.0365,  0.5589, -0.3248,  0.3440,  0.5464,  0.5036, -1.1858,  0.0558],
        [-0.0562,  0.6790, -0.2769,  0.1014,  0.5174,  0.3958, -1.3063, -0.2127],
        [ 0.8656,  0.6404, -0.3868,  0.3136,  0.5987,  0.1772, -0.8534, -1.1139],
        [ 0.1007,  0.3048, -0.2229, -0.0454,  0.8346,  0.1894, -1.7477, -0.4765],
        [ 0.0917,  0.2388, -0.1373,  0.0277,  0.8108,  0.1992, -1.8741, -0.5031],
        [ 0.1764,  0.4292,  0.0317,  0.2224,  0.5785, -0.1219, -1.3690, -0.6114]])...
	Related Layers:
		- parent layers: dropout_7_87, linear_21_88
		- child layers: cat_1_102
		- shares parents with no other layers
		- shares children with layers: matmul_2_17, matmul_4_29, matmul_6_41, matmul_8_53, matmul_10_65, matmul_12_77, matmul_16_101
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.6:1
	Time elapsed:  1.118E-04s
	Output of modules: blocks.0.sa.heads.6
	Lookup keys: -602, 96, blocks.0.sa.heads.6, blocks.0.sa.heads.6:1, matmul_14, matmul_14:1, matmul_14_89, matmul_14_89:1
--------------------------------------------
Layer linear_22_90, operation 90/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-3.7492, -0.1843, -1.1992,  0.6688,  1.0709, -0.0678, -0.1054,  1.1640],
        [-0.9604, -0.7854, -1.2175, -0.7915, -0.3387, -0.5947, -1.2489, -0.0998],
        [ 0.7230,  0.7118,  0.3125, -0.5111, -0.2906, -1.1642, -1.1677, -0.9925],
        [ 2.0368,  1.3613, -0.3790,  0.3629, -0.4664, -2.1893, -0.3414, -1.6033],
        [ 1.9753,  3.8312,  1.0411,  1.7405, -0.8341, -0.4709,  1.3851,  0.6335],
        [ 2.6646,  0.0429, -0.8879,  1.0128, -0.7241,  0.5790, -0.7573, -0.1237],
        [-1.2357,  1.3769, -1.0236,  0.4776, -0.2898, -0.4754, -1.1721,  1.6559],
        [-0.3242, -0.2501,  0.9228,  0.7891, -0.4234,  1.0876, -0.0115,  0.6528]])...
	Related Layers:
		- parent layers: layernorm_1_5
		- child layers: transpose_8_92
		- shares parents with layers: linear_1_6, linear_2_7, linear_3_16, linear_4_18, linear_5_19, linear_6_28, linear_7_30, linear_8_31, linear_9_40, linear_10_42, linear_11_43, linear_12_52, linear_13_54, linear_14_55, linear_15_64, linear_16_66, linear_17_67, linear_18_76, linear_19_78, linear_20_79, linear_21_88, linear_23_91, linear_24_100
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.7.key:1
	Time elapsed:  1.445E-04s
	Output of modules: blocks.0.sa.heads.7.key
	Output of bottom-level module: blocks.0.sa.heads.7.key:1
	Lookup keys: -601, 97, blocks.0.sa.heads.7.key, blocks.0.sa.heads.7.key:1, linear_22, linear_22:1, linear_22_90, linear_22_90:1
--------------------------------------------
Layer linear_23_91, operation 91/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.0526, -0.2145,  0.0907, -0.2708,  0.4293,  0.9030, -0.5879, -0.5078],
        [-0.0234, -0.9977,  2.4400,  0.3789,  1.5237,  3.3193,  1.4901, -0.6928],
        [-0.2035, -0.0139,  2.5057, -0.6640, -0.4209, -0.8125, -1.0642, -1.0314],
        [-0.4618,  0.9202,  1.8456, -0.6180, -0.5037, -0.4035, -0.0386,  0.1179],
        [ 0.3727,  1.1991,  0.4293,  0.1970, -1.2247, -1.4422,  0.2238, -0.7672],
        [ 0.5331,  1.1467,  3.0373, -1.3208,  1.0664,  1.1926,  2.2131, -0.5517],
        [ 0.8212,  1.5575,  1.9396, -2.2548,  0.3688,  0.1494,  1.5725, -2.3328],
        [-1.4199,  0.0780,  0.1395,  0.1499, -0.1974, -0.5044, -1.5901,  1.4845]])...
	Related Layers:
		- parent layers: layernorm_1_5
		- child layers: matmul_15_93
		- shares parents with layers: linear_1_6, linear_2_7, linear_3_16, linear_4_18, linear_5_19, linear_6_28, linear_7_30, linear_8_31, linear_9_40, linear_10_42, linear_11_43, linear_12_52, linear_13_54, linear_14_55, linear_15_64, linear_16_66, linear_17_67, linear_18_76, linear_19_78, linear_20_79, linear_21_88, linear_22_90, linear_24_100
		- shares children with layers: transpose_8_92
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.7.query:1
	Time elapsed:  1.338E-04s
	Output of modules: blocks.0.sa.heads.7.query
	Output of bottom-level module: blocks.0.sa.heads.7.query:1
	Lookup keys: -600, 98, blocks.0.sa.heads.7.query, blocks.0.sa.heads.7.query:1, linear_23, linear_23:1, linear_23_91, linear_23_91:1
--------------------------------------------
Layer transpose_8_92, operation 92/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-3.7492, -0.9604,  0.7230,  2.0368,  1.9753,  2.6646, -1.2357, -0.3242],
        [-0.1843, -0.7854,  0.7118,  1.3613,  3.8312,  0.0429,  1.3769, -0.2501],
        [-1.1992, -1.2175,  0.3125, -0.3790,  1.0411, -0.8879, -1.0236,  0.9228],
        [ 0.6688, -0.7915, -0.5111,  0.3629,  1.7405,  1.0128,  0.4776,  0.7891],
        [ 1.0709, -0.3387, -0.2906, -0.4664, -0.8341, -0.7241, -0.2898, -0.4234],
        [-0.0678, -0.5947, -1.1642, -2.1893, -0.4709,  0.5790, -0.4754,  1.0876],
        [-0.1054, -1.2489, -1.1677, -0.3414,  1.3851, -0.7573, -1.1721, -0.0115],
        [ 1.1640, -0.0998, -0.9925, -1.6033,  0.6335, -0.1237,  1.6559,  0.6528]])...
	Related Layers:
		- parent layers: linear_22_90
		- child layers: matmul_15_93
		- shares parents with no other layers
		- shares children with layers: linear_23_91
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.0.sa.heads.7:1
	Time elapsed:  6.008E-05s
	Output of modules: none
	Lookup keys: -599, 99, transpose_8, transpose_8:1, transpose_8_92, transpose_8_92:1
--------------------------------------------
Layer matmul_15_93, operation 93/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-20.0020,  -7.2092,   6.3661,   7.1720,   4.1357,   0.9693, -16.3554,
          -1.9413],
        [-12.0874, -23.4057, -12.9388,  -4.1738,   5.8568, -51.3700, -44.2555,
         -15.9594],
        [-17.9003,  -1.0610,   4.3679,  12.4260,  -5.5987, -28.7806, -11.9260,
          -3.9435],
        [-34.2576, -25.8923,  20.9881,  17.0838,  11.1757, -42.2223, -15.6859,
         -11.5256],
        [-20.4464, -17.8053,  17.4312,  24.6900,   9.3832,   4.8293,  -0.6503,
           3.1066],
        [-23.7360, -36.7548, -28.5225,  -7.3936,  12.2729, -73.4039, -50.5210,
         -28.6942],
        [-16.1866,  -0.2456,   0.5690,  21.6642,   8.2209, -29.1925, -35.3596,
          -1.6715],
        [  7.5572,   5.6311,  -8.7901, -22.6876, -11.3037,   0.5371,  39.8772,
          -1.5082]])...
	Related Layers:
		- parent layers: linear_23_91, transpose_8_92
		- child layers: mul_8_94
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.7:1
	Time elapsed:  1.221E-04s
	Output of modules: none
	Lookup keys: -598, 100, matmul_15, matmul_15:1, matmul_15_93, matmul_15_93:1
--------------------------------------------
Layer mul_8_94, operation 94/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-1.0207, -0.3679,  0.3249,  0.3660,  0.2110,  0.0495, -0.8346, -0.0991],
        [-0.6168, -1.1944, -0.6603, -0.2130,  0.2989, -2.6215, -2.2584, -0.8144],
        [-0.9135, -0.0541,  0.2229,  0.6341, -0.2857, -1.4687, -0.6086, -0.2012],
        [-1.7482, -1.3213,  1.0710,  0.8718,  0.5703, -2.1546, -0.8005, -0.5882],
        [-1.0434, -0.9086,  0.8895,  1.2600,  0.4788,  0.2464, -0.0332,  0.1585],
        [-1.2113, -1.8756, -1.4555, -0.3773,  0.6263, -3.7459, -2.5781, -1.4643],
        [-0.8260, -0.0125,  0.0290,  1.1055,  0.4195, -1.4897, -1.8044, -0.0853],
        [ 0.3857,  0.2874, -0.4486, -1.1578, -0.5768,  0.0274,  2.0350, -0.0770]])...
	Related Layers:
		- parent layers: matmul_15_93
		- child layers: maskedfill_8_97
		- shares parents with no other layers
		- shares children with layers: eq_8_96
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.0.sa.heads.7:1
	Time elapsed:  8.893E-05s
	Output of modules: none
	Lookup keys: -597, 101, mul_8, mul_8:1, mul_8_94, mul_8_94:1
--------------------------------------------
Layer getitem_8_95, operation 95/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_8
		- child layers: eq_8_96
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  1.421E-04s
	Output of modules: none
	Lookup keys: -595, 103, getitem_8, getitem_8:1, getitem_8_95, getitem_8_95:1
--------------------------------------------
Layer eq_8_96, operation 96/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_8_95
		- child layers: maskedfill_8_97
		- shares parents with no other layers
		- shares children with layers: mul_8_94
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.012E-05s
	Output of modules: none
	Lookup keys: -594, 104, eq_8, eq_8:1, eq_8_96, eq_8_96:1
--------------------------------------------
Layer maskedfill_8_97, operation 97/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-1.0207,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-0.6168, -1.1944,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-0.9135, -0.0541,  0.2229,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-1.7482, -1.3213,  1.0710,  0.8718,    -inf,    -inf,    -inf,    -inf],
        [-1.0434, -0.9086,  0.8895,  1.2600,  0.4788,    -inf,    -inf,    -inf],
        [-1.2113, -1.8756, -1.4555, -0.3773,  0.6263, -3.7459,    -inf,    -inf],
        [-0.8260, -0.0125,  0.0290,  1.1055,  0.4195, -1.4897, -1.8044,    -inf],
        [ 0.3857,  0.2874, -0.4486, -1.1578, -0.5768,  0.0274,  2.0350, -0.0770]])...
	Related Layers:
		- parent layers: mul_8_94, eq_8_96
		- child layers: softmax_8_98
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.0.sa.heads.7:1
	Time elapsed:  9.632E-05s
	Output of modules: none
	Lookup keys: -593, 105, maskedfill_8, maskedfill_8:1, maskedfill_8_97, maskedfill_8_97:1
--------------------------------------------
Layer softmax_8_98, operation 98/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.6405, 0.3595, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1544, 0.3646, 0.4810, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0303, 0.0464, 0.5075, 0.4158, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0423, 0.0484, 0.2922, 0.4233, 0.1938, 0.0000, 0.0000, 0.0000],
        [0.0912, 0.0469, 0.0715, 0.2101, 0.5731, 0.0072, 0.0000, 0.0000],
        [0.0593, 0.1337, 0.1394, 0.4089, 0.2059, 0.0305, 0.0223, 0.0000],
        [0.1056, 0.0957, 0.0459, 0.0226, 0.0403, 0.0738, 0.5496, 0.0665]])...
	Related Layers:
		- parent layers: maskedfill_8_97
		- child layers: dropout_8_99
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.0.sa.heads.7:1
	Time elapsed:  8.631E-05s
	Output of modules: none
	Lookup keys: -592, 106, softmax_8, softmax_8:1, softmax_8_98, softmax_8_98:1
--------------------------------------------
Layer dropout_8_99, operation 99/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.6405, 0.3595, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1544, 0.3646, 0.4810, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0303, 0.0464, 0.5075, 0.4158, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0423, 0.0484, 0.2922, 0.4233, 0.1938, 0.0000, 0.0000, 0.0000],
        [0.0912, 0.0469, 0.0715, 0.2101, 0.5731, 0.0072, 0.0000, 0.0000],
        [0.0593, 0.1337, 0.1394, 0.4089, 0.2059, 0.0305, 0.0223, 0.0000],
        [0.1056, 0.0957, 0.0459, 0.0226, 0.0403, 0.0738, 0.5496, 0.0665]])...
	Related Layers:
		- parent layers: softmax_8_98
		- child layers: matmul_16_101
		- shares parents with no other layers
		- shares children with layers: linear_24_100
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.0.sa.heads.7.dropout:1
	Time elapsed:  5.841E-05s
	Output of modules: blocks.0.sa.heads.7.dropout
	Output of bottom-level module: blocks.0.sa.heads.7.dropout:1
	Lookup keys: -591, 107, blocks.0.sa.heads.7.dropout, blocks.0.sa.heads.7.dropout:1, dropout_8, dropout_8:1, dropout_8_99, dropout_8_99:1
--------------------------------------------
Layer linear_24_100, operation 100/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.3730,  0.9540, -1.8136,  1.5176,  0.5332,  0.2080, -0.9824,  1.3937],
        [-0.3059,  0.3647, -0.8114,  0.0696,  0.6155,  1.1908, -1.8129, -0.3535],
        [ 0.1710,  0.6859, -0.9901,  1.2863, -0.9470, -0.7058,  0.5389, -0.0441],
        [ 0.1989,  0.5822, -0.3998,  0.7928, -1.3617, -0.3609, -0.4568,  1.3908],
        [ 0.7538, -0.3690,  0.0038,  0.7353, -1.5894, -0.7779,  1.3348,  1.9441],
        [ 1.3138,  1.6725,  0.1693,  0.0086,  0.2752,  1.2567, -0.8824,  0.4594],
        [ 0.3062,  0.4493,  1.0435, -0.3297,  0.1984, -0.6757,  0.2865,  0.5025],
        [-0.2843,  0.5164, -1.5419,  0.0241, -0.2935, -0.2284,  0.4323,  0.3652]])...
	Related Layers:
		- parent layers: layernorm_1_5
		- child layers: matmul_16_101
		- shares parents with layers: linear_1_6, linear_2_7, linear_3_16, linear_4_18, linear_5_19, linear_6_28, linear_7_30, linear_8_31, linear_9_40, linear_10_42, linear_11_43, linear_12_52, linear_13_54, linear_14_55, linear_15_64, linear_16_66, linear_17_67, linear_18_76, linear_19_78, linear_20_79, linear_21_88, linear_22_90, linear_23_91
		- shares children with layers: dropout_8_99
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.7.value:1
	Time elapsed:  1.643E-04s
	Output of modules: blocks.0.sa.heads.7.value
	Output of bottom-level module: blocks.0.sa.heads.7.value:1
	Lookup keys: -590, 108, blocks.0.sa.heads.7.value, blocks.0.sa.heads.7.value:1, linear_24, linear_24:1, linear_24_100, linear_24_100:1
--------------------------------------------
Layer matmul_16_101, operation 101/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.3730,  0.9540, -1.8136,  1.5176,  0.5332,  0.2080, -0.9824,  1.3937],
        [ 0.7694,  0.7422, -1.4533,  0.9971,  0.5628,  0.5613, -1.2809,  0.7656],
        [ 0.1827,  0.6102, -1.0521,  0.8784, -0.1488,  0.1268, -0.5535,  0.0651],
        [ 0.1969,  0.6360, -0.7613,  1.0316, -1.0021, -0.4467, -0.0303,  0.5817],
        [ 0.3235,  0.4334, -0.5738,  0.9215, -1.1088, -0.4433,  0.0935,  0.9944],
        [ 0.6064,  0.0761, -0.3549,  0.8216, -1.1850, -0.4881,  0.5263,  1.5170],
        [ 0.3478,  0.4240, -0.4882,  0.7471, -0.8895, -0.2113, -0.1580,  1.0235],
        [ 0.4048,  0.5701,  0.1600,  0.0945,  0.0868, -0.2297, -0.0880,  0.5555]])...
	Related Layers:
		- parent layers: dropout_8_99, linear_24_100
		- child layers: cat_1_102
		- shares parents with no other layers
		- shares children with layers: matmul_2_17, matmul_4_29, matmul_6_41, matmul_8_53, matmul_10_65, matmul_12_77, matmul_14_89
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.0.sa.heads.7:1
	Time elapsed:  1.132E-04s
	Output of modules: blocks.0.sa.heads.7
	Lookup keys: -589, 109, blocks.0.sa.heads.7, blocks.0.sa.heads.7:1, matmul_16, matmul_16:1, matmul_16_101, matmul_16_101:1
--------------------------------------------
Layer cat_1_102, operation 102/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-0.7485,  1.7668,  2.7114, -0.3272, -0.4495, -1.4062, -0.5263, -0.2089],
        [ 0.0488,  0.5932,  1.8512,  0.3814, -0.5629, -1.4958, -0.0473,  0.7863],
        [-0.2104,  0.5040,  1.2285, -0.3021, -0.7507, -0.8758, -0.4382,  0.4668],
        [-0.4504,  0.5926,  1.2388, -0.2349, -0.9793, -0.3752, -0.6688, -0.1356],
        [-0.5666,  0.6140,  1.4047, -0.1179, -0.7375, -0.2558, -0.5647, -0.0216],
        [-0.7251,  0.6597,  1.6156,  0.0274, -0.4198, -0.1434, -0.4085,  0.1707],
        [-0.5795,  0.6088,  1.6096,  0.1727, -0.5878, -0.2633, -0.3608,  0.0820],
        [-0.5913,  0.4183,  1.1682,  0.0176, -0.7912, -0.0218, -0.0840, -0.1045]])...
	Related Layers:
		- parent layers: matmul_2_17, matmul_4_29, matmul_6_41, matmul_8_53, matmul_10_65, matmul_12_77, matmul_14_89, matmul_16_101
		- child layers: linear_25_103
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: cat (grad_fn: CatBackward0) 
	Computed inside module: blocks.0.sa:1
	Time elapsed:  1.428E-04s
	Output of modules: none
	Lookup keys: -588, 110, cat_1, cat_1:1, cat_1_102, cat_1_102:1
--------------------------------------------
Layer linear_25_103, operation 103/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-2.5518, -2.9512,  1.1676, -3.4907, -0.3170,  4.3211, -1.6654,  0.3714],
        [-1.8215, -3.6152,  1.2366, -3.1221, -0.7137,  3.8440, -1.3984,  0.2326],
        [-1.9169, -3.5051,  1.9257, -3.4905, -1.0329,  3.3075, -1.1466, -0.1326],
        [-1.9273, -3.7669,  2.0411, -3.8697, -0.7868,  2.9919, -1.2906,  0.0300],
        [-1.4752, -3.8986,  1.7151, -3.5353, -0.6862,  2.8976, -1.2537,  0.2775],
        [-0.7872, -2.1482,  0.7105, -2.5867, -0.1260,  2.9757, -0.9048,  0.3280],
        [-0.5487, -2.1807,  0.6931, -2.5043, -0.0686,  3.6586, -1.1576, -0.0984],
        [-1.6741, -3.2595,  1.6287, -3.0363, -0.0442,  2.3723, -0.9310, -0.2996]])...
	Related Layers:
		- parent layers: cat_1_102
		- child layers: dropout_9_104
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (384, 384), (384,); 147840 params total (577.7 KB)
	Function: linear (grad_fn: ViewBackward0) 
	Computed inside module: blocks.0.sa.proj:1
	Time elapsed:  1.022E-03s
	Output of modules: blocks.0.sa.proj
	Output of bottom-level module: blocks.0.sa.proj:1
	Lookup keys: -587, 111, blocks.0.sa.proj, blocks.0.sa.proj:1, linear_25, linear_25:1, linear_25_103, linear_25_103:1
--------------------------------------------
Layer dropout_9_104, operation 104/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-2.5518, -2.9512,  1.1676, -3.4907, -0.3170,  4.3211, -1.6654,  0.3714],
        [-1.8215, -3.6152,  1.2366, -3.1221, -0.7137,  3.8440, -1.3984,  0.2326],
        [-1.9169, -3.5051,  1.9257, -3.4905, -1.0329,  3.3075, -1.1466, -0.1326],
        [-1.9273, -3.7669,  2.0411, -3.8697, -0.7868,  2.9919, -1.2906,  0.0300],
        [-1.4752, -3.8986,  1.7151, -3.5353, -0.6862,  2.8976, -1.2537,  0.2775],
        [-0.7872, -2.1482,  0.7105, -2.5867, -0.1260,  2.9757, -0.9048,  0.3280],
        [-0.5487, -2.1807,  0.6931, -2.5043, -0.0686,  3.6586, -1.1576, -0.0984],
        [-1.6741, -3.2595,  1.6287, -3.0363, -0.0442,  2.3723, -0.9310, -0.2996]])...
	Related Layers:
		- parent layers: linear_25_103
		- child layers: add_2_105:1
		- shares parents with no other layers
		- shares children with layers: add_1_4
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: ViewBackward0) 
	Computed inside module: blocks.0.sa.dropout:1
	Time elapsed:  7.963E-05s
	Output of modules: blocks.0.sa.dropout, blocks.0.sa
	Output of bottom-level module: blocks.0.sa.dropout:1
	Lookup keys: -586, 112, blocks.0.sa, blocks.0.sa.dropout, blocks.0.sa.dropout:1, blocks.0.sa:1, dropout_9, dropout_9:1, dropout_9_104, dropout_9_104:1
--------------------------------------------
Layer add_2_105 (pass 1/2), operation 105/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-2.8193, -2.5355,  0.9694, -3.2006, -1.3149,  6.2642, -0.1899,  1.3317],
        [-1.5858, -5.4525,  2.4953, -1.3269, -2.5863,  3.3435, -1.2560, -1.0358],
        [-1.9387, -4.5045,  0.6639, -2.2306, -1.6205,  3.9233, -1.1974, -1.2967],
        [-1.4708, -4.0944,  2.6068, -3.7077, -0.6562,  3.7679, -0.2987,  0.3209],
        [ 0.5688, -3.5041,  0.7451, -4.7865,  1.4301,  1.3897, -0.3017,  1.0912],
        [ 0.3218, -0.5172, -1.2781, -2.0078, -1.3603,  3.5006, -2.9308,  0.8727],
        [-1.8224, -2.3448,  1.4574, -3.1549, -0.2277,  6.7659, -2.0243, -1.4078],
        [-4.1850, -1.9900,  2.1060, -4.2413, -1.3200,  2.3884, -4.9665, -1.4444]])...
	Related Layers:
		- parent layers: add_1_4, dropout_9_104
		- child layers: layernorm_2_106, add_2_105:2
		- shares parents with layers: layernorm_1_5
		- shares children with layers: dropout_10_110
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __add__ (grad_fn: AddBackward0) 
	Computed inside module: blocks.0:1
	Time elapsed:  9.131E-05s
	Output of modules: none
	Lookup keys: -585, 113, add_2:1, add_2_105:1
--------------------------------------------
Layer layernorm_2_106, operation 106/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-1.1417, -0.9872,  0.2681, -1.2451, -0.5353,  2.1907, -0.1508,  0.4134],
        [-0.6988, -2.2425,  1.0074, -0.5746, -1.0614,  1.3540, -0.5354, -0.4449],
        [-0.8742, -1.9469,  0.2942, -0.9781, -0.6824,  1.7272, -0.5148, -0.5590],
        [-0.6727, -1.7584,  1.1046, -1.6193, -0.2831,  1.6009, -0.1494,  0.1279],
        [ 0.3053, -1.5690,  0.3657, -2.1846,  0.6899,  0.6618, -0.1163,  0.5393],
        [ 0.1501, -0.2476, -0.6101, -0.9433, -0.6169,  1.6061, -1.3444,  0.4034],
        [-0.8698, -1.0684,  0.6675, -1.4592, -0.0914,  3.1133, -0.9212, -0.6465],
        [-2.0123, -0.9206,  0.9469, -1.9688, -0.6023,  1.0729, -2.2578, -0.6780]])...
	Related Layers:
		- parent layers: add_2_105:1
		- child layers: linear_26_107
		- shares parents with layers: add_2_105:2
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (384,), (384,); 768 params total (3.2 KB)
	Function: layer_norm (grad_fn: NativeLayerNormBackward0) 
	Computed inside module: blocks.0.ln2:1
	Time elapsed:  2.882E-04s
	Output of modules: blocks.0.ln2
	Output of bottom-level module: blocks.0.ln2:1
	Lookup keys: -584, 114, blocks.0.ln2, blocks.0.ln2:1, layernorm_2, layernorm_2:1, layernorm_2_106, layernorm_2_106:1
--------------------------------------------
Layer linear_26_107, operation 107/648:
	Output tensor: shape=1x11x1536(67584 bytes), dype=torch.float32, Tensor size=67728 bytes, Tensor storage size=67648
		tensor([[-0.8447,  0.1944,  0.7592,  0.5438, -0.2072, -0.0866, -0.5591,  0.8152],
        [-0.6223, -0.1902, -0.8890,  0.8013, -1.1700, -0.1284, -1.1690,  0.6895],
        [-0.7464,  0.6083, -0.2962,  0.5679, -0.2777, -0.5627, -1.1903,  0.1255],
        [-0.6916,  1.0466, -0.3053,  0.1635, -0.1945, -1.4419, -0.8553,  0.1346],
        [-1.0882,  0.5634, -0.3397, -0.7201, -0.1316, -0.6501, -0.6221,  0.1940],
        [-0.0856,  0.2112, -0.8503,  0.6471, -0.3679,  1.4034,  0.0371,  0.1104],
        [ 0.3241,  0.1536, -0.5355,  0.1122, -0.5506,  2.0765,  2.5700,  0.0328],
        [-0.3383, -0.2493, -0.9514,  1.3710, -1.0068, -0.8594, -1.1524, -0.0240]])...
	Related Layers:
		- parent layers: layernorm_2_106
		- child layers: relu_1_108
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (1536, 384), (1536,); 591360 params total (2.3 MB)
	Function: linear (grad_fn: ViewBackward0) 
	Computed inside module: blocks.0.ffwd.net.0:1
	Time elapsed:  5.934E-04s
	Output of modules: blocks.0.ffwd.net.0
	Output of bottom-level module: blocks.0.ffwd.net.0:1
	Lookup keys: -583, 115, blocks.0.ffwd.net.0, blocks.0.ffwd.net.0:1, linear_26, linear_26:1, linear_26_107, linear_26_107:1
--------------------------------------------
Layer relu_1_108, operation 108/648:
	Output tensor: shape=1x11x1536(67584 bytes), dype=torch.float32, Tensor size=67728 bytes, Tensor storage size=67648
		tensor([[0.0000, 0.1944, 0.7592, 0.5438, 0.0000, 0.0000, 0.0000, 0.8152],
        [0.0000, 0.0000, 0.0000, 0.8013, 0.0000, 0.0000, 0.0000, 0.6895],
        [0.0000, 0.6083, 0.0000, 0.5679, 0.0000, 0.0000, 0.0000, 0.1255],
        [0.0000, 1.0466, 0.0000, 0.1635, 0.0000, 0.0000, 0.0000, 0.1346],
        [0.0000, 0.5634, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1940],
        [0.0000, 0.2112, 0.0000, 0.6471, 0.0000, 1.4034, 0.0371, 0.1104],
        [0.3241, 0.1536, 0.0000, 0.1122, 0.0000, 2.0765, 2.5700, 0.0328],
        [0.0000, 0.0000, 0.0000, 1.3710, 0.0000, 0.0000, 0.0000, 0.0000]])...
	Related Layers:
		- parent layers: linear_26_107
		- child layers: linear_27_109
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: relu (grad_fn: ReluBackward0) 
	Computed inside module: blocks.0.ffwd.net.1:1
	Time elapsed:  2.780E-04s
	Output of modules: blocks.0.ffwd.net.1
	Output of bottom-level module: blocks.0.ffwd.net.1:1
	Lookup keys: -582, 116, blocks.0.ffwd.net.1, blocks.0.ffwd.net.1:1, relu_1, relu_1:1, relu_1_108, relu_1_108:1
--------------------------------------------
Layer linear_27_109, operation 109/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-2.4301, -2.5156,  2.4304,  0.3663,  1.8254,  0.3577,  0.6335,  2.6160],
        [-1.1275, -2.1289,  2.0360, -0.1497,  1.7887,  0.7289,  0.3916,  2.7737],
        [-1.9790, -2.9666,  2.9713, -0.2881, -0.5659, -1.4685,  0.5868,  1.7313],
        [-1.9342, -3.1701,  3.0318, -0.4883, -0.2549, -1.6151,  0.2304,  1.9014],
        [-2.6130, -2.6512,  3.5409,  0.4508,  1.8612, -0.2696,  0.6621,  2.5482],
        [-0.5362, -1.8803,  1.3070,  0.5320,  2.0019,  0.6422,  0.1513,  2.2169],
        [-1.8072, -0.8156,  2.0673,  0.8029,  2.9158,  2.1701,  0.8960,  1.2265],
        [-2.1078, -1.8918,  2.5104,  0.0715,  1.9424, -0.3894,  0.1580,  2.4668]])...
	Related Layers:
		- parent layers: relu_1_108
		- child layers: dropout_10_110
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (384, 1536), (384,); 590208 params total (2.3 MB)
	Function: linear (grad_fn: ViewBackward0) 
	Computed inside module: blocks.0.ffwd.net.2:1
	Time elapsed:  6.909E-04s
	Output of modules: blocks.0.ffwd.net.2
	Output of bottom-level module: blocks.0.ffwd.net.2:1
	Lookup keys: -581, 117, blocks.0.ffwd.net.2, blocks.0.ffwd.net.2:1, linear_27, linear_27:1, linear_27_109, linear_27_109:1
--------------------------------------------
Layer dropout_10_110, operation 110/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-2.4301, -2.5156,  2.4304,  0.3663,  1.8254,  0.3577,  0.6335,  2.6160],
        [-1.1275, -2.1289,  2.0360, -0.1497,  1.7887,  0.7289,  0.3916,  2.7737],
        [-1.9790, -2.9666,  2.9713, -0.2881, -0.5659, -1.4685,  0.5868,  1.7313],
        [-1.9342, -3.1701,  3.0318, -0.4883, -0.2549, -1.6151,  0.2304,  1.9014],
        [-2.6130, -2.6512,  3.5409,  0.4508,  1.8612, -0.2696,  0.6621,  2.5482],
        [-0.5362, -1.8803,  1.3070,  0.5320,  2.0019,  0.6422,  0.1513,  2.2169],
        [-1.8072, -0.8156,  2.0673,  0.8029,  2.9158,  2.1701,  0.8960,  1.2265],
        [-2.1078, -1.8918,  2.5104,  0.0715,  1.9424, -0.3894,  0.1580,  2.4668]])...
	Related Layers:
		- parent layers: linear_27_109
		- child layers: add_2_105:2
		- shares parents with no other layers
		- shares children with layers: add_2_105:1
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: ViewBackward0) 
	Computed inside module: blocks.0.ffwd.net.3:1
	Time elapsed:  6.700E-05s
	Output of modules: blocks.0.ffwd.net.3, blocks.0.ffwd.net, blocks.0.ffwd
	Output of bottom-level module: blocks.0.ffwd.net.3:1
	Lookup keys: -580, 118, blocks.0.ffwd, blocks.0.ffwd.net, blocks.0.ffwd.net.3, blocks.0.ffwd.net.3:1, blocks.0.ffwd.net:1, blocks.0.ffwd:1, dropout_10, dropout_10:1, dropout_10_110, dropout_10_110:1
--------------------------------------------
Layer add_2_105 (pass 2/2), operation 111/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-5.2494, -5.0511,  3.3998, -2.8343,  0.5105,  6.6219,  0.4436,  3.9476],
        [-2.7134, -7.5815,  4.5313, -1.4766, -0.7976,  4.0725, -0.8644,  1.7379],
        [-3.9177, -7.4711,  3.6352, -2.5186, -2.1864,  2.4549, -0.6107,  0.4346],
        [-3.4050, -7.2645,  5.6386, -4.1960, -0.9111,  2.1529, -0.0683,  2.2223],
        [-2.0443, -6.1553,  4.2860, -4.3357,  3.2913,  1.1201,  0.3604,  3.6394],
        [-0.2144, -2.3975,  0.0289, -1.4758,  0.6417,  4.1428, -2.7794,  3.0896],
        [-3.6296, -3.1604,  3.5246, -2.3521,  2.6881,  8.9360, -1.1283, -0.1814],
        [-6.2928, -3.8818,  4.6165, -4.1698,  0.6224,  1.9990, -4.8085,  1.0224]])...
	Related Layers:
		- parent layers: add_2_105:1, dropout_10_110
		- child layers: layernorm_3_111, add_3_211:1
		- shares parents with layers: layernorm_2_106
		- shares children with layers: dropout_19_210
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __add__ (grad_fn: AddBackward0) 
	Computed inside module: blocks.0:1
	Time elapsed:  8.535E-05s
	Output of modules: blocks.0
	Lookup keys: -579, 119, add_2:2, add_2_105:2, blocks.0, blocks.0:1
--------------------------------------------
Layer layernorm_3_111, operation 112/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-1.5815, -1.5340,  0.9294, -0.9158,  0.0934,  1.9130,  0.0672,  1.0996],
        [-0.8862, -2.4680,  1.4619, -0.5103, -0.2699,  1.3394, -0.3011,  0.5552],
        [-1.3313, -2.5448,  1.2315, -0.8918, -0.7560,  0.8473, -0.2219,  0.1395],
        [-1.1745, -2.4879,  1.8940, -1.4922, -0.3359,  0.7199, -0.0537,  0.7325],
        [-0.6725, -2.0703,  1.4744, -1.4966,  1.1516,  0.4055,  0.1388,  1.2625],
        [-0.0789, -0.8356, -0.0059, -0.5346,  0.2213,  1.4527, -0.9865,  1.0664],
        [-1.0713, -0.9409,  1.0640, -0.7200,  0.8278,  2.7459, -0.3390, -0.0495],
        [-2.3739, -1.4796,  1.7328, -1.6325,  0.2271,  0.7564, -1.8693,  0.3722]])...
	Related Layers:
		- parent layers: add_2_105:2
		- child layers: linear_28_112, linear_29_113, linear_30_122, linear_31_124, linear_32_125, linear_33_134, linear_34_136, linear_35_137, linear_36_146, linear_37_148, linear_38_149, linear_39_158, linear_40_160, linear_41_161, linear_42_170, linear_43_172, linear_44_173, linear_45_182, linear_46_184, linear_47_185, linear_48_194, linear_49_196, linear_50_197, linear_51_206
		- shares parents with layers: add_3_211:1
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (384,), (384,); 768 params total (3.2 KB)
	Function: layer_norm (grad_fn: NativeLayerNormBackward0) 
	Computed inside module: blocks.1.ln1:1
	Time elapsed:  2.787E-04s
	Output of modules: blocks.1.ln1
	Output of bottom-level module: blocks.1.ln1:1
	Lookup keys: -578, 120, blocks.1.ln1, blocks.1.ln1:1, layernorm_3, layernorm_3:1, layernorm_3_111, layernorm_3_111:1
--------------------------------------------
Layer linear_28_112, operation 113/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 3.2559,  4.5158, -1.2818,  2.9067, -4.6409, -3.8223,  3.3517,  3.2364],
        [ 2.2899,  3.7072, -0.3987,  2.1663, -4.4407, -2.2830,  2.7083,  2.1267],
        [ 2.4200,  4.0922, -2.3740,  1.9809, -3.6214, -2.9862,  2.6974,  2.1474],
        [ 3.2899,  4.0484, -2.0842,  2.5231, -3.3055, -2.9613,  3.0174,  2.0884],
        [ 2.6760,  3.8271, -2.4486,  1.8545, -2.7807, -4.0703,  2.0613,  2.6922],
        [ 3.0384,  3.1662, -0.1218,  5.7368, -7.1437, -3.9294,  4.1465,  4.2046],
        [ 2.2148,  2.6600, -1.3686,  3.7182, -6.1197, -3.3361,  3.5129,  3.5162],
        [ 2.7483,  3.7453, -2.4585,  1.9075, -3.7092, -2.1631,  2.2770,  2.2155]])...
	Related Layers:
		- parent layers: layernorm_3_111
		- child layers: transpose_9_114
		- shares parents with layers: linear_29_113, linear_30_122, linear_31_124, linear_32_125, linear_33_134, linear_34_136, linear_35_137, linear_36_146, linear_37_148, linear_38_149, linear_39_158, linear_40_160, linear_41_161, linear_42_170, linear_43_172, linear_44_173, linear_45_182, linear_46_184, linear_47_185, linear_48_194, linear_49_196, linear_50_197, linear_51_206
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.0.key:1
	Time elapsed:  1.566E-04s
	Output of modules: blocks.1.sa.heads.0.key
	Output of bottom-level module: blocks.1.sa.heads.0.key:1
	Lookup keys: -577, 121, blocks.1.sa.heads.0.key, blocks.1.sa.heads.0.key:1, linear_28, linear_28:1, linear_28_112, linear_28_112:1
--------------------------------------------
Layer linear_29_113, operation 114/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.5497, -1.1301, -0.1014,  0.6791,  0.7871,  1.0465,  0.0394, -0.6898],
        [-0.7488, -0.7641, -0.5978,  0.1702,  2.8617,  0.6302, -1.1832, -1.0800],
        [-0.2650, -1.3058, -0.0182,  0.3465,  0.6765,  0.3357, -0.4179, -0.6112],
        [-0.1468, -0.4611, -0.4343, -0.0542,  0.9410,  0.3096, -0.4721, -0.4018],
        [-0.6961, -2.4911,  0.1074,  1.0127,  0.4826,  0.6065, -0.3224, -0.0370],
        [-1.1419, -0.3865,  0.7665, -1.6501,  3.3108,  3.2296, -0.2038, -3.1189],
        [-1.3552,  0.2807,  0.3474, -2.6744,  3.5922,  2.6148, -1.0711, -3.7151],
        [-1.3611, -0.8967,  0.1180, -0.0934,  0.5331,  0.2360, -0.3783, -1.0195]])...
	Related Layers:
		- parent layers: layernorm_3_111
		- child layers: matmul_17_115
		- shares parents with layers: linear_28_112, linear_30_122, linear_31_124, linear_32_125, linear_33_134, linear_34_136, linear_35_137, linear_36_146, linear_37_148, linear_38_149, linear_39_158, linear_40_160, linear_41_161, linear_42_170, linear_43_172, linear_44_173, linear_45_182, linear_46_184, linear_47_185, linear_48_194, linear_49_196, linear_50_197, linear_51_206
		- shares children with layers: transpose_9_114
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.0.query:1
	Time elapsed:  1.371E-04s
	Output of modules: blocks.1.sa.heads.0.query
	Output of bottom-level module: blocks.1.sa.heads.0.query:1
	Lookup keys: -576, 122, blocks.1.sa.heads.0.query, blocks.1.sa.heads.0.query:1, linear_29, linear_29:1, linear_29_113, linear_29_113:1
--------------------------------------------
Layer transpose_9_114, operation 115/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 3.2559,  2.2899,  2.4200,  3.2899,  2.6760,  3.0384,  2.2148,  2.7483],
        [ 4.5158,  3.7072,  4.0922,  4.0484,  3.8271,  3.1662,  2.6600,  3.7453],
        [-1.2818, -0.3987, -2.3740, -2.0842, -2.4486, -0.1218, -1.3686, -2.4585],
        [ 2.9067,  2.1663,  1.9809,  2.5231,  1.8545,  5.7368,  3.7182,  1.9075],
        [-4.6409, -4.4407, -3.6214, -3.3055, -2.7807, -7.1437, -6.1197, -3.7092],
        [-3.8223, -2.2830, -2.9862, -2.9613, -4.0703, -3.9294, -3.3361, -2.1631],
        [ 3.3517,  2.7083,  2.6974,  3.0174,  2.0613,  4.1465,  3.5129,  2.2770],
        [ 3.2364,  2.1267,  2.1474,  2.0884,  2.6922,  4.2046,  3.5162,  2.2155]])...
	Related Layers:
		- parent layers: linear_28_112
		- child layers: matmul_17_115
		- shares parents with no other layers
		- shares children with layers: linear_29_113
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.1.sa.heads.0:1
	Time elapsed:  6.080E-05s
	Output of modules: none
	Lookup keys: -575, 123, transpose_9, transpose_9:1, transpose_9_114, transpose_9_114:1
--------------------------------------------
Layer matmul_17_115, operation 116/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -89.3706,  -77.3598,  -66.3041,  -68.2578,  -62.2049, -100.1582,
          -83.2224,  -58.3101],
        [-145.5183, -133.0145, -110.1856, -115.5170, -104.5312, -176.3825,
         -146.4597,  -99.1780],
        [-112.2316, -101.7912,  -87.7994,  -91.1948,  -81.4128, -128.3788,
         -106.2269,  -75.2747],
        [ -78.1641,  -71.3240,  -56.9528,  -59.3219,  -48.1258,  -98.0927,
          -78.6399,  -46.1212],
        [ -79.5689,  -71.4683,  -62.8905,  -64.5719,  -57.1191,  -75.6617,
          -67.1620,  -53.0780],
        [-275.1425, -220.5691, -213.7152, -220.4099, -226.8215, -328.6180,
         -269.0308, -207.9220],
        [-288.5507, -241.2568, -220.5444, -230.4466, -229.8908, -359.1442,
         -293.7980, -215.7964],
        [-115.4460, -100.0313,  -89.6787,  -93.8715,  -89.4730, -122.8250,
         -100.6858,  -76.5318]])...
	Related Layers:
		- parent layers: linear_29_113, transpose_9_114
		- child layers: mul_9_116
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.0:1
	Time elapsed:  1.245E-04s
	Output of modules: none
	Lookup keys: -574, 124, matmul_17, matmul_17:1, matmul_17_115, matmul_17_115:1
--------------------------------------------
Layer mul_9_116, operation 117/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -4.5607,  -3.9478,  -3.3836,  -3.4833,  -3.1744,  -5.1112,  -4.2469,
          -2.9756],
        [ -7.4259,  -6.7879,  -5.6229,  -5.8950,  -5.3343,  -9.0010,  -7.4740,
          -5.0612],
        [ -5.7273,  -5.1945,  -4.4805,  -4.6538,  -4.1546,  -6.5513,  -5.4209,
          -3.8413],
        [ -3.9888,  -3.6397,  -2.9064,  -3.0273,  -2.4559,  -5.0058,  -4.0131,
          -2.3536],
        [ -4.0605,  -3.6471,  -3.2094,  -3.2952,  -2.9148,  -3.8611,  -3.4273,
          -2.7086],
        [-14.0408, -11.2559, -10.9061, -11.2477, -11.5749, -16.7697, -13.7289,
         -10.6105],
        [-14.7250, -12.3116, -11.2546, -11.7599, -11.7316, -18.3275, -14.9928,
         -11.0123],
        [ -5.8913,  -5.1047,  -4.5764,  -4.7904,  -4.5659,  -6.2679,  -5.1381,
          -3.9055]])...
	Related Layers:
		- parent layers: matmul_17_115
		- child layers: maskedfill_9_119
		- shares parents with no other layers
		- shares children with layers: eq_9_118
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.1.sa.heads.0:1
	Time elapsed:  8.750E-05s
	Output of modules: none
	Lookup keys: -573, 125, mul_9, mul_9:1, mul_9_116, mul_9_116:1
--------------------------------------------
Layer getitem_9_117, operation 118/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_9
		- child layers: eq_9_118
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  1.359E-04s
	Output of modules: none
	Lookup keys: -571, 127, getitem_9, getitem_9:1, getitem_9_117, getitem_9_117:1
--------------------------------------------
Layer eq_9_118, operation 119/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_9_117
		- child layers: maskedfill_9_119
		- shares parents with no other layers
		- shares children with layers: mul_9_116
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  1.004E-04s
	Output of modules: none
	Lookup keys: -570, 128, eq_9, eq_9:1, eq_9_118, eq_9_118:1
--------------------------------------------
Layer maskedfill_9_119, operation 120/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -4.5607,     -inf,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -7.4259,  -6.7879,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -5.7273,  -5.1945,  -4.4805,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -3.9888,  -3.6397,  -2.9064,  -3.0273,     -inf,     -inf,     -inf,
             -inf],
        [ -4.0605,  -3.6471,  -3.2094,  -3.2952,  -2.9148,     -inf,     -inf,
             -inf],
        [-14.0408, -11.2559, -10.9061, -11.2477, -11.5749, -16.7697,     -inf,
             -inf],
        [-14.7250, -12.3116, -11.2546, -11.7599, -11.7316, -18.3275, -14.9928,
             -inf],
        [ -5.8913,  -5.1047,  -4.5764,  -4.7904,  -4.5659,  -6.2679,  -5.1381,
          -3.9055]])...
	Related Layers:
		- parent layers: mul_9_116, eq_9_118
		- child layers: softmax_9_120
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.1.sa.heads.0:1
	Time elapsed:  9.370E-05s
	Output of modules: none
	Lookup keys: -569, 129, maskedfill_9, maskedfill_9:1, maskedfill_9_119, maskedfill_9_119:1
--------------------------------------------
Layer softmax_9_120, operation 121/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [3.4568e-01, 6.5432e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.6174e-01, 2.7555e-01, 5.6272e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.2523e-01, 1.7754e-01, 3.6966e-01, 3.2757e-01, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [9.8539e-02, 1.4898e-01, 2.3080e-01, 2.1183e-01, 3.0985e-01, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.4630e-02, 2.3700e-01, 3.3623e-01, 2.3893e-01, 1.7226e-01, 9.5523e-04,
         0.0000e+00, 0.0000e+00],
        [1.1839e-02, 1.3227e-01, 3.8063e-01, 2.2964e-01, 2.3624e-01, 3.2269e-04,
         9.0577e-03, 0.0000e+00],
        [4.2041e-02, 9.2320e-02, 1.5658e-01, 1.2642e-01, 1.5823e-01, 2.8849e-02,
         8.9288e-02, 3.0627e-01]])...
	Related Layers:
		- parent layers: maskedfill_9_119
		- child layers: dropout_11_121
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.1.sa.heads.0:1
	Time elapsed:  7.772E-05s
	Output of modules: none
	Lookup keys: -568, 130, softmax_9, softmax_9:1, softmax_9_120, softmax_9_120:1
--------------------------------------------
Layer dropout_11_121, operation 122/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [3.4568e-01, 6.5432e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.6174e-01, 2.7555e-01, 5.6272e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.2523e-01, 1.7754e-01, 3.6966e-01, 3.2757e-01, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [9.8539e-02, 1.4898e-01, 2.3080e-01, 2.1183e-01, 3.0985e-01, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.4630e-02, 2.3700e-01, 3.3623e-01, 2.3893e-01, 1.7226e-01, 9.5523e-04,
         0.0000e+00, 0.0000e+00],
        [1.1839e-02, 1.3227e-01, 3.8063e-01, 2.2964e-01, 2.3624e-01, 3.2269e-04,
         9.0577e-03, 0.0000e+00],
        [4.2041e-02, 9.2320e-02, 1.5658e-01, 1.2642e-01, 1.5823e-01, 2.8849e-02,
         8.9288e-02, 3.0627e-01]])...
	Related Layers:
		- parent layers: softmax_9_120
		- child layers: matmul_18_123
		- shares parents with no other layers
		- shares children with layers: linear_30_122
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.1.sa.heads.0.dropout:1
	Time elapsed:  5.603E-05s
	Output of modules: blocks.1.sa.heads.0.dropout
	Output of bottom-level module: blocks.1.sa.heads.0.dropout:1
	Lookup keys: -567, 131, blocks.1.sa.heads.0.dropout, blocks.1.sa.heads.0.dropout:1, dropout_11, dropout_11:1, dropout_11_121, dropout_11_121:1
--------------------------------------------
Layer linear_30_122, operation 123/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-1.6575e+00,  1.3522e-01,  7.6760e-01,  3.3121e-01, -9.9546e-01,
         -1.7543e-01,  3.3236e-01, -4.7132e-01],
        [-9.5963e-01,  5.3955e-01,  6.7620e-01,  1.4024e-01, -7.7392e-01,
         -3.8538e-01, -1.9130e-01,  1.0029e+00],
        [-7.3211e-01, -8.3895e-01,  4.8927e-01,  1.2175e-01, -4.6536e-01,
         -1.2657e-01,  4.1840e-01,  6.2268e-01],
        [-4.4139e-01, -8.5592e-01,  4.8909e-01, -1.9360e-01, -8.1506e-01,
          7.7913e-02,  4.1780e-01,  6.5029e-01],
        [-1.0848e+00, -2.3859e-01,  7.9278e-01,  3.1943e-01, -9.2734e-01,
         -6.2663e-01,  2.2630e-01,  7.6068e-01],
        [-6.4749e-01, -1.2598e-01,  8.2516e-01, -3.3779e-01, -7.2621e-01,
          7.0861e-02,  7.7339e-04, -3.9913e-01],
        [-1.6446e+00,  6.6292e-01,  1.5372e+00, -4.3606e-01, -9.4698e-02,
          1.1765e+00, -9.8313e-01, -9.7521e-01],
        [-8.4685e-01,  6.7465e-02,  2.6638e-01, -5.2024e-02, -7.8256e-01,
          2.5410e-01,  4.4189e-01,  7.5224e-01]])...
	Related Layers:
		- parent layers: layernorm_3_111
		- child layers: matmul_18_123
		- shares parents with layers: linear_28_112, linear_29_113, linear_31_124, linear_32_125, linear_33_134, linear_34_136, linear_35_137, linear_36_146, linear_37_148, linear_38_149, linear_39_158, linear_40_160, linear_41_161, linear_42_170, linear_43_172, linear_44_173, linear_45_182, linear_46_184, linear_47_185, linear_48_194, linear_49_196, linear_50_197, linear_51_206
		- shares children with layers: dropout_11_121
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.0.value:1
	Time elapsed:  1.464E-04s
	Output of modules: blocks.1.sa.heads.0.value
	Output of bottom-level module: blocks.1.sa.heads.0.value:1
	Lookup keys: -566, 132, blocks.1.sa.heads.0.value, blocks.1.sa.heads.0.value:1, linear_30, linear_30:1, linear_30_122, linear_30_122:1
--------------------------------------------
Layer matmul_18_123, operation 124/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-1.6575,  0.1352,  0.7676,  0.3312, -0.9955, -0.1754,  0.3324, -0.4713],
        [-1.2009,  0.3998,  0.7078,  0.2063, -0.8505, -0.3128, -0.0103,  0.4933],
        [-0.9445, -0.3015,  0.5858,  0.1607, -0.6361, -0.2058,  0.2365,  0.5505],
        [-0.7932, -0.4778,  0.5573,  0.0480, -0.7011, -0.1117,  0.2992,  0.5622],
        [-0.9049, -0.3552,  0.6385,  0.1396, -0.7808, -0.2816,  0.2594,  0.6201],
        [-0.7908, -0.3980,  0.5902,  0.0875, -0.7096, -0.2257,  0.2390,  0.7262],
        [-0.7980, -0.4933,  0.5985,  0.0958, -0.6986, -0.2207,  0.2784,  0.6842],
        [-0.9253, -0.1456,  0.6012,  0.0074, -0.7050,  0.0328,  0.1980,  0.5046]])...
	Related Layers:
		- parent layers: dropout_11_121, linear_30_122
		- child layers: cat_2_208
		- shares parents with no other layers
		- shares children with layers: matmul_20_135, matmul_22_147, matmul_24_159, matmul_26_171, matmul_28_183, matmul_30_195, matmul_32_207
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.0:1
	Time elapsed:  9.418E-05s
	Output of modules: blocks.1.sa.heads.0
	Lookup keys: -565, 133, blocks.1.sa.heads.0, blocks.1.sa.heads.0:1, matmul_18, matmul_18:1, matmul_18_123, matmul_18_123:1
--------------------------------------------
Layer linear_31_124, operation 125/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 5.0796e-02,  5.4887e+00, -2.9528e+00,  4.7936e+00, -4.8896e+00,
          6.1442e-01, -5.0028e+00,  4.5328e+00],
        [ 2.5503e-01,  4.2673e+00, -2.4595e+00,  4.8064e+00, -4.4627e+00,
          1.8789e+00, -4.9687e+00,  3.3106e+00],
        [-1.7166e-01,  3.8111e+00, -2.4736e+00,  3.8432e+00, -4.6286e+00,
          1.1282e+00, -4.3564e+00,  2.9377e+00],
        [-2.1841e-01,  4.1037e+00, -2.1063e+00,  3.6994e+00, -5.1594e+00,
          1.4710e+00, -4.3661e+00,  3.2993e+00],
        [-3.7702e-01,  3.2762e+00, -2.2167e+00,  3.5423e+00, -4.1216e+00,
          5.6769e-01, -3.3896e+00,  3.9186e+00],
        [ 1.0970e+00,  5.7794e+00, -4.0667e+00,  5.1734e+00, -4.9350e+00,
         -5.1194e-01, -5.8990e+00,  4.8564e+00],
        [-2.5489e-01,  4.7667e+00, -3.0141e+00,  3.5579e+00, -3.9807e+00,
         -5.8223e-03, -4.9259e+00,  3.9250e+00],
        [-2.7599e-02,  3.5138e+00, -2.0142e+00,  3.8923e+00, -4.1526e+00,
          1.4524e+00, -4.4593e+00,  2.9211e+00]])...
	Related Layers:
		- parent layers: layernorm_3_111
		- child layers: transpose_10_126
		- shares parents with layers: linear_28_112, linear_29_113, linear_30_122, linear_32_125, linear_33_134, linear_34_136, linear_35_137, linear_36_146, linear_37_148, linear_38_149, linear_39_158, linear_40_160, linear_41_161, linear_42_170, linear_43_172, linear_44_173, linear_45_182, linear_46_184, linear_47_185, linear_48_194, linear_49_196, linear_50_197, linear_51_206
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.1.key:1
	Time elapsed:  1.121E-04s
	Output of modules: blocks.1.sa.heads.1.key
	Output of bottom-level module: blocks.1.sa.heads.1.key:1
	Lookup keys: -564, 134, blocks.1.sa.heads.1.key, blocks.1.sa.heads.1.key:1, linear_31, linear_31:1, linear_31_124, linear_31_124:1
--------------------------------------------
Layer linear_32_125, operation 126/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-1.0632, -0.5211,  0.6755,  1.5554, -0.2213,  0.1491, -0.2819, -0.6837],
        [-0.1891,  0.3050,  0.6430,  1.1279, -1.8626,  0.7986, -0.4784, -0.6109],
        [-1.0571,  0.1512,  0.4734,  1.6853, -0.8272,  0.5403, -0.5801, -1.3547],
        [-1.0099, -0.1750,  1.0458,  2.0055, -0.8081,  0.1577, -0.4609, -1.7235],
        [-0.8675,  0.1206,  0.1357,  1.8876,  0.0207, -0.2449, -0.2958, -0.2912],
        [ 0.4381,  0.1333,  0.4685,  1.6491, -1.0355,  1.3803,  1.6113, -2.5026],
        [ 0.7751, -0.8720, -0.1189,  0.7757, -1.6302,  1.1168,  0.1235, -0.8335],
        [-1.2360, -0.3215,  0.2650,  0.4856,  0.4620, -0.2258, -1.1392, -1.4991]])...
	Related Layers:
		- parent layers: layernorm_3_111
		- child layers: matmul_19_127
		- shares parents with layers: linear_28_112, linear_29_113, linear_30_122, linear_31_124, linear_33_134, linear_34_136, linear_35_137, linear_36_146, linear_37_148, linear_38_149, linear_39_158, linear_40_160, linear_41_161, linear_42_170, linear_43_172, linear_44_173, linear_45_182, linear_46_184, linear_47_185, linear_48_194, linear_49_196, linear_50_197, linear_51_206
		- shares children with layers: transpose_10_126
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.1.query:1
	Time elapsed:  1.111E-04s
	Output of modules: blocks.1.sa.heads.1.query
	Output of bottom-level module: blocks.1.sa.heads.1.query:1
	Lookup keys: -563, 135, blocks.1.sa.heads.1.query, blocks.1.sa.heads.1.query:1, linear_32, linear_32:1, linear_32_125, linear_32_125:1
--------------------------------------------
Layer transpose_10_126, operation 127/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 5.0796e-02,  2.5503e-01, -1.7166e-01, -2.1841e-01, -3.7702e-01,
          1.0970e+00, -2.5489e-01, -2.7599e-02],
        [ 5.4887e+00,  4.2673e+00,  3.8111e+00,  4.1037e+00,  3.2762e+00,
          5.7794e+00,  4.7667e+00,  3.5138e+00],
        [-2.9528e+00, -2.4595e+00, -2.4736e+00, -2.1063e+00, -2.2167e+00,
         -4.0667e+00, -3.0141e+00, -2.0142e+00],
        [ 4.7936e+00,  4.8064e+00,  3.8432e+00,  3.6994e+00,  3.5423e+00,
          5.1734e+00,  3.5579e+00,  3.8923e+00],
        [-4.8896e+00, -4.4627e+00, -4.6286e+00, -5.1594e+00, -4.1216e+00,
         -4.9350e+00, -3.9807e+00, -4.1526e+00],
        [ 6.1442e-01,  1.8789e+00,  1.1282e+00,  1.4710e+00,  5.6769e-01,
         -5.1194e-01, -5.8223e-03,  1.4524e+00],
        [-5.0028e+00, -4.9687e+00, -4.3564e+00, -4.3661e+00, -3.3896e+00,
         -5.8990e+00, -4.9259e+00, -4.4593e+00],
        [ 4.5328e+00,  3.3106e+00,  2.9377e+00,  3.2993e+00,  3.9186e+00,
          4.8564e+00,  3.9250e+00,  2.9211e+00]])...
	Related Layers:
		- parent layers: linear_31_124
		- child layers: matmul_19_127
		- shares parents with no other layers
		- shares children with layers: linear_32_125
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.1.sa.heads.1:1
	Time elapsed:  5.984E-05s
	Output of modules: none
	Lookup keys: -562, 136, transpose_10, transpose_10:1, transpose_10_126, transpose_10_126:1
--------------------------------------------
Layer matmul_19_127, operation 128/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-112.3288,  -96.7935,  -93.0536,  -95.7518,  -88.7735, -120.0470,
          -96.9688,  -84.5280],
        [   1.8305,    5.2401,    6.6196,    6.3305,    1.7125,    7.2412,
            5.9903,   11.4560],
        [ -66.0841,  -50.2151,  -50.6287,  -51.9582,  -52.5942,  -75.3667,
          -58.5908,  -46.2968],
        [ -45.2359,  -28.9295,  -29.9947,  -32.1479,  -33.6954,  -50.1811,
          -36.5160,  -26.8027],
        [ -68.8277,  -56.0945,  -55.9917,  -59.4951,  -54.6739,  -76.7532,
          -60.6822,  -52.2556],
        [-127.5210, -112.9151, -113.5730, -113.5504, -108.2954, -140.0761,
         -128.7067, -113.0429],
        [ -80.4217,  -76.2637,  -70.0068,  -72.3627,  -69.6204,  -73.5418,
          -69.1332,  -63.0905],
        [ -89.4326,  -73.4147,  -69.9411,  -73.3863,  -73.9583,  -94.4306,
          -74.7053,  -66.3295]])...
	Related Layers:
		- parent layers: linear_32_125, transpose_10_126
		- child layers: mul_10_128
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.1:1
	Time elapsed:  9.465E-05s
	Output of modules: none
	Lookup keys: -561, 137, matmul_19, matmul_19:1, matmul_19_127, matmul_19_127:1
--------------------------------------------
Layer mul_10_128, operation 129/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-5.7323, -4.9395, -4.7486, -4.8863, -4.5302, -6.1261, -4.9484, -4.3136],
        [ 0.0934,  0.2674,  0.3378,  0.3231,  0.0874,  0.3695,  0.3057,  0.5846],
        [-3.3723, -2.5625, -2.5836, -2.6515, -2.6839, -3.8460, -2.9899, -2.3626],
        [-2.3084, -1.4763, -1.5307, -1.6405, -1.7195, -2.5608, -1.8635, -1.3678],
        [-3.5124, -2.8626, -2.8573, -3.0361, -2.7901, -3.9168, -3.0967, -2.6667],
        [-6.5075, -5.7622, -5.7957, -5.7946, -5.5264, -7.1482, -6.5680, -5.7687],
        [-4.1040, -3.8918, -3.5725, -3.6927, -3.5528, -3.7529, -3.5279, -3.2196],
        [-4.5638, -3.7464, -3.5692, -3.7450, -3.7742, -4.8189, -3.8123, -3.3849]])...
	Related Layers:
		- parent layers: matmul_19_127
		- child layers: maskedfill_10_131
		- shares parents with no other layers
		- shares children with layers: eq_10_130
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.1.sa.heads.1:1
	Time elapsed:  7.033E-05s
	Output of modules: none
	Lookup keys: -560, 138, mul_10, mul_10:1, mul_10_128, mul_10_128:1
--------------------------------------------
Layer getitem_10_129, operation 130/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_10
		- child layers: eq_10_130
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.274E-05s
	Output of modules: none
	Lookup keys: -558, 140, getitem_10, getitem_10:1, getitem_10_129, getitem_10_129:1
--------------------------------------------
Layer eq_10_130, operation 131/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_10_129
		- child layers: maskedfill_10_131
		- shares parents with no other layers
		- shares children with layers: mul_10_128
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  7.915E-05s
	Output of modules: none
	Lookup keys: -557, 141, eq_10, eq_10:1, eq_10_130, eq_10_130:1
--------------------------------------------
Layer maskedfill_10_131, operation 132/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-5.7323,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [ 0.0934,  0.2674,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-3.3723, -2.5625, -2.5836,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-2.3084, -1.4763, -1.5307, -1.6405,    -inf,    -inf,    -inf,    -inf],
        [-3.5124, -2.8626, -2.8573, -3.0361, -2.7901,    -inf,    -inf,    -inf],
        [-6.5075, -5.7622, -5.7957, -5.7946, -5.5264, -7.1482,    -inf,    -inf],
        [-4.1040, -3.8918, -3.5725, -3.6927, -3.5528, -3.7529, -3.5279,    -inf],
        [-4.5638, -3.7464, -3.5692, -3.7450, -3.7742, -4.8189, -3.8123, -3.3849]])...
	Related Layers:
		- parent layers: mul_10_128, eq_10_130
		- child layers: softmax_10_132
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.1.sa.heads.1:1
	Time elapsed:  8.392E-05s
	Output of modules: none
	Lookup keys: -556, 142, maskedfill_10, maskedfill_10:1, maskedfill_10_131, maskedfill_10_131:1
--------------------------------------------
Layer softmax_10_132, operation 133/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.4566, 0.5434, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1836, 0.4125, 0.4039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1347, 0.3095, 0.2931, 0.2626, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1175, 0.2251, 0.2262, 0.1892, 0.2420, 0.0000, 0.0000, 0.0000],
        [0.0963, 0.2030, 0.1963, 0.1965, 0.2570, 0.0508, 0.0000, 0.0000],
        [0.0963, 0.1191, 0.1639, 0.1453, 0.1672, 0.1368, 0.1714, 0.0000],
        [0.0603, 0.1365, 0.1630, 0.1367, 0.1328, 0.0467, 0.1278, 0.1960]])...
	Related Layers:
		- parent layers: maskedfill_10_131
		- child layers: dropout_12_133
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.1.sa.heads.1:1
	Time elapsed:  7.200E-05s
	Output of modules: none
	Lookup keys: -555, 143, softmax_10, softmax_10:1, softmax_10_132, softmax_10_132:1
--------------------------------------------
Layer dropout_12_133, operation 134/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.4566, 0.5434, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1836, 0.4125, 0.4039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1347, 0.3095, 0.2931, 0.2626, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1175, 0.2251, 0.2262, 0.1892, 0.2420, 0.0000, 0.0000, 0.0000],
        [0.0963, 0.2030, 0.1963, 0.1965, 0.2570, 0.0508, 0.0000, 0.0000],
        [0.0963, 0.1191, 0.1639, 0.1453, 0.1672, 0.1368, 0.1714, 0.0000],
        [0.0603, 0.1365, 0.1630, 0.1367, 0.1328, 0.0467, 0.1278, 0.1960]])...
	Related Layers:
		- parent layers: softmax_10_132
		- child layers: matmul_20_135
		- shares parents with no other layers
		- shares children with layers: linear_33_134
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.1.sa.heads.1.dropout:1
	Time elapsed:  5.531E-05s
	Output of modules: blocks.1.sa.heads.1.dropout
	Output of bottom-level module: blocks.1.sa.heads.1.dropout:1
	Lookup keys: -554, 144, blocks.1.sa.heads.1.dropout, blocks.1.sa.heads.1.dropout:1, dropout_12, dropout_12:1, dropout_12_133, dropout_12_133:1
--------------------------------------------
Layer linear_33_134, operation 135/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.5044, -0.1418,  0.3266,  0.4801, -0.8784,  0.8338,  0.8766, -0.0126],
        [ 0.5283, -0.6820,  0.7599, -0.2630, -1.1969,  1.0396,  0.8941,  0.6091],
        [-0.0924, -0.6260,  0.1475,  0.0883, -0.2903,  0.7728,  0.2701, -0.4106],
        [-0.0336, -0.5305,  0.1550,  0.1716, -0.2379,  1.1046,  0.6218,  0.0683],
        [-1.0343, -1.2145, -0.1306,  0.2879, -0.3874,  0.9249,  0.6479, -0.7401],
        [ 0.3365,  0.8422,  1.9452, -0.3884, -0.8847,  0.6368,  1.0293,  0.9968],
        [ 0.0448,  0.8921,  2.4341, -1.1351, -0.3443,  1.2396,  1.1709,  1.2996],
        [-0.3638, -0.4301,  0.6941, -0.3106, -0.0026,  1.5539,  0.2418,  0.0708]])...
	Related Layers:
		- parent layers: layernorm_3_111
		- child layers: matmul_20_135
		- shares parents with layers: linear_28_112, linear_29_113, linear_30_122, linear_31_124, linear_32_125, linear_34_136, linear_35_137, linear_36_146, linear_37_148, linear_38_149, linear_39_158, linear_40_160, linear_41_161, linear_42_170, linear_43_172, linear_44_173, linear_45_182, linear_46_184, linear_47_185, linear_48_194, linear_49_196, linear_50_197, linear_51_206
		- shares children with layers: dropout_12_133
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.1.value:1
	Time elapsed:  1.297E-04s
	Output of modules: blocks.1.sa.heads.1.value
	Output of bottom-level module: blocks.1.sa.heads.1.value:1
	Lookup keys: -553, 145, blocks.1.sa.heads.1.value, blocks.1.sa.heads.1.value:1, linear_33, linear_33:1, linear_33_134, linear_33_134:1
--------------------------------------------
Layer matmul_20_135, operation 136/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.5044, -0.1418,  0.3266,  0.4801, -0.8784,  0.8338,  0.8766, -0.0126],
        [ 0.0568, -0.4353,  0.5620,  0.0763, -1.0515,  0.9456,  0.8861,  0.3252],
        [ 0.0881, -0.5602,  0.4330,  0.0153, -0.7723,  0.8940,  0.6388,  0.0831],
        [ 0.0597, -0.5530,  0.3631,  0.0542, -0.6364,  0.9507,  0.6373,  0.0844],
        [-0.2179, -0.7060,  0.2405,  0.1193, -0.5771,  0.9396,  0.6398, -0.1234],
        [-0.2148, -0.6487,  0.3103,  0.0982, -0.5759,  0.9302,  0.6600, -0.0843],
        [-0.1249, -0.2095,  0.8301, -0.1452, -0.5541,  0.9455,  0.7753,  0.2494],
        [-0.1652, -0.3685,  0.6895, -0.1550, -0.4336,  1.0849,  0.6352,  0.1531]])...
	Related Layers:
		- parent layers: dropout_12_133, linear_33_134
		- child layers: cat_2_208
		- shares parents with no other layers
		- shares children with layers: matmul_18_123, matmul_22_147, matmul_24_159, matmul_26_171, matmul_28_183, matmul_30_195, matmul_32_207
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.1:1
	Time elapsed:  8.726E-05s
	Output of modules: blocks.1.sa.heads.1
	Lookup keys: -552, 146, blocks.1.sa.heads.1, blocks.1.sa.heads.1:1, matmul_20, matmul_20:1, matmul_20_135, matmul_20_135:1
--------------------------------------------
Layer linear_34_136, operation 137/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-5.7507,  1.1998,  2.8902, -2.0300, -4.2284,  3.7422,  1.9982, -3.4784],
        [-5.0583,  0.7189,  2.8793, -2.0093, -3.5791,  4.3936,  2.4212, -5.5153],
        [-4.7533,  0.9661,  2.9403, -1.1898, -4.0508,  3.3296,  2.6548, -3.7203],
        [-4.3343,  1.3297,  3.0225, -1.2746, -3.6349,  3.6648,  2.0262, -3.8078],
        [-2.8202,  1.5568,  2.9147, -1.2481, -3.6493,  3.3818,  1.7200, -3.0586],
        [-6.6118,  0.0643,  2.9100, -1.6514, -4.7473,  5.3440,  2.5163, -6.0721],
        [-5.6256, -0.2248,  3.5958, -1.5461, -3.9959,  3.8530,  3.3356, -6.6211],
        [-4.9134,  1.3159,  3.1524, -1.4405, -5.4150,  2.7019,  2.2141, -5.3696]])...
	Related Layers:
		- parent layers: layernorm_3_111
		- child layers: transpose_11_138
		- shares parents with layers: linear_28_112, linear_29_113, linear_30_122, linear_31_124, linear_32_125, linear_33_134, linear_35_137, linear_36_146, linear_37_148, linear_38_149, linear_39_158, linear_40_160, linear_41_161, linear_42_170, linear_43_172, linear_44_173, linear_45_182, linear_46_184, linear_47_185, linear_48_194, linear_49_196, linear_50_197, linear_51_206
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.2.key:1
	Time elapsed:  1.128E-04s
	Output of modules: blocks.1.sa.heads.2.key
	Output of bottom-level module: blocks.1.sa.heads.2.key:1
	Lookup keys: -551, 147, blocks.1.sa.heads.2.key, blocks.1.sa.heads.2.key:1, linear_34, linear_34:1, linear_34_136, linear_34_136:1
--------------------------------------------
Layer linear_35_137, operation 138/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.3309,  0.1104, -2.0526, -0.8558, -1.5568,  0.9629, -2.4213, -1.2729],
        [ 2.4882, -0.8158, -2.0455, -0.4371, -0.2469,  0.5464, -2.6278, -1.2035],
        [ 0.9065, -0.3706, -2.3365, -0.8037, -1.4716,  1.1924, -2.0118, -0.7163],
        [ 0.8999,  0.3564, -2.1323, -0.9368, -1.9805,  0.5989, -2.0349, -0.6884],
        [ 1.9539,  0.4338, -2.2465, -0.4863, -1.0088, -0.6155, -2.4605, -0.8022],
        [ 2.1064,  0.2062, -1.4147,  0.7918,  0.2662,  0.8107, -2.2303,  0.5149],
        [ 2.6455, -0.7721, -1.5140, -0.5611, -0.6678,  0.3447, -1.1584,  1.1610],
        [ 3.4338, -0.5495, -1.8309, -0.7940,  0.6536,  0.1511, -1.8501,  0.6146]])...
	Related Layers:
		- parent layers: layernorm_3_111
		- child layers: matmul_21_139
		- shares parents with layers: linear_28_112, linear_29_113, linear_30_122, linear_31_124, linear_32_125, linear_33_134, linear_34_136, linear_36_146, linear_37_148, linear_38_149, linear_39_158, linear_40_160, linear_41_161, linear_42_170, linear_43_172, linear_44_173, linear_45_182, linear_46_184, linear_47_185, linear_48_194, linear_49_196, linear_50_197, linear_51_206
		- shares children with layers: transpose_11_138
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.2.query:1
	Time elapsed:  1.111E-04s
	Output of modules: blocks.1.sa.heads.2.query
	Output of bottom-level module: blocks.1.sa.heads.2.query:1
	Lookup keys: -550, 148, blocks.1.sa.heads.2.query, blocks.1.sa.heads.2.query:1, linear_35, linear_35:1, linear_35_137, linear_35_137:1
--------------------------------------------
Layer transpose_11_138, operation 139/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-5.7507, -5.0583, -4.7533, -4.3343, -2.8202, -6.6118, -5.6256, -4.9134],
        [ 1.1998,  0.7189,  0.9661,  1.3297,  1.5568,  0.0643, -0.2248,  1.3159],
        [ 2.8902,  2.8793,  2.9403,  3.0225,  2.9147,  2.9100,  3.5958,  3.1524],
        [-2.0300, -2.0093, -1.1898, -1.2746, -1.2481, -1.6514, -1.5461, -1.4405],
        [-4.2284, -3.5791, -4.0508, -3.6349, -3.6493, -4.7473, -3.9959, -5.4150],
        [ 3.7422,  4.3936,  3.3296,  3.6648,  3.3818,  5.3440,  3.8530,  2.7019],
        [ 1.9982,  2.4212,  2.6548,  2.0262,  1.7200,  2.5163,  3.3356,  2.2141],
        [-3.4784, -5.5153, -3.7203, -3.8078, -3.0586, -6.0721, -6.6211, -5.3696]])...
	Related Layers:
		- parent layers: linear_34_136
		- child layers: matmul_21_139
		- shares parents with no other layers
		- shares children with layers: linear_35_137
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.1.sa.heads.2:1
	Time elapsed:  6.008E-05s
	Output of modules: none
	Lookup keys: -549, 149, transpose_11, transpose_11:1, transpose_11_138, transpose_11_138:1
--------------------------------------------
Layer matmul_21_139, operation 140/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -63.3623,  -51.8100,  -64.0378,  -64.1320,  -51.0126,  -58.3349,
          -78.2938,  -54.8290],
        [-115.4014, -108.8022, -113.4179, -112.5202,  -93.4419, -125.1705,
         -138.2072, -112.9806],
        [ -52.0970,  -39.8200,  -51.3828,  -50.7531,  -41.8130,  -44.1444,
          -65.4889,  -49.6706],
        [ -30.6610,  -24.2286,  -31.3536,  -30.3395,  -23.0823,  -25.1000,
          -45.9440,  -28.2738],
        [-101.0405,  -94.4478,  -97.5604,  -99.1556,  -86.6146, -110.7228,
         -121.9429,  -92.9140],
        [ -82.3121,  -88.7174,  -87.6679,  -86.2516,  -64.4503,  -86.4251,
         -109.6439, -100.6087],
        [-111.6514, -113.9492, -110.1986, -109.6722,  -86.1013, -123.4960,
         -141.2995, -117.0128],
        [-143.7902, -137.1376, -135.5479, -137.1256, -113.4083, -157.8645,
         -167.6069, -142.2357]])...
	Related Layers:
		- parent layers: linear_35_137, transpose_11_138
		- child layers: mul_11_140
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.2:1
	Time elapsed:  8.655E-05s
	Output of modules: none
	Lookup keys: -548, 150, matmul_21, matmul_21:1, matmul_21_139, matmul_21_139:1
--------------------------------------------
Layer mul_11_140, operation 141/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-3.2334, -2.6439, -3.2679, -3.2727, -2.6032, -2.9769, -3.9954, -2.7980],
        [-5.8891, -5.5523, -5.7878, -5.7420, -4.7684, -6.3876, -7.0529, -5.7655],
        [-2.6586, -2.0321, -2.6221, -2.5900, -2.1338, -2.2527, -3.3420, -2.5347],
        [-1.5647, -1.2364, -1.6000, -1.5483, -1.1779, -1.2809, -2.3446, -1.4428],
        [-5.1562, -4.8198, -4.9786, -5.0600, -4.4200, -5.6503, -6.2229, -4.7415],
        [-4.2005, -4.5273, -4.4738, -4.4015, -3.2890, -4.4104, -5.5952, -5.1342],
        [-5.6977, -5.8149, -5.6235, -5.5967, -4.3938, -6.3021, -7.2107, -5.9713],
        [-7.3378, -6.9983, -6.9172, -6.9977, -5.7873, -8.0560, -8.5532, -7.2584]])...
	Related Layers:
		- parent layers: matmul_21_139
		- child layers: maskedfill_11_143
		- shares parents with no other layers
		- shares children with layers: eq_11_142
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.1.sa.heads.2:1
	Time elapsed:  7.367E-05s
	Output of modules: none
	Lookup keys: -547, 151, mul_11, mul_11:1, mul_11_140, mul_11_140:1
--------------------------------------------
Layer getitem_11_141, operation 142/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_11
		- child layers: eq_11_142
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.726E-05s
	Output of modules: none
	Lookup keys: -545, 153, getitem_11, getitem_11:1, getitem_11_141, getitem_11_141:1
--------------------------------------------
Layer eq_11_142, operation 143/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_11_141
		- child layers: maskedfill_11_143
		- shares parents with no other layers
		- shares children with layers: mul_11_140
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.225E-05s
	Output of modules: none
	Lookup keys: -544, 154, eq_11, eq_11:1, eq_11_142, eq_11_142:1
--------------------------------------------
Layer maskedfill_11_143, operation 144/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-3.2334,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-5.8891, -5.5523,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-2.6586, -2.0321, -2.6221,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-1.5647, -1.2364, -1.6000, -1.5483,    -inf,    -inf,    -inf,    -inf],
        [-5.1562, -4.8198, -4.9786, -5.0600, -4.4200,    -inf,    -inf,    -inf],
        [-4.2005, -4.5273, -4.4738, -4.4015, -3.2890, -4.4104,    -inf,    -inf],
        [-5.6977, -5.8149, -5.6235, -5.5967, -4.3938, -6.3021, -7.2107,    -inf],
        [-7.3378, -6.9983, -6.9172, -6.9977, -5.7873, -8.0560, -8.5532, -7.2584]])...
	Related Layers:
		- parent layers: mul_11_140, eq_11_142
		- child layers: softmax_11_144
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.1.sa.heads.2:1
	Time elapsed:  8.607E-05s
	Output of modules: none
	Lookup keys: -543, 155, maskedfill_11, maskedfill_11:1, maskedfill_11_143, maskedfill_11_143:1
--------------------------------------------
Layer softmax_11_144, operation 145/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.4166, 0.5834, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2559, 0.4788, 0.2654, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2288, 0.3177, 0.2209, 0.2326, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1474, 0.2064, 0.1761, 0.1623, 0.3078, 0.0000, 0.0000, 0.0000],
        [0.1515, 0.1093, 0.1153, 0.1239, 0.3771, 0.1229, 0.0000, 0.0000],
        [0.1173, 0.1044, 0.1264, 0.1298, 0.4322, 0.0641, 0.0258, 0.0000],
        [0.0839, 0.1179, 0.1278, 0.1180, 0.3957, 0.0409, 0.0249, 0.0909]])...
	Related Layers:
		- parent layers: maskedfill_11_143
		- child layers: dropout_13_145
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.1.sa.heads.2:1
	Time elapsed:  6.485E-05s
	Output of modules: none
	Lookup keys: -542, 156, softmax_11, softmax_11:1, softmax_11_144, softmax_11_144:1
--------------------------------------------
Layer dropout_13_145, operation 146/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.4166, 0.5834, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2559, 0.4788, 0.2654, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2288, 0.3177, 0.2209, 0.2326, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1474, 0.2064, 0.1761, 0.1623, 0.3078, 0.0000, 0.0000, 0.0000],
        [0.1515, 0.1093, 0.1153, 0.1239, 0.3771, 0.1229, 0.0000, 0.0000],
        [0.1173, 0.1044, 0.1264, 0.1298, 0.4322, 0.0641, 0.0258, 0.0000],
        [0.0839, 0.1179, 0.1278, 0.1180, 0.3957, 0.0409, 0.0249, 0.0909]])...
	Related Layers:
		- parent layers: softmax_11_144
		- child layers: matmul_22_147
		- shares parents with no other layers
		- shares children with layers: linear_36_146
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.1.sa.heads.2.dropout:1
	Time elapsed:  5.221E-05s
	Output of modules: blocks.1.sa.heads.2.dropout
	Output of bottom-level module: blocks.1.sa.heads.2.dropout:1
	Lookup keys: -541, 157, blocks.1.sa.heads.2.dropout, blocks.1.sa.heads.2.dropout:1, dropout_13, dropout_13:1, dropout_13_145, dropout_13_145:1
--------------------------------------------
Layer linear_36_146, operation 147/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.7120,  0.2247, -0.3985, -0.3150, -0.2903, -0.7162,  1.1778,  0.4650],
        [ 0.5338, -0.1509, -0.4553, -0.3873, -0.5941, -0.2465,  1.0609,  0.4164],
        [ 0.5199,  0.7420,  0.4447, -0.1204, -0.5081, -0.6446,  1.0842,  0.8341],
        [ 0.1657,  0.0735,  0.1686, -0.5276, -0.3178, -0.6589,  1.1333,  0.9717],
        [ 0.1757, -0.1254,  0.1059, -0.3866, -0.0312,  0.0916,  0.8940,  1.1632],
        [ 1.9071,  0.4549, -0.3340, -0.0334,  0.3560,  0.0408,  1.1852, -0.0724],
        [ 2.2844,  0.3262, -0.8214,  0.0906,  0.3317,  0.0024,  0.7979, -0.0494],
        [ 0.3072, -0.1330, -0.3080, -0.3313,  0.0108,  0.0906,  0.0946,  0.8072]])...
	Related Layers:
		- parent layers: layernorm_3_111
		- child layers: matmul_22_147
		- shares parents with layers: linear_28_112, linear_29_113, linear_30_122, linear_31_124, linear_32_125, linear_33_134, linear_34_136, linear_35_137, linear_37_148, linear_38_149, linear_39_158, linear_40_160, linear_41_161, linear_42_170, linear_43_172, linear_44_173, linear_45_182, linear_46_184, linear_47_185, linear_48_194, linear_49_196, linear_50_197, linear_51_206
		- shares children with layers: dropout_13_145
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.2.value:1
	Time elapsed:  1.190E-04s
	Output of modules: blocks.1.sa.heads.2.value
	Output of bottom-level module: blocks.1.sa.heads.2.value:1
	Lookup keys: -540, 158, blocks.1.sa.heads.2.value, blocks.1.sa.heads.2.value:1, linear_36, linear_36:1, linear_36_146, linear_36_146:1
--------------------------------------------
Layer matmul_22_147, operation 148/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.7120,  0.2247, -0.3985, -0.3150, -0.2903, -0.7162,  1.1778,  0.4650],
        [ 0.6080,  0.0056, -0.4316, -0.3572, -0.4675, -0.4422,  1.1096,  0.4366],
        [ 0.5757,  0.1822, -0.2019, -0.2980, -0.4935, -0.4723,  1.0970,  0.5397],
        [ 0.4859,  0.1844, -0.0984, -0.3445, -0.4413, -0.5378,  1.1097,  0.6489],
        [ 0.3877,  0.1060, -0.0144, -0.3522, -0.3160, -0.3487,  1.0426,  0.8171],
        [ 0.5473,  0.1208, -0.0391, -0.3192, -0.1749, -0.2519,  1.0426,  0.7623],
        [ 0.4837,  0.0973, -0.0130, -0.3280, -0.1836, -0.2345,  1.0160,  0.8264],
        [ 0.4411,  0.0697, -0.0306, -0.3319, -0.1854, -0.2031,  0.9270,  0.8388]])...
	Related Layers:
		- parent layers: dropout_13_145, linear_36_146
		- child layers: cat_2_208
		- shares parents with no other layers
		- shares children with layers: matmul_18_123, matmul_20_135, matmul_24_159, matmul_26_171, matmul_28_183, matmul_30_195, matmul_32_207
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.2:1
	Time elapsed:  8.368E-05s
	Output of modules: blocks.1.sa.heads.2
	Lookup keys: -539, 159, blocks.1.sa.heads.2, blocks.1.sa.heads.2:1, matmul_22, matmul_22:1, matmul_22_147, matmul_22_147:1
--------------------------------------------
Layer linear_37_148, operation 149/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-1.4483, -5.2385,  4.3713, -3.9794,  0.1471,  1.4998,  5.6396, -5.1469],
        [-0.9935, -4.8087,  4.7620, -3.6343, -0.3356,  2.3485,  5.3269, -4.7005],
        [-0.6263, -3.9446,  3.8166, -3.1758, -0.7059,  1.3590,  4.9309, -4.2824],
        [-0.8451, -4.1681,  4.3998, -2.7693, -0.6049,  0.7304,  4.8011, -3.8757],
        [-1.6791, -4.7387,  4.3021, -3.2788,  0.1992,  1.0437,  4.4926, -4.5936],
        [-0.9653, -4.7718,  3.5895, -4.8536, -2.2354,  1.5817,  5.8911, -4.8792],
        [-0.7887, -3.9399,  2.9022, -3.8704, -1.6824,  1.8847,  5.2230, -4.4802],
        [-1.5996, -3.8230,  4.2685, -4.0502, -0.2034,  1.3206,  4.8851, -5.2357]])...
	Related Layers:
		- parent layers: layernorm_3_111
		- child layers: transpose_12_150
		- shares parents with layers: linear_28_112, linear_29_113, linear_30_122, linear_31_124, linear_32_125, linear_33_134, linear_34_136, linear_35_137, linear_36_146, linear_38_149, linear_39_158, linear_40_160, linear_41_161, linear_42_170, linear_43_172, linear_44_173, linear_45_182, linear_46_184, linear_47_185, linear_48_194, linear_49_196, linear_50_197, linear_51_206
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.3.key:1
	Time elapsed:  1.130E-04s
	Output of modules: blocks.1.sa.heads.3.key
	Output of bottom-level module: blocks.1.sa.heads.3.key:1
	Lookup keys: -538, 160, blocks.1.sa.heads.3.key, blocks.1.sa.heads.3.key:1, linear_37, linear_37:1, linear_37_148, linear_37_148:1
--------------------------------------------
Layer linear_38_149, operation 150/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.8869,  0.8233, -0.2675, -0.8585, -0.2059, -0.0835, -0.1103, -0.8467],
        [-1.8279,  0.9989, -0.3468, -1.0098, -1.3787, -0.6457,  0.0953, -1.2835],
        [-1.5095,  0.3893,  0.1369, -1.3339,  0.0905, -0.2565, -1.0854, -1.6448],
        [-1.5535,  0.7007,  0.0464, -2.0966, -0.4239, -0.0955, -0.4678, -1.8122],
        [-0.7928,  0.2003,  0.2798, -0.7621,  0.3603,  0.2011, -1.4893, -0.3496],
        [-2.7366,  1.6724,  1.2789, -1.7482,  0.1307,  0.8161,  0.2438, -2.1939],
        [-0.6091,  1.0546,  0.4611, -2.0760, -1.6580,  0.0194,  0.5972, -2.1553],
        [-1.2674,  0.4381,  0.4122, -0.9327, -0.7616, -0.0079, -0.6117, -1.1292]])...
	Related Layers:
		- parent layers: layernorm_3_111
		- child layers: matmul_23_151
		- shares parents with layers: linear_28_112, linear_29_113, linear_30_122, linear_31_124, linear_32_125, linear_33_134, linear_34_136, linear_35_137, linear_36_146, linear_37_148, linear_39_158, linear_40_160, linear_41_161, linear_42_170, linear_43_172, linear_44_173, linear_45_182, linear_46_184, linear_47_185, linear_48_194, linear_49_196, linear_50_197, linear_51_206
		- shares children with layers: transpose_12_150
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.3.query:1
	Time elapsed:  1.132E-04s
	Output of modules: blocks.1.sa.heads.3.query
	Output of bottom-level module: blocks.1.sa.heads.3.query:1
	Lookup keys: -537, 161, blocks.1.sa.heads.3.query, blocks.1.sa.heads.3.query:1, linear_38, linear_38:1, linear_38_149, linear_38_149:1
--------------------------------------------
Layer transpose_12_150, operation 151/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-1.4483, -0.9935, -0.6263, -0.8451, -1.6791, -0.9653, -0.7887, -1.5996],
        [-5.2385, -4.8087, -3.9446, -4.1681, -4.7387, -4.7718, -3.9399, -3.8230],
        [ 4.3713,  4.7620,  3.8166,  4.3998,  4.3021,  3.5895,  2.9022,  4.2685],
        [-3.9794, -3.6343, -3.1758, -2.7693, -3.2788, -4.8536, -3.8704, -4.0502],
        [ 0.1471, -0.3356, -0.7059, -0.6049,  0.1992, -2.2354, -1.6824, -0.2034],
        [ 1.4998,  2.3485,  1.3590,  0.7304,  1.0437,  1.5817,  1.8847,  1.3206],
        [ 5.6396,  5.3269,  4.9309,  4.8011,  4.4926,  5.8911,  5.2230,  4.8851],
        [-5.1469, -4.7005, -4.2824, -3.8757, -4.5936, -4.8792, -4.4802, -5.2357]])...
	Related Layers:
		- parent layers: linear_37_148
		- child layers: matmul_23_151
		- shares parents with no other layers
		- shares children with layers: linear_38_149
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.1.sa.heads.3:1
	Time elapsed:  5.865E-05s
	Output of modules: none
	Lookup keys: -536, 162, transpose_12, transpose_12:1, transpose_12_150, transpose_12_150:1
--------------------------------------------
Layer matmul_23_151, operation 152/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-117.2023, -113.2970,  -95.6610,  -94.9006,  -87.4619, -105.7726,
         -109.8017, -104.2740],
        [ -74.4879,  -69.4918,  -52.8828,  -55.1566,  -52.5881,  -61.2201,
          -60.8151,  -59.4429],
        [-101.0981, -101.5143,  -81.2781,  -83.5782,  -76.3460,  -91.2885,
          -95.0980,  -86.6901],
        [ -67.6655,  -68.8706,  -51.6437,  -55.0140,  -49.5959,  -56.3782,
          -61.7055,  -56.1793],
        [-100.5588, -100.6604,  -82.2569,  -85.2927,  -81.6168,  -94.9759,
          -93.6701,  -90.6169],
        [ -16.0726,  -15.4567,  -20.3699,  -16.1219,   -5.9295,   -9.2728,
          -27.4473,  -15.7817],
        [  -3.2823,   -0.6650,    3.1321,    5.2671,    5.4776,   13.2311,
           -5.1534,    0.1678],
        [ -67.0278,  -71.6933,  -53.7982,  -55.9383,  -52.9477,  -54.4041,
          -68.4188,  -67.4120]])...
	Related Layers:
		- parent layers: linear_38_149, transpose_12_150
		- child layers: mul_12_152
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.3:1
	Time elapsed:  8.726E-05s
	Output of modules: none
	Lookup keys: -535, 163, matmul_23, matmul_23:1, matmul_23_151, matmul_23_151:1
--------------------------------------------
Layer mul_12_152, operation 153/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-5.9810, -5.7817, -4.8817, -4.8429, -4.4633, -5.3977, -5.6033, -5.3212],
        [-3.8012, -3.5462, -2.6987, -2.8147, -2.6836, -3.1241, -3.1035, -3.0334],
        [-5.1591, -5.1804, -4.1477, -4.2651, -3.8960, -4.6585, -4.8530, -4.4239],
        [-3.4530, -3.5145, -2.6354, -2.8074, -2.5309, -2.8770, -3.1489, -2.8669],
        [-5.1316, -5.1368, -4.1977, -4.3526, -4.1650, -4.8467, -4.7801, -4.6243],
        [-0.8202, -0.7888, -1.0395, -0.8227, -0.3026, -0.4732, -1.4007, -0.8054],
        [-0.1675, -0.0339,  0.1598,  0.2688,  0.2795,  0.6752, -0.2630,  0.0086],
        [-3.4205, -3.6586, -2.7454, -2.8546, -2.7020, -2.7763, -3.4915, -3.4401]])...
	Related Layers:
		- parent layers: matmul_23_151
		- child layers: maskedfill_12_155
		- shares parents with no other layers
		- shares children with layers: eq_12_154
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.1.sa.heads.3:1
	Time elapsed:  6.771E-05s
	Output of modules: none
	Lookup keys: -534, 164, mul_12, mul_12:1, mul_12_152, mul_12_152:1
--------------------------------------------
Layer getitem_12_153, operation 154/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_12
		- child layers: eq_12_154
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.465E-05s
	Output of modules: none
	Lookup keys: -532, 166, getitem_12, getitem_12:1, getitem_12_153, getitem_12_153:1
--------------------------------------------
Layer eq_12_154, operation 155/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_12_153
		- child layers: maskedfill_12_155
		- shares parents with no other layers
		- shares children with layers: mul_12_152
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  7.772E-05s
	Output of modules: none
	Lookup keys: -531, 167, eq_12, eq_12:1, eq_12_154, eq_12_154:1
--------------------------------------------
Layer maskedfill_12_155, operation 156/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-5.9810,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-3.8012, -3.5462,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-5.1591, -5.1804, -4.1477,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-3.4530, -3.5145, -2.6354, -2.8074,    -inf,    -inf,    -inf,    -inf],
        [-5.1316, -5.1368, -4.1977, -4.3526, -4.1650,    -inf,    -inf,    -inf],
        [-0.8202, -0.7888, -1.0395, -0.8227, -0.3026, -0.4732,    -inf,    -inf],
        [-0.1675, -0.0339,  0.1598,  0.2688,  0.2795,  0.6752, -0.2630,    -inf],
        [-3.4205, -3.6586, -2.7454, -2.8546, -2.7020, -2.7763, -3.4915, -3.4401]])...
	Related Layers:
		- parent layers: mul_12_152, eq_12_154
		- child layers: softmax_12_156
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.1.sa.heads.3:1
	Time elapsed:  8.368E-05s
	Output of modules: none
	Lookup keys: -530, 168, maskedfill_12, maskedfill_12:1, maskedfill_12_155, maskedfill_12_155:1
--------------------------------------------
Layer softmax_12_156, operation 157/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.4366, 0.5634, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2115, 0.2070, 0.5815, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1636, 0.1538, 0.3706, 0.3120, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1070, 0.1064, 0.2722, 0.2331, 0.2812, 0.0000, 0.0000, 0.0000],
        [0.1444, 0.1490, 0.1160, 0.1440, 0.2423, 0.2043, 0.0000, 0.0000],
        [0.1013, 0.1158, 0.1405, 0.1567, 0.1584, 0.2353, 0.0921, 0.0000],
        [0.0879, 0.0692, 0.1726, 0.1547, 0.1802, 0.1673, 0.0818, 0.0862]])...
	Related Layers:
		- parent layers: maskedfill_12_155
		- child layers: dropout_14_157
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.1.sa.heads.3:1
	Time elapsed:  6.866E-05s
	Output of modules: none
	Lookup keys: -529, 169, softmax_12, softmax_12:1, softmax_12_156, softmax_12_156:1
--------------------------------------------
Layer dropout_14_157, operation 158/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.4366, 0.5634, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2115, 0.2070, 0.5815, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1636, 0.1538, 0.3706, 0.3120, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1070, 0.1064, 0.2722, 0.2331, 0.2812, 0.0000, 0.0000, 0.0000],
        [0.1444, 0.1490, 0.1160, 0.1440, 0.2423, 0.2043, 0.0000, 0.0000],
        [0.1013, 0.1158, 0.1405, 0.1567, 0.1584, 0.2353, 0.0921, 0.0000],
        [0.0879, 0.0692, 0.1726, 0.1547, 0.1802, 0.1673, 0.0818, 0.0862]])...
	Related Layers:
		- parent layers: softmax_12_156
		- child layers: matmul_24_159
		- shares parents with no other layers
		- shares children with layers: linear_39_158
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.1.sa.heads.3.dropout:1
	Time elapsed:  5.341E-05s
	Output of modules: blocks.1.sa.heads.3.dropout
	Output of bottom-level module: blocks.1.sa.heads.3.dropout:1
	Lookup keys: -528, 170, blocks.1.sa.heads.3.dropout, blocks.1.sa.heads.3.dropout:1, dropout_14, dropout_14:1, dropout_14_157, dropout_14_157:1
--------------------------------------------
Layer linear_39_158, operation 159/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.5376,  1.5459, -0.6759, -0.8167, -0.3035, -1.1188, -1.4820,  1.0224],
        [-0.8707,  0.6971, -0.4167, -1.7107, -0.1421, -0.7689, -1.5292,  0.1260],
        [-0.6595,  1.1225,  0.0437, -1.0192, -0.2349, -0.3322, -1.0731,  0.0740],
        [-0.9846,  1.3504,  0.1109, -0.4954, -0.1349, -0.4574, -1.4673, -0.0225],
        [-1.0233,  1.3751,  0.7769, -0.7078, -0.0100,  0.0976, -1.3222,  0.9882],
        [-1.7380,  2.1808, -2.0837, -0.4743, -0.7300, -1.3062, -1.8673, -0.4648],
        [-1.6684,  1.8984, -2.1381, -0.5509, -0.6145, -0.4557, -1.6106, -0.1580],
        [-1.4647,  1.6307, -0.8284, -1.6983, -0.5614, -1.0256, -1.5852, -0.0970]])...
	Related Layers:
		- parent layers: layernorm_3_111
		- child layers: matmul_24_159
		- shares parents with layers: linear_28_112, linear_29_113, linear_30_122, linear_31_124, linear_32_125, linear_33_134, linear_34_136, linear_35_137, linear_36_146, linear_37_148, linear_38_149, linear_40_160, linear_41_161, linear_42_170, linear_43_172, linear_44_173, linear_45_182, linear_46_184, linear_47_185, linear_48_194, linear_49_196, linear_50_197, linear_51_206
		- shares children with layers: dropout_14_157
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.3.value:1
	Time elapsed:  1.209E-04s
	Output of modules: blocks.1.sa.heads.3.value
	Output of bottom-level module: blocks.1.sa.heads.3.value:1
	Lookup keys: -527, 171, blocks.1.sa.heads.3.value, blocks.1.sa.heads.3.value:1, linear_39, linear_39:1, linear_39_158, linear_39_158:1
--------------------------------------------
Layer matmul_24_159, operation 160/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.5376,  1.5459, -0.6759, -0.8167, -0.3035, -1.1188, -1.4820,  1.0224],
        [-0.7253,  1.0677, -0.5298, -1.3204, -0.2126, -0.9217, -1.5086,  0.5174],
        [-0.6774,  1.1240, -0.2038, -1.1195, -0.2302, -0.5889, -1.2540,  0.2853],
        [-0.7735,  1.1974, -0.1239, -0.9290, -0.2006, -0.5671, -1.3332,  0.2070],
        [-0.8470,  1.2467,  0.1396, -0.8614, -0.1458, -0.3711, -1.3274,  0.4156],
        [-1.0287,  1.4305, -0.3761, -0.8308, -0.2632, -0.6237, -1.4795,  0.3162],
        [-1.1268,  1.5123, -0.6572, -0.7760, -0.3313, -0.6545, -1.5049,  0.1576],
        [-1.1117,  1.4954, -0.5185, -0.8411, -0.3205, -0.6063, -1.4675,  0.1869]])...
	Related Layers:
		- parent layers: dropout_14_157, linear_39_158
		- child layers: cat_2_208
		- shares parents with no other layers
		- shares children with layers: matmul_18_123, matmul_20_135, matmul_22_147, matmul_26_171, matmul_28_183, matmul_30_195, matmul_32_207
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.3:1
	Time elapsed:  8.440E-05s
	Output of modules: blocks.1.sa.heads.3
	Lookup keys: -526, 172, blocks.1.sa.heads.3, blocks.1.sa.heads.3:1, matmul_24, matmul_24:1, matmul_24_159, matmul_24_159:1
--------------------------------------------
Layer linear_40_160, operation 161/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.7719, -4.2601, -3.7644,  6.2503,  5.2337,  3.2200,  4.3573, -3.8456],
        [-1.4057, -2.9856, -1.7326,  5.0611,  3.9728,  2.2033,  3.6141, -3.4918],
        [-1.4735, -2.8196, -2.9042,  5.3083,  3.9260,  2.5514,  2.2173, -3.1245],
        [-1.3941, -2.9943, -3.1129,  5.6935,  3.9922,  2.8788,  2.2800, -3.2361],
        [-1.2460, -2.7097, -3.5267,  4.4737,  3.8446,  3.1120,  2.5531, -3.2201],
        [-0.3524, -5.7149, -3.7746,  5.9171,  4.8415,  3.6089,  6.0277, -4.4375],
        [-0.3811, -3.6169, -3.4928,  3.6611,  3.9411,  2.7236,  4.1817, -3.4535],
        [-0.4174, -3.3571, -3.0577,  4.3726,  3.6685,  1.5377,  2.8347, -3.3941]])...
	Related Layers:
		- parent layers: layernorm_3_111
		- child layers: transpose_13_162
		- shares parents with layers: linear_28_112, linear_29_113, linear_30_122, linear_31_124, linear_32_125, linear_33_134, linear_34_136, linear_35_137, linear_36_146, linear_37_148, linear_38_149, linear_39_158, linear_41_161, linear_42_170, linear_43_172, linear_44_173, linear_45_182, linear_46_184, linear_47_185, linear_48_194, linear_49_196, linear_50_197, linear_51_206
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.4.key:1
	Time elapsed:  1.147E-04s
	Output of modules: blocks.1.sa.heads.4.key
	Output of bottom-level module: blocks.1.sa.heads.4.key:1
	Lookup keys: -525, 173, blocks.1.sa.heads.4.key, blocks.1.sa.heads.4.key:1, linear_40, linear_40:1, linear_40_160, linear_40_160:1
--------------------------------------------
Layer linear_41_161, operation 162/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.9332,  0.0182,  0.6831, -2.7303, -0.8795,  1.8245,  0.1111,  1.2133],
        [ 2.7523,  0.0976,  1.1437, -4.3307, -1.1183,  0.9812, -0.4830,  1.3111],
        [ 1.1738,  0.6690,  0.9825, -2.4852, -0.5540,  1.2579,  0.0845, -0.2249],
        [ 0.6292,  0.4432,  0.9233, -1.8411, -0.6002,  0.9943,  0.2020, -0.3411],
        [ 1.7680, -0.2449,  0.2195, -2.8313, -1.5724,  1.3803,  1.3066,  0.6852],
        [ 0.1179,  0.4102,  1.6069, -0.9821, -0.2674,  1.6340, -1.5323,  0.9491],
        [ 0.8369,  1.9546,  1.1971, -3.0552, -0.9281,  0.3474, -1.6723,  2.2568],
        [ 1.5689,  0.7363,  0.7152, -2.2840, -1.4763,  0.1166, -0.9134,  1.3892]])...
	Related Layers:
		- parent layers: layernorm_3_111
		- child layers: matmul_25_163
		- shares parents with layers: linear_28_112, linear_29_113, linear_30_122, linear_31_124, linear_32_125, linear_33_134, linear_34_136, linear_35_137, linear_36_146, linear_37_148, linear_38_149, linear_39_158, linear_40_160, linear_42_170, linear_43_172, linear_44_173, linear_45_182, linear_46_184, linear_47_185, linear_48_194, linear_49_196, linear_50_197, linear_51_206
		- shares children with layers: transpose_13_162
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.4.query:1
	Time elapsed:  1.183E-04s
	Output of modules: blocks.1.sa.heads.4.query
	Output of bottom-level module: blocks.1.sa.heads.4.query:1
	Lookup keys: -524, 174, blocks.1.sa.heads.4.query, blocks.1.sa.heads.4.query:1, linear_41, linear_41:1, linear_41_161, linear_41_161:1
--------------------------------------------
Layer transpose_13_162, operation 163/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.7719, -1.4057, -1.4735, -1.3941, -1.2460, -0.3524, -0.3811, -0.4174],
        [-4.2601, -2.9856, -2.8196, -2.9943, -2.7097, -5.7149, -3.6169, -3.3571],
        [-3.7644, -1.7326, -2.9042, -3.1129, -3.5267, -3.7746, -3.4928, -3.0577],
        [ 6.2503,  5.0611,  5.3083,  5.6935,  4.4737,  5.9171,  3.6611,  4.3726],
        [ 5.2337,  3.9728,  3.9260,  3.9922,  3.8446,  4.8415,  3.9411,  3.6685],
        [ 3.2200,  2.2033,  2.5514,  2.8788,  3.1120,  3.6089,  2.7236,  1.5377],
        [ 4.3573,  3.6141,  2.2173,  2.2800,  2.5531,  6.0277,  4.1817,  2.8347],
        [-3.8456, -3.4918, -3.1245, -3.2361, -3.2201, -4.4375, -3.4535, -3.3941]])...
	Related Layers:
		- parent layers: linear_40_160
		- child layers: matmul_25_163
		- shares parents with no other layers
		- shares children with layers: linear_41_161
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.1.sa.heads.4:1
	Time elapsed:  5.889E-05s
	Output of modules: none
	Lookup keys: -523, 175, transpose_13, transpose_13:1, transpose_13_162, transpose_13_162:1
--------------------------------------------
Layer matmul_25_163, operation 164/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-101.3968,  -78.9663,  -81.9592,  -82.9478,  -72.9433, -108.2658,
          -64.3081,  -84.4916],
        [-197.9994, -161.0478, -164.1654, -167.0144, -145.7686, -213.7897,
         -133.1888, -161.5525],
        [ -98.6613,  -79.2532,  -82.9837,  -84.3322,  -72.1319, -116.8876,
          -68.4214,  -87.6721],
        [ -74.1763,  -57.4985,  -62.3587,  -63.0644,  -53.3648,  -88.4906,
          -49.3421,  -66.7423],
        [-106.7209,  -78.0244,  -88.7367,  -89.6388,  -79.7431,  -88.7882,
          -59.6499,  -76.3916],
        [ -39.4889,  -38.0542,  -28.5444,  -29.0418,  -30.0713,  -69.3230,
          -44.0593,  -47.2138],
        [-161.0263, -135.5416, -126.8785, -130.4425, -112.1334, -209.6474,
         -125.0476, -146.1299],
        [-157.5633, -127.4358, -128.8890, -132.2155, -118.1875, -180.9756,
         -118.0647, -133.0115]])...
	Related Layers:
		- parent layers: linear_41_161, transpose_13_162
		- child layers: mul_13_164
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.4:1
	Time elapsed:  8.750E-05s
	Output of modules: none
	Lookup keys: -522, 176, matmul_25, matmul_25:1, matmul_25_163, matmul_25_163:1
--------------------------------------------
Layer mul_13_164, operation 165/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -5.1744,  -4.0297,  -4.1825,  -4.2329,  -3.7224,  -5.5249,  -3.2817,
          -4.3117],
        [-10.1041,  -8.2184,  -8.3775,  -8.5229,  -7.4387, -10.9099,  -6.7968,
          -8.2442],
        [ -5.0348,  -4.0444,  -4.2347,  -4.3036,  -3.6810,  -5.9649,  -3.4916,
          -4.4740],
        [ -3.7853,  -2.9342,  -3.1822,  -3.2182,  -2.7233,  -4.5158,  -2.5180,
          -3.4059],
        [ -5.4461,  -3.9817,  -4.5283,  -4.5744,  -4.0694,  -4.5310,  -3.0440,
          -3.8983],
        [ -2.0152,  -1.9419,  -1.4567,  -1.4820,  -1.5346,  -3.5376,  -2.2484,
          -2.4094],
        [ -8.2173,  -6.9168,  -6.4747,  -6.6566,  -5.7223, -10.6985,  -6.3813,
          -7.4572],
        [ -8.0406,  -6.5032,  -6.5773,  -6.7471,  -6.0312,  -9.2354,  -6.0250,
          -6.7877]])...
	Related Layers:
		- parent layers: matmul_25_163
		- child layers: maskedfill_13_167
		- shares parents with no other layers
		- shares children with layers: eq_13_166
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.1.sa.heads.4:1
	Time elapsed:  6.843E-05s
	Output of modules: none
	Lookup keys: -521, 177, mul_13, mul_13:1, mul_13_164, mul_13_164:1
--------------------------------------------
Layer getitem_13_165, operation 166/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_13
		- child layers: eq_13_166
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.775E-05s
	Output of modules: none
	Lookup keys: -519, 179, getitem_13, getitem_13:1, getitem_13_165, getitem_13_165:1
--------------------------------------------
Layer eq_13_166, operation 167/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_13_165
		- child layers: maskedfill_13_167
		- shares parents with no other layers
		- shares children with layers: mul_13_164
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  7.796E-05s
	Output of modules: none
	Lookup keys: -518, 180, eq_13, eq_13:1, eq_13_166, eq_13_166:1
--------------------------------------------
Layer maskedfill_13_167, operation 168/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -5.1744,     -inf,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [-10.1041,  -8.2184,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -5.0348,  -4.0444,  -4.2347,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -3.7853,  -2.9342,  -3.1822,  -3.2182,     -inf,     -inf,     -inf,
             -inf],
        [ -5.4461,  -3.9817,  -4.5283,  -4.5744,  -4.0694,     -inf,     -inf,
             -inf],
        [ -2.0152,  -1.9419,  -1.4567,  -1.4820,  -1.5346,  -3.5376,     -inf,
             -inf],
        [ -8.2173,  -6.9168,  -6.4747,  -6.6566,  -5.7223, -10.6985,  -6.3813,
             -inf],
        [ -8.0406,  -6.5032,  -6.5773,  -6.7471,  -6.0312,  -9.2354,  -6.0250,
          -6.7877]])...
	Related Layers:
		- parent layers: mul_13_164, eq_13_166
		- child layers: softmax_13_168
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.1.sa.heads.4:1
	Time elapsed:  8.345E-05s
	Output of modules: none
	Lookup keys: -517, 181, maskedfill_13, maskedfill_13:1, maskedfill_13_167, maskedfill_13_167:1
--------------------------------------------
Layer softmax_13_168, operation 169/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1317, 0.8683, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1690, 0.4549, 0.3761, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1442, 0.3378, 0.2636, 0.2543, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0705, 0.3050, 0.1765, 0.1686, 0.2794, 0.0000, 0.0000, 0.0000],
        [0.1358, 0.1461, 0.2374, 0.2314, 0.2196, 0.0296, 0.0000, 0.0000],
        [0.0297, 0.1092, 0.1699, 0.1416, 0.3605, 0.0025, 0.1865, 0.0000],
        [0.0309, 0.1437, 0.1334, 0.1126, 0.2303, 0.0093, 0.2318, 0.1081]])...
	Related Layers:
		- parent layers: maskedfill_13_167
		- child layers: dropout_15_169
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.1.sa.heads.4:1
	Time elapsed:  6.890E-05s
	Output of modules: none
	Lookup keys: -516, 182, softmax_13, softmax_13:1, softmax_13_168, softmax_13_168:1
--------------------------------------------
Layer dropout_15_169, operation 170/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1317, 0.8683, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1690, 0.4549, 0.3761, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1442, 0.3378, 0.2636, 0.2543, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0705, 0.3050, 0.1765, 0.1686, 0.2794, 0.0000, 0.0000, 0.0000],
        [0.1358, 0.1461, 0.2374, 0.2314, 0.2196, 0.0296, 0.0000, 0.0000],
        [0.0297, 0.1092, 0.1699, 0.1416, 0.3605, 0.0025, 0.1865, 0.0000],
        [0.0309, 0.1437, 0.1334, 0.1126, 0.2303, 0.0093, 0.2318, 0.1081]])...
	Related Layers:
		- parent layers: softmax_13_168
		- child layers: matmul_26_171
		- shares parents with no other layers
		- shares children with layers: linear_42_170
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.1.sa.heads.4.dropout:1
	Time elapsed:  5.269E-05s
	Output of modules: blocks.1.sa.heads.4.dropout
	Output of bottom-level module: blocks.1.sa.heads.4.dropout:1
	Lookup keys: -515, 183, blocks.1.sa.heads.4.dropout, blocks.1.sa.heads.4.dropout:1, dropout_15, dropout_15:1, dropout_15_169, dropout_15_169:1
--------------------------------------------
Layer linear_42_170, operation 171/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.3666, -1.4779, -0.6445,  0.3633, -0.0056, -0.5266,  1.2000, -0.0491],
        [ 0.2348, -0.4738, -0.9165, -1.2844,  0.5199, -0.8715,  1.5196,  0.6544],
        [-0.8143, -0.2391, -0.0521,  0.0258,  0.0143,  0.2201,  0.6158, -0.5646],
        [-0.5614, -0.5413, -0.5173,  0.1781,  0.2080,  0.5830,  0.3998, -0.0410],
        [-0.2985, -1.1872, -0.2669, -0.4569, -0.3534, -0.3874,  0.9753, -0.4266],
        [ 0.4715, -0.8790, -0.8300,  0.1348,  0.7378, -1.4390,  0.5210,  0.5387],
        [ 0.7281, -1.3553, -0.7348, -0.7337, -0.0682, -0.8751,  1.8595,  1.1202],
        [ 0.3543, -0.8362, -0.3495,  0.0919, -0.5957,  0.3394,  1.2916,  0.1872]])...
	Related Layers:
		- parent layers: layernorm_3_111
		- child layers: matmul_26_171
		- shares parents with layers: linear_28_112, linear_29_113, linear_30_122, linear_31_124, linear_32_125, linear_33_134, linear_34_136, linear_35_137, linear_36_146, linear_37_148, linear_38_149, linear_39_158, linear_40_160, linear_41_161, linear_43_172, linear_44_173, linear_45_182, linear_46_184, linear_47_185, linear_48_194, linear_49_196, linear_50_197, linear_51_206
		- shares children with layers: dropout_15_169
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.4.value:1
	Time elapsed:  1.204E-04s
	Output of modules: blocks.1.sa.heads.4.value
	Output of bottom-level module: blocks.1.sa.heads.4.value:1
	Lookup keys: -514, 184, blocks.1.sa.heads.4.value, blocks.1.sa.heads.4.value:1, linear_42, linear_42:1, linear_42_170, linear_42_170:1
--------------------------------------------
Layer matmul_26_171, operation 172/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.3666, -1.4779, -0.6445,  0.3633, -0.0056, -0.5266,  1.2000, -0.0491],
        [ 0.1556, -0.6060, -0.8807, -1.0674,  0.4506, -0.8261,  1.4775,  0.5617],
        [-0.2613, -0.5552, -0.5455, -0.5133,  0.2409, -0.4027,  1.1257,  0.0771],
        [-0.3310, -0.5739, -0.5479, -0.3294,  0.2315, -0.1641,  0.9505,  0.0547],
        [-0.2760, -0.7138, -0.4959, -0.4592,  0.0970, -0.2740,  0.9966, -0.0296],
        [-0.3903, -0.7387, -0.4368, -0.1873,  0.0710, -0.1394,  0.8533, -0.1323],
        [-0.1738, -0.8960, -0.4367, -0.4011, -0.0498, -0.2973,  1.0626,  0.0248],
        [-0.0067, -0.8926, -0.4941, -0.4139, -0.0549, -0.3153,  1.1826,  0.1992]])...
	Related Layers:
		- parent layers: dropout_15_169, linear_42_170
		- child layers: cat_2_208
		- shares parents with no other layers
		- shares children with layers: matmul_18_123, matmul_20_135, matmul_22_147, matmul_24_159, matmul_28_183, matmul_30_195, matmul_32_207
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.4:1
	Time elapsed:  8.726E-05s
	Output of modules: blocks.1.sa.heads.4
	Lookup keys: -513, 185, blocks.1.sa.heads.4, blocks.1.sa.heads.4:1, matmul_26, matmul_26:1, matmul_26_171, matmul_26_171:1
--------------------------------------------
Layer linear_43_172, operation 173/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-2.1842, -5.2404, -2.0511,  1.7531,  6.1425,  5.3825, -0.9038,  2.3146],
        [-1.2661, -4.4427, -0.7730,  0.6878,  5.8961,  4.1226, -0.9159,  1.4673],
        [-2.1747, -4.2206, -1.8998,  1.8895,  5.5508,  4.1098, -0.4699,  1.3960],
        [-1.7345, -4.0802, -1.4653,  2.1302,  5.2125,  4.2986, -0.6049,  0.9922],
        [-1.9315, -3.9398, -1.3222,  1.6596,  4.2990,  3.6691, -0.3779, -0.2676],
        [-1.1081, -4.5291, -0.6371, -0.2093,  3.4779,  4.7570, -0.6386,  1.6437],
        [-1.8348, -3.8549, -0.4393,  0.6836,  2.7934,  3.4124,  0.0919,  1.0940],
        [-2.4516, -3.8638, -0.5110,  1.9013,  4.8022,  2.7075,  0.4790,  1.7203]])...
	Related Layers:
		- parent layers: layernorm_3_111
		- child layers: transpose_14_174
		- shares parents with layers: linear_28_112, linear_29_113, linear_30_122, linear_31_124, linear_32_125, linear_33_134, linear_34_136, linear_35_137, linear_36_146, linear_37_148, linear_38_149, linear_39_158, linear_40_160, linear_41_161, linear_42_170, linear_44_173, linear_45_182, linear_46_184, linear_47_185, linear_48_194, linear_49_196, linear_50_197, linear_51_206
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.5.key:1
	Time elapsed:  1.132E-04s
	Output of modules: blocks.1.sa.heads.5.key
	Output of bottom-level module: blocks.1.sa.heads.5.key:1
	Lookup keys: -512, 186, blocks.1.sa.heads.5.key, blocks.1.sa.heads.5.key:1, linear_43, linear_43:1, linear_43_172, linear_43_172:1
--------------------------------------------
Layer linear_44_173, operation 174/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.5131, -0.2203,  0.7459,  0.1169, -0.4984, -2.4529,  1.8093, -2.1518],
        [-0.1627,  0.2475,  1.3894,  0.1581, -2.4864, -2.4154,  1.4055, -2.6743],
        [-0.0403, -0.4403,  0.6497, -0.8401, -0.9390, -1.6635,  1.1217, -1.7136],
        [-0.1450, -1.0113,  1.2817, -0.6595, -0.1835, -1.8409,  1.1674, -2.2117],
        [-0.8683, -0.0488,  1.0419, -0.2795,  0.8440, -1.1717,  0.9302, -2.2636],
        [-0.5981,  1.1368,  0.1815,  2.0345, -0.6573, -3.5487,  1.8363, -0.1849],
        [ 0.0805,  1.4166, -0.2246,  1.6560, -1.1931, -3.8192,  1.6951, -1.7124],
        [-0.0158, -0.7365,  0.2919, -0.9283, -0.8849, -1.2615,  0.9277, -2.4489]])...
	Related Layers:
		- parent layers: layernorm_3_111
		- child layers: matmul_27_175
		- shares parents with layers: linear_28_112, linear_29_113, linear_30_122, linear_31_124, linear_32_125, linear_33_134, linear_34_136, linear_35_137, linear_36_146, linear_37_148, linear_38_149, linear_39_158, linear_40_160, linear_41_161, linear_42_170, linear_43_172, linear_45_182, linear_46_184, linear_47_185, linear_48_194, linear_49_196, linear_50_197, linear_51_206
		- shares children with layers: transpose_14_174
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.5.query:1
	Time elapsed:  1.125E-04s
	Output of modules: blocks.1.sa.heads.5.query
	Output of bottom-level module: blocks.1.sa.heads.5.query:1
	Lookup keys: -511, 187, blocks.1.sa.heads.5.query, blocks.1.sa.heads.5.query:1, linear_44, linear_44:1, linear_44_173, linear_44_173:1
--------------------------------------------
Layer transpose_14_174, operation 175/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-2.1842, -1.2661, -2.1747, -1.7345, -1.9315, -1.1081, -1.8348, -2.4516],
        [-5.2404, -4.4427, -4.2206, -4.0802, -3.9398, -4.5291, -3.8549, -3.8638],
        [-2.0511, -0.7730, -1.8998, -1.4653, -1.3222, -0.6371, -0.4393, -0.5110],
        [ 1.7531,  0.6878,  1.8895,  2.1302,  1.6596, -0.2093,  0.6836,  1.9013],
        [ 6.1425,  5.8961,  5.5508,  5.2125,  4.2990,  3.4779,  2.7934,  4.8022],
        [ 5.3825,  4.1226,  4.1098,  4.2986,  3.6691,  4.7570,  3.4124,  2.7075],
        [-0.9038, -0.9159, -0.4699, -0.6049, -0.3779, -0.6386,  0.0919,  0.4790],
        [ 2.3146,  1.4673,  1.3960,  0.9922, -0.2676,  1.6437,  1.0940,  1.7203]])...
	Related Layers:
		- parent layers: linear_43_172
		- child layers: matmul_27_175
		- shares parents with no other layers
		- shares children with layers: linear_44_173
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.1.sa.heads.5:1
	Time elapsed:  5.889E-05s
	Output of modules: none
	Lookup keys: -510, 188, transpose_14, transpose_14:1, transpose_14_174, transpose_14_174:1
--------------------------------------------
Layer matmul_27_175, operation 176/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-193.7992, -163.8537, -157.9588, -152.6820, -141.4800, -151.0183,
         -141.2097, -135.1456],
        [-278.7701, -243.0963, -237.8258, -230.3001, -187.7272, -204.1564,
         -200.1737, -198.3265],
        [-126.9689, -104.2947, -107.8905, -103.6878,  -92.5750,  -89.8794,
          -89.3316,  -97.3971],
        [-136.0484, -110.3304, -113.9699, -110.0093, -100.7380,  -96.2694,
          -97.0443, -102.8716],
        [-155.0200, -124.0818, -124.5838, -121.2504, -111.9322, -116.2328,
         -113.5307, -105.7412],
        [-134.5273, -130.2291, -104.3417, -100.8675,  -96.4376, -134.7752,
         -104.5509,  -81.7851],
        [-195.0612, -179.9153, -156.4557, -152.3287, -134.1826, -172.4863,
         -145.9125, -127.2239],
        [-130.7203, -104.7017, -106.6218, -102.0723,  -89.3746,  -91.8839,
          -86.5136,  -94.5419]])...
	Related Layers:
		- parent layers: linear_44_173, transpose_14_174
		- child layers: mul_14_176
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.5:1
	Time elapsed:  8.607E-05s
	Output of modules: none
	Lookup keys: -509, 189, matmul_27, matmul_27:1, matmul_27_175, matmul_27_175:1
--------------------------------------------
Layer mul_14_176, operation 177/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -9.8898,  -8.3616,  -8.0608,  -7.7915,  -7.2199,  -7.7066,  -7.2061,
          -6.8966],
        [-14.2259, -12.4055, -12.1365, -11.7525,  -9.5799, -10.4183, -10.2151,
         -10.1208],
        [ -6.4794,  -5.3223,  -5.5058,  -5.2913,  -4.7242,  -4.5866,  -4.5587,
          -4.9703],
        [ -6.9427,  -5.6303,  -5.8160,  -5.6139,  -5.1408,  -4.9127,  -4.9523,
          -5.2496],
        [ -7.9108,  -6.3320,  -6.3576,  -6.1875,  -5.7120,  -5.9315,  -5.7936,
          -5.3961],
        [ -6.8651,  -6.6457,  -5.3247,  -5.1474,  -4.9213,  -6.8777,  -5.3353,
          -4.1736],
        [ -9.9542,  -9.1813,  -7.9841,  -7.7735,  -6.8475,  -8.8022,  -7.4461,
          -6.4924],
        [ -6.6708,  -5.3430,  -5.4410,  -5.2089,  -4.5609,  -4.6889,  -4.4149,
          -4.8246]])...
	Related Layers:
		- parent layers: matmul_27_175
		- child layers: maskedfill_14_179
		- shares parents with no other layers
		- shares children with layers: eq_14_178
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.1.sa.heads.5:1
	Time elapsed:  6.962E-05s
	Output of modules: none
	Lookup keys: -508, 190, mul_14, mul_14:1, mul_14_176, mul_14_176:1
--------------------------------------------
Layer getitem_14_177, operation 178/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_14
		- child layers: eq_14_178
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.036E-05s
	Output of modules: none
	Lookup keys: -506, 192, getitem_14, getitem_14:1, getitem_14_177, getitem_14_177:1
--------------------------------------------
Layer eq_14_178, operation 179/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_14_177
		- child layers: maskedfill_14_179
		- shares parents with no other layers
		- shares children with layers: mul_14_176
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.368E-05s
	Output of modules: none
	Lookup keys: -505, 193, eq_14, eq_14:1, eq_14_178, eq_14_178:1
--------------------------------------------
Layer maskedfill_14_179, operation 180/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -9.8898,     -inf,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [-14.2259, -12.4055,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -6.4794,  -5.3223,  -5.5058,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -6.9427,  -5.6303,  -5.8160,  -5.6139,     -inf,     -inf,     -inf,
             -inf],
        [ -7.9108,  -6.3320,  -6.3576,  -6.1875,  -5.7120,     -inf,     -inf,
             -inf],
        [ -6.8651,  -6.6457,  -5.3247,  -5.1474,  -4.9213,  -6.8777,     -inf,
             -inf],
        [ -9.9542,  -9.1813,  -7.9841,  -7.7735,  -6.8475,  -8.8022,  -7.4461,
             -inf],
        [ -6.6708,  -5.3430,  -5.4410,  -5.2089,  -4.5609,  -4.6889,  -4.4149,
          -4.8246]])...
	Related Layers:
		- parent layers: mul_14_176, eq_14_178
		- child layers: softmax_14_180
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.1.sa.heads.5:1
	Time elapsed:  8.583E-05s
	Output of modules: none
	Lookup keys: -504, 194, maskedfill_14, maskedfill_14:1, maskedfill_14_179, maskedfill_14_179:1
--------------------------------------------
Layer softmax_14_180, operation 181/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1394, 0.8606, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1465, 0.4658, 0.3877, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0864, 0.3209, 0.2665, 0.3262, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0397, 0.1925, 0.1876, 0.2224, 0.3578, 0.0000, 0.0000, 0.0000],
        [0.0489, 0.0609, 0.2281, 0.2724, 0.3415, 0.0483, 0.0000, 0.0000],
        [0.0175, 0.0380, 0.1258, 0.1554, 0.3922, 0.0555, 0.2155, 0.0000],
        [0.0228, 0.0860, 0.0779, 0.0983, 0.1879, 0.1653, 0.2174, 0.1444]])...
	Related Layers:
		- parent layers: maskedfill_14_179
		- child layers: dropout_16_181
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.1.sa.heads.5:1
	Time elapsed:  6.294E-05s
	Output of modules: none
	Lookup keys: -503, 195, softmax_14, softmax_14:1, softmax_14_180, softmax_14_180:1
--------------------------------------------
Layer dropout_16_181, operation 182/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1394, 0.8606, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1465, 0.4658, 0.3877, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0864, 0.3209, 0.2665, 0.3262, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0397, 0.1925, 0.1876, 0.2224, 0.3578, 0.0000, 0.0000, 0.0000],
        [0.0489, 0.0609, 0.2281, 0.2724, 0.3415, 0.0483, 0.0000, 0.0000],
        [0.0175, 0.0380, 0.1258, 0.1554, 0.3922, 0.0555, 0.2155, 0.0000],
        [0.0228, 0.0860, 0.0779, 0.0983, 0.1879, 0.1653, 0.2174, 0.1444]])...
	Related Layers:
		- parent layers: softmax_14_180
		- child layers: matmul_28_183
		- shares parents with no other layers
		- shares children with layers: linear_45_182
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.1.sa.heads.5.dropout:1
	Time elapsed:  5.603E-05s
	Output of modules: blocks.1.sa.heads.5.dropout
	Output of bottom-level module: blocks.1.sa.heads.5.dropout:1
	Lookup keys: -502, 196, blocks.1.sa.heads.5.dropout, blocks.1.sa.heads.5.dropout:1, dropout_16, dropout_16:1, dropout_16_181, dropout_16_181:1
--------------------------------------------
Layer linear_45_182, operation 183/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-1.4519,  0.0236,  0.6543, -0.6101, -0.8709,  2.4836, -1.8800,  0.5645],
        [-1.6148, -0.2864,  0.8972, -0.7524, -0.2418,  1.6893, -1.6242,  0.1661],
        [-1.3338, -0.1420,  0.4350, -0.4889, -0.8186,  2.1107, -1.3275,  0.2892],
        [-1.0147, -0.1135,  0.7733, -0.3705, -0.7571,  2.0040, -0.8080,  0.3565],
        [-1.4466,  0.1397,  1.1585, -1.3069, -0.2519,  2.2019, -2.7322,  0.3498],
        [-0.3744,  0.1823,  1.3799,  0.6825, -0.0109,  2.2068,  0.5837,  0.6541],
        [-1.3533, -0.0690,  0.8282,  0.0229,  0.4057,  2.0949, -0.2138, -0.4854],
        [-1.3035, -0.4663,  0.0225, -1.2789, -0.9210,  1.5169, -1.4528, -0.0921]])...
	Related Layers:
		- parent layers: layernorm_3_111
		- child layers: matmul_28_183
		- shares parents with layers: linear_28_112, linear_29_113, linear_30_122, linear_31_124, linear_32_125, linear_33_134, linear_34_136, linear_35_137, linear_36_146, linear_37_148, linear_38_149, linear_39_158, linear_40_160, linear_41_161, linear_42_170, linear_43_172, linear_44_173, linear_46_184, linear_47_185, linear_48_194, linear_49_196, linear_50_197, linear_51_206
		- shares children with layers: dropout_16_181
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.5.value:1
	Time elapsed:  1.321E-04s
	Output of modules: blocks.1.sa.heads.5.value
	Output of bottom-level module: blocks.1.sa.heads.5.value:1
	Lookup keys: -501, 197, blocks.1.sa.heads.5.value, blocks.1.sa.heads.5.value:1, linear_45, linear_45:1, linear_45_182, linear_45_182:1
--------------------------------------------
Layer matmul_28_183, operation 184/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-1.4519,  0.0236,  0.6543, -0.6101, -0.8709,  2.4836, -1.8800,  0.5645],
        [-1.5921, -0.2432,  0.8633, -0.7326, -0.3295,  1.8000, -1.6598,  0.2216],
        [-1.4820, -0.1850,  0.6824, -0.6294, -0.5576,  1.9690, -1.5466,  0.2722],
        [-1.3301, -0.1648,  0.7126, -0.5453, -0.6180,  1.9729, -1.3010,  0.2954],
        [-1.3620, -0.0561,  0.8668, -0.8108, -0.4932,  2.0533, -1.7936,  0.3131],
        [-1.2620, -0.0231,  0.8587, -0.7014, -0.5368,  2.1100, -1.6185,  0.3518],
        [-1.2921,  0.0041,  0.9299, -0.6281, -0.2571,  2.1223, -1.4725,  0.1769],
        [-1.1917, -0.0722,  0.8311, -0.4655, -0.2727,  2.0164, -1.0384,  0.1397]])...
	Related Layers:
		- parent layers: dropout_16_181, linear_45_182
		- child layers: cat_2_208
		- shares parents with no other layers
		- shares children with layers: matmul_18_123, matmul_20_135, matmul_22_147, matmul_24_159, matmul_26_171, matmul_30_195, matmul_32_207
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.5:1
	Time elapsed:  8.655E-05s
	Output of modules: blocks.1.sa.heads.5
	Lookup keys: -500, 198, blocks.1.sa.heads.5, blocks.1.sa.heads.5:1, matmul_28, matmul_28:1, matmul_28_183, matmul_28_183:1
--------------------------------------------
Layer linear_46_184, operation 185/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-4.2403,  2.8277,  1.1239,  5.2559,  4.2347,  4.5214, -1.9116,  3.7263],
        [-3.7840,  1.6451,  0.5622,  4.3537,  3.6458,  3.3847, -2.1995,  3.5962],
        [-4.0656,  2.8614,  0.3650,  5.3471,  3.2521,  4.2099, -2.2820,  3.8480],
        [-3.7477,  2.7849,  0.0734,  5.4309,  2.8711,  3.9848, -1.9192,  3.7610],
        [-2.4944,  2.4984,  1.3522,  4.6822,  2.8620,  4.5090, -1.4889,  2.5130],
        [-3.1639,  2.1911,  0.8818,  3.7571,  3.0635,  2.5400, -1.6324,  3.3591],
        [-3.8316,  1.7810,  1.7913,  3.2921,  3.7401,  2.0290, -2.3835,  2.4476],
        [-3.8067,  2.7690,  1.5135,  4.9520,  2.5021,  2.7501, -2.3515,  2.9426]])...
	Related Layers:
		- parent layers: layernorm_3_111
		- child layers: transpose_15_186
		- shares parents with layers: linear_28_112, linear_29_113, linear_30_122, linear_31_124, linear_32_125, linear_33_134, linear_34_136, linear_35_137, linear_36_146, linear_37_148, linear_38_149, linear_39_158, linear_40_160, linear_41_161, linear_42_170, linear_43_172, linear_44_173, linear_45_182, linear_47_185, linear_48_194, linear_49_196, linear_50_197, linear_51_206
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.6.key:1
	Time elapsed:  1.144E-04s
	Output of modules: blocks.1.sa.heads.6.key
	Output of bottom-level module: blocks.1.sa.heads.6.key:1
	Lookup keys: -499, 199, blocks.1.sa.heads.6.key, blocks.1.sa.heads.6.key:1, linear_46, linear_46:1, linear_46_184, linear_46_184:1
--------------------------------------------
Layer linear_47_185, operation 186/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.6043,  0.2499,  0.3549, -1.3594, -2.1073, -0.3365, -0.1870, -2.3376],
        [ 1.5958,  0.2929,  1.2080, -0.8611, -1.8169,  0.3112, -0.9204, -1.8913],
        [ 0.3870,  1.0615,  1.4502, -0.4975, -1.5012,  0.4483, -1.6200, -1.5723],
        [ 0.7390,  1.0668,  1.1039, -0.4303, -1.0110,  0.3437, -1.3634, -2.0062],
        [ 0.6423,  0.3977,  0.7372, -1.0250, -0.3006, -0.8820, -0.4856, -1.5705],
        [ 1.2161,  0.9671, -0.1087, -0.4212, -1.7061,  1.0142, -1.3450, -1.8225],
        [ 1.2018,  0.7570,  0.7317, -0.1953, -2.1734,  1.2377, -0.7283, -1.4945],
        [ 0.8110,  0.2156,  1.2632, -0.6684, -1.0873,  0.3771, -0.2855, -0.5385]])...
	Related Layers:
		- parent layers: layernorm_3_111
		- child layers: matmul_29_187
		- shares parents with layers: linear_28_112, linear_29_113, linear_30_122, linear_31_124, linear_32_125, linear_33_134, linear_34_136, linear_35_137, linear_36_146, linear_37_148, linear_38_149, linear_39_158, linear_40_160, linear_41_161, linear_42_170, linear_43_172, linear_44_173, linear_45_182, linear_46_184, linear_48_194, linear_49_196, linear_50_197, linear_51_206
		- shares children with layers: transpose_15_186
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.6.query:1
	Time elapsed:  1.128E-04s
	Output of modules: blocks.1.sa.heads.6.query
	Output of bottom-level module: blocks.1.sa.heads.6.query:1
	Lookup keys: -498, 200, blocks.1.sa.heads.6.query, blocks.1.sa.heads.6.query:1, linear_47, linear_47:1, linear_47_185, linear_47_185:1
--------------------------------------------
Layer transpose_15_186, operation 187/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-4.2403, -3.7840, -4.0656, -3.7477, -2.4944, -3.1639, -3.8316, -3.8067],
        [ 2.8277,  1.6451,  2.8614,  2.7849,  2.4984,  2.1911,  1.7810,  2.7690],
        [ 1.1239,  0.5622,  0.3650,  0.0734,  1.3522,  0.8818,  1.7913,  1.5135],
        [ 5.2559,  4.3537,  5.3471,  5.4309,  4.6822,  3.7571,  3.2921,  4.9520],
        [ 4.2347,  3.6458,  3.2521,  2.8711,  2.8620,  3.0635,  3.7401,  2.5021],
        [ 4.5214,  3.3847,  4.2099,  3.9848,  4.5090,  2.5400,  2.0290,  2.7501],
        [-1.9116, -2.1995, -2.2820, -1.9192, -1.4889, -1.6324, -2.3835, -2.3515],
        [ 3.7263,  3.5962,  3.8480,  3.7610,  2.5130,  3.3591,  2.4476,  2.9426]])...
	Related Layers:
		- parent layers: linear_46_184
		- child layers: matmul_29_187
		- shares parents with no other layers
		- shares children with layers: linear_47_185
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.1.sa.heads.6:1
	Time elapsed:  6.318E-05s
	Output of modules: none
	Lookup keys: -497, 201, transpose_15, transpose_15:1, transpose_15_186, transpose_15_186:1
--------------------------------------------
Layer matmul_29_187, operation 188/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-99.6357, -95.2300, -89.2124, -83.1432, -69.7656, -74.8914, -76.0927,
         -79.2242],
        [-45.9244, -49.8767, -41.9641, -39.0426, -20.9582, -35.0136, -40.5667,
         -32.5986],
        [-23.8757, -30.5585, -20.5294, -17.8941,  -7.9977, -20.1386, -17.0215,
         -16.1610],
        [-47.8759, -48.6343, -42.0862, -38.3415, -27.1588, -36.6928, -28.9798,
         -34.8151],
        [-48.7487, -50.2315, -43.3921, -41.3164, -29.3564, -35.0479, -29.6684,
         -35.1640],
        [-37.1727, -53.4782, -43.1949, -40.7443, -31.6079, -33.3848, -40.8314,
         -43.0675],
        [-27.9948, -41.9032, -30.0244, -27.2967, -11.2046, -28.5457, -37.1083,
         -27.9231],
        [-23.6262, -19.6879, -16.0603, -15.2137,  -9.5501, -15.5436, -14.2078,
         -12.8427]])...
	Related Layers:
		- parent layers: linear_47_185, transpose_15_186
		- child layers: mul_15_188
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.6:1
	Time elapsed:  8.655E-05s
	Output of modules: none
	Lookup keys: -496, 202, matmul_29, matmul_29:1, matmul_29_187, matmul_29_187:1
--------------------------------------------
Layer mul_15_188, operation 189/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-5.0845, -4.8597, -4.5526, -4.2429, -3.5602, -3.8218, -3.8831, -4.0429],
        [-2.3436, -2.5453, -2.1415, -1.9924, -1.0695, -1.7868, -2.0702, -1.6635],
        [-1.2184, -1.5594, -1.0476, -0.9132, -0.4081, -1.0277, -0.8686, -0.8247],
        [-2.4432, -2.4819, -2.1477, -1.9566, -1.3859, -1.8725, -1.4789, -1.7767],
        [-2.4877, -2.5634, -2.2143, -2.1084, -1.4981, -1.7885, -1.5140, -1.7945],
        [-1.8970, -2.7290, -2.2043, -2.0792, -1.6130, -1.7037, -2.0837, -2.1978],
        [-1.4286, -2.1384, -1.5322, -1.3930, -0.5718, -1.4567, -1.8937, -1.4249],
        [-1.2057, -1.0047, -0.8196, -0.7764, -0.4874, -0.7932, -0.7250, -0.6554]])...
	Related Layers:
		- parent layers: matmul_29_187
		- child layers: maskedfill_15_191
		- shares parents with no other layers
		- shares children with layers: eq_15_190
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.1.sa.heads.6:1
	Time elapsed:  6.962E-05s
	Output of modules: none
	Lookup keys: -495, 203, mul_15, mul_15:1, mul_15_188, mul_15_188:1
--------------------------------------------
Layer getitem_15_189, operation 190/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_15
		- child layers: eq_15_190
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.537E-05s
	Output of modules: none
	Lookup keys: -493, 205, getitem_15, getitem_15:1, getitem_15_189, getitem_15_189:1
--------------------------------------------
Layer eq_15_190, operation 191/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_15_189
		- child layers: maskedfill_15_191
		- shares parents with no other layers
		- shares children with layers: mul_15_188
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.273E-05s
	Output of modules: none
	Lookup keys: -492, 206, eq_15, eq_15:1, eq_15_190, eq_15_190:1
--------------------------------------------
Layer maskedfill_15_191, operation 192/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-5.0845,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-2.3436, -2.5453,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-1.2184, -1.5594, -1.0476,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-2.4432, -2.4819, -2.1477, -1.9566,    -inf,    -inf,    -inf,    -inf],
        [-2.4877, -2.5634, -2.2143, -2.1084, -1.4981,    -inf,    -inf,    -inf],
        [-1.8970, -2.7290, -2.2043, -2.0792, -1.6130, -1.7037,    -inf,    -inf],
        [-1.4286, -2.1384, -1.5322, -1.3930, -0.5718, -1.4567, -1.8937,    -inf],
        [-1.2057, -1.0047, -0.8196, -0.7764, -0.4874, -0.7932, -0.7250, -0.6554]])...
	Related Layers:
		- parent layers: mul_15_188, eq_15_190
		- child layers: softmax_15_192
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.1.sa.heads.6:1
	Time elapsed:  8.368E-05s
	Output of modules: none
	Lookup keys: -491, 207, maskedfill_15, maskedfill_15:1, maskedfill_15_191, maskedfill_15_191:1
--------------------------------------------
Layer softmax_15_192, operation 193/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5503, 0.4497, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3452, 0.2454, 0.4094, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2027, 0.1950, 0.2724, 0.3298, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1353, 0.1254, 0.1778, 0.1977, 0.3639, 0.0000, 0.0000, 0.0000],
        [0.1803, 0.0785, 0.1326, 0.1503, 0.2395, 0.2188, 0.0000, 0.0000],
        [0.1354, 0.0666, 0.1221, 0.1403, 0.3190, 0.1316, 0.0850, 0.0000],
        [0.0824, 0.1007, 0.1212, 0.1265, 0.1689, 0.1244, 0.1332, 0.1428]])...
	Related Layers:
		- parent layers: maskedfill_15_191
		- child layers: dropout_17_193
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.1.sa.heads.6:1
	Time elapsed:  7.105E-05s
	Output of modules: none
	Lookup keys: -490, 208, softmax_15, softmax_15:1, softmax_15_192, softmax_15_192:1
--------------------------------------------
Layer dropout_17_193, operation 194/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5503, 0.4497, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3452, 0.2454, 0.4094, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2027, 0.1950, 0.2724, 0.3298, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1353, 0.1254, 0.1778, 0.1977, 0.3639, 0.0000, 0.0000, 0.0000],
        [0.1803, 0.0785, 0.1326, 0.1503, 0.2395, 0.2188, 0.0000, 0.0000],
        [0.1354, 0.0666, 0.1221, 0.1403, 0.3190, 0.1316, 0.0850, 0.0000],
        [0.0824, 0.1007, 0.1212, 0.1265, 0.1689, 0.1244, 0.1332, 0.1428]])...
	Related Layers:
		- parent layers: softmax_15_192
		- child layers: matmul_30_195
		- shares parents with no other layers
		- shares children with layers: linear_48_194
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.1.sa.heads.6.dropout:1
	Time elapsed:  5.293E-05s
	Output of modules: blocks.1.sa.heads.6.dropout
	Output of bottom-level module: blocks.1.sa.heads.6.dropout:1
	Lookup keys: -489, 209, blocks.1.sa.heads.6.dropout, blocks.1.sa.heads.6.dropout:1, dropout_17, dropout_17:1, dropout_17_193, dropout_17_193:1
--------------------------------------------
Layer linear_48_194, operation 195/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 3.2355e-01, -1.0003e-01,  7.5759e-01,  6.4633e-01,  4.5211e-01,
         -6.8910e-01, -4.1465e-04,  5.7573e-01],
        [ 2.8556e-01,  2.2408e-01,  3.6760e-01,  1.5149e+00,  1.1476e-02,
         -1.0096e+00,  6.3043e-01,  3.5022e-01],
        [ 2.4073e-02,  4.3771e-02,  7.4715e-01,  1.0271e+00,  1.6651e-01,
         -9.7661e-01,  1.3440e-01, -2.1457e-01],
        [-3.0447e-01,  2.3956e-01,  7.1754e-01,  8.8768e-01,  6.4464e-01,
         -1.0721e+00,  2.2113e-01, -1.9923e-01],
        [ 9.0221e-01, -2.6837e-01,  5.2785e-01,  3.6750e-01,  1.2520e+00,
         -4.4256e-01,  2.7047e-01,  7.9642e-01],
        [-1.5368e+00,  9.7198e-01,  5.0060e-02,  1.1446e+00, -7.3421e-02,
         -6.8477e-01,  5.1636e-01, -1.3413e-01],
        [ 1.3321e+00,  8.1165e-01, -2.1240e-01,  1.7169e+00,  3.6067e-01,
         -4.5478e-01,  4.9063e-01, -4.1809e-01],
        [ 3.5852e-01, -1.2183e-01,  8.6606e-01,  9.7204e-01,  3.7590e-01,
         -1.1733e+00,  4.7903e-01, -3.1395e-02]])...
	Related Layers:
		- parent layers: layernorm_3_111
		- child layers: matmul_30_195
		- shares parents with layers: linear_28_112, linear_29_113, linear_30_122, linear_31_124, linear_32_125, linear_33_134, linear_34_136, linear_35_137, linear_36_146, linear_37_148, linear_38_149, linear_39_158, linear_40_160, linear_41_161, linear_42_170, linear_43_172, linear_44_173, linear_45_182, linear_46_184, linear_47_185, linear_49_196, linear_50_197, linear_51_206
		- shares children with layers: dropout_17_193
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.6.value:1
	Time elapsed:  1.242E-04s
	Output of modules: blocks.1.sa.heads.6.value
	Output of bottom-level module: blocks.1.sa.heads.6.value:1
	Lookup keys: -488, 210, blocks.1.sa.heads.6.value, blocks.1.sa.heads.6.value:1, linear_48, linear_48:1, linear_48_194, linear_48_194:1
--------------------------------------------
Layer matmul_30_195, operation 196/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 3.2355e-01, -1.0003e-01,  7.5759e-01,  6.4633e-01,  4.5211e-01,
         -6.8910e-01, -4.1465e-04,  5.7573e-01],
        [ 3.0647e-01,  4.5738e-02,  5.8219e-01,  1.0370e+00,  2.5394e-01,
         -8.3326e-01,  2.8331e-01,  4.7431e-01],
        [ 1.9161e-01,  3.8389e-02,  6.5760e-01,  1.0154e+00,  2.2704e-01,
         -8.8548e-01,  2.0961e-01,  1.9682e-01],
        [ 2.7439e-02,  1.1435e-01,  6.6547e-01,  9.9907e-01,  3.5186e-01,
         -9.5624e-01,  2.3242e-01,  6.0872e-02],
        [ 3.5198e-01, -2.7955e-02,  6.1531e-01,  7.6920e-01,  6.7519e-01,
         -7.6639e-01,  2.4503e-01,  3.3408e-01],
        [-8.1905e-02,  1.8971e-01,  5.0976e-01,  8.4347e-01,  4.8522e-01,
         -7.4993e-01,  2.7820e-01,  2.3433e-01],
        [ 2.2178e-01,  1.5171e-01,  4.7582e-01,  8.5221e-01,  5.9308e-01,
         -7.0014e-01,  2.8532e-01,  2.4793e-01],
        [ 2.0961e-01,  2.1622e-01,  4.7147e-01,  1.0144e+00,  4.4417e-01,
         -8.0041e-01,  3.5136e-01,  8.9149e-02]])...
	Related Layers:
		- parent layers: dropout_17_193, linear_48_194
		- child layers: cat_2_208
		- shares parents with no other layers
		- shares children with layers: matmul_18_123, matmul_20_135, matmul_22_147, matmul_24_159, matmul_26_171, matmul_28_183, matmul_32_207
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.6:1
	Time elapsed:  8.249E-05s
	Output of modules: blocks.1.sa.heads.6
	Lookup keys: -487, 211, blocks.1.sa.heads.6, blocks.1.sa.heads.6:1, matmul_30, matmul_30:1, matmul_30_195, matmul_30_195:1
--------------------------------------------
Layer linear_49_196, operation 197/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.3421,  4.7402,  6.3065, -2.5195,  4.9173,  3.3865,  5.5172,  4.3553],
        [ 1.1008,  4.4448,  4.2839, -1.7220,  3.0292,  2.6868,  3.4418,  2.9663],
        [ 0.2172,  4.8122,  4.9685, -2.1444,  4.0298,  3.7038,  4.7111,  4.1521],
        [ 0.5997,  4.9377,  4.8657, -2.4619,  4.4287,  3.2901,  4.2350,  3.9074],
        [ 0.5332,  3.4336,  4.6260, -2.0571,  3.9053,  3.0519,  4.3237,  3.4008],
        [-1.2149,  6.1294,  4.3640, -2.4841,  3.4757,  4.0787,  5.2321,  3.5626],
        [-1.0619,  4.4245,  2.7022, -2.0810,  2.9830,  3.1468,  3.9735,  2.1124],
        [-0.1568,  4.4793,  4.3646, -1.2060,  3.8483,  2.6792,  4.4398,  2.9948]])...
	Related Layers:
		- parent layers: layernorm_3_111
		- child layers: transpose_16_198
		- shares parents with layers: linear_28_112, linear_29_113, linear_30_122, linear_31_124, linear_32_125, linear_33_134, linear_34_136, linear_35_137, linear_36_146, linear_37_148, linear_38_149, linear_39_158, linear_40_160, linear_41_161, linear_42_170, linear_43_172, linear_44_173, linear_45_182, linear_46_184, linear_47_185, linear_48_194, linear_50_197, linear_51_206
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.7.key:1
	Time elapsed:  1.135E-04s
	Output of modules: blocks.1.sa.heads.7.key
	Output of bottom-level module: blocks.1.sa.heads.7.key:1
	Lookup keys: -486, 212, blocks.1.sa.heads.7.key, blocks.1.sa.heads.7.key:1, linear_49, linear_49:1, linear_49_196, linear_49_196:1
--------------------------------------------
Layer linear_50_197, operation 198/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-2.5661,  0.0902, -0.1722,  3.3745,  2.6052, -0.8460, -2.6023, -0.7218],
        [-2.3590,  0.7259, -0.8293,  2.9291,  1.8517, -0.6820, -2.3495, -0.9466],
        [-3.1255, -0.0740, -0.6281,  2.9019,  1.1973, -0.8082, -2.2662, -0.8129],
        [-2.5350,  0.1159, -0.7430,  2.9816,  0.7270, -0.3303, -1.1614, -0.7334],
        [-2.5080,  0.4243, -0.2066,  2.7115,  2.7293, -0.1819, -1.2547, -0.1883],
        [ 0.8281, -0.7877,  0.8732,  2.3003,  3.3157, -1.4739, -2.7382, -1.1257],
        [-0.3427, -0.2568, -0.4232,  1.7555,  2.0074, -1.0318, -3.3099, -1.0390],
        [-1.2274,  0.2482, -1.9106,  1.5242,  1.2190,  0.0744, -2.2434, -1.0879]])...
	Related Layers:
		- parent layers: layernorm_3_111
		- child layers: matmul_31_199
		- shares parents with layers: linear_28_112, linear_29_113, linear_30_122, linear_31_124, linear_32_125, linear_33_134, linear_34_136, linear_35_137, linear_36_146, linear_37_148, linear_38_149, linear_39_158, linear_40_160, linear_41_161, linear_42_170, linear_43_172, linear_44_173, linear_45_182, linear_46_184, linear_47_185, linear_48_194, linear_49_196, linear_51_206
		- shares children with layers: transpose_16_198
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.7.query:1
	Time elapsed:  1.140E-04s
	Output of modules: blocks.1.sa.heads.7.query
	Output of bottom-level module: blocks.1.sa.heads.7.query:1
	Lookup keys: -485, 213, blocks.1.sa.heads.7.query, blocks.1.sa.heads.7.query:1, linear_50, linear_50:1, linear_50_197, linear_50_197:1
--------------------------------------------
Layer transpose_16_198, operation 199/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.3421,  1.1008,  0.2172,  0.5997,  0.5332, -1.2149, -1.0619, -0.1568],
        [ 4.7402,  4.4448,  4.8122,  4.9377,  3.4336,  6.1294,  4.4245,  4.4793],
        [ 6.3065,  4.2839,  4.9685,  4.8657,  4.6260,  4.3640,  2.7022,  4.3646],
        [-2.5195, -1.7220, -2.1444, -2.4619, -2.0571, -2.4841, -2.0810, -1.2060],
        [ 4.9173,  3.0292,  4.0298,  4.4287,  3.9053,  3.4757,  2.9830,  3.8483],
        [ 3.3865,  2.6868,  3.7038,  3.2901,  3.0519,  4.0787,  3.1468,  2.6792],
        [ 5.5172,  3.4418,  4.7111,  4.2350,  4.3237,  5.2321,  3.9735,  4.4398],
        [ 4.3553,  2.9663,  4.1521,  3.9074,  3.4008,  3.5626,  2.1124,  2.9948]])...
	Related Layers:
		- parent layers: linear_49_196
		- child layers: matmul_31_199
		- shares parents with no other layers
		- shares children with layers: linear_50_197
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.1.sa.heads.7:1
	Time elapsed:  5.841E-05s
	Output of modules: none
	Lookup keys: -484, 214, transpose_16, transpose_16:1, transpose_16_198, transpose_16_198:1
--------------------------------------------
Layer matmul_31_199, operation 200/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-130.8586,  -92.3865, -106.3950, -104.1938,  -88.7111, -111.2258,
          -80.2983,  -77.7967],
        [-149.0414, -103.4729, -114.9106, -113.7876,  -98.7507, -116.2170,
          -76.5577,  -88.5502],
        [-152.7204, -106.3341, -118.7522, -117.1303, -108.5247, -118.6249,
          -80.3465,  -95.4820],
        [-136.1959,  -94.7886, -103.3439, -104.3559,  -93.4714, -103.3467,
          -68.1035,  -81.8049],
        [ -64.2794,  -45.0344,  -45.7564,  -43.1979,  -39.6633,  -49.2529,
          -31.2225,  -34.2557],
        [-135.7507, -100.9239, -131.7811, -123.4700, -106.1824, -145.7783,
         -118.9229,  -98.9421],
        [-169.2080, -119.8782, -143.9806, -138.2174, -121.7816, -151.2365,
         -108.2072, -114.1325],
        [-161.2835, -106.9836, -128.6751, -125.0287, -119.4645, -134.2359,
          -93.2586, -108.0370]])...
	Related Layers:
		- parent layers: linear_50_197, transpose_16_198
		- child layers: mul_16_200
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.7:1
	Time elapsed:  8.750E-05s
	Output of modules: none
	Lookup keys: -483, 215, matmul_31, matmul_31:1, matmul_31_199, matmul_31_199:1
--------------------------------------------
Layer mul_16_200, operation 201/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-6.6778, -4.7146, -5.4294, -5.3171, -4.5270, -5.6760, -4.0977, -3.9700],
        [-7.6057, -5.2803, -5.8640, -5.8067, -5.0394, -5.9307, -3.9068, -4.5188],
        [-7.7935, -5.4263, -6.0600, -5.9773, -5.5381, -6.0536, -4.1002, -4.8725],
        [-6.9502, -4.8372, -5.2737, -5.3254, -4.7699, -5.2739, -3.4754, -4.1746],
        [-3.2802, -2.2982, -2.3350, -2.2044, -2.0241, -2.5134, -1.5933, -1.7481],
        [-6.9275, -5.1502, -6.7249, -6.3008, -5.4186, -7.4392, -6.0688, -5.0491],
        [-8.6349, -6.1175, -7.3475, -7.0534, -6.2146, -7.7178, -5.5219, -5.8243],
        [-8.2305, -5.4595, -6.5664, -6.3803, -6.0964, -6.8502, -4.7591, -5.5132]])...
	Related Layers:
		- parent layers: matmul_31_199
		- child layers: maskedfill_16_203
		- shares parents with no other layers
		- shares children with layers: eq_16_202
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.1.sa.heads.7:1
	Time elapsed:  6.914E-05s
	Output of modules: none
	Lookup keys: -482, 216, mul_16, mul_16:1, mul_16_200, mul_16_200:1
--------------------------------------------
Layer getitem_16_201, operation 202/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_16
		- child layers: eq_16_202
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.203E-05s
	Output of modules: none
	Lookup keys: -480, 218, getitem_16, getitem_16:1, getitem_16_201, getitem_16_201:1
--------------------------------------------
Layer eq_16_202, operation 203/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_16_201
		- child layers: maskedfill_16_203
		- shares parents with no other layers
		- shares children with layers: mul_16_200
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  7.749E-05s
	Output of modules: none
	Lookup keys: -479, 219, eq_16, eq_16:1, eq_16_202, eq_16_202:1
--------------------------------------------
Layer maskedfill_16_203, operation 204/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-6.6778,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-7.6057, -5.2803,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-7.7935, -5.4263, -6.0600,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-6.9502, -4.8372, -5.2737, -5.3254,    -inf,    -inf,    -inf,    -inf],
        [-3.2802, -2.2982, -2.3350, -2.2044, -2.0241,    -inf,    -inf,    -inf],
        [-6.9275, -5.1502, -6.7249, -6.3008, -5.4186, -7.4392,    -inf,    -inf],
        [-8.6349, -6.1175, -7.3475, -7.0534, -6.2146, -7.7178, -5.5219,    -inf],
        [-8.2305, -5.4595, -6.5664, -6.3803, -6.0964, -6.8502, -4.7591, -5.5132]])...
	Related Layers:
		- parent layers: mul_16_200, eq_16_202
		- child layers: softmax_16_204
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.1.sa.heads.7:1
	Time elapsed:  8.225E-05s
	Output of modules: none
	Lookup keys: -478, 220, maskedfill_16, maskedfill_16:1, maskedfill_16_203, maskedfill_16_203:1
--------------------------------------------
Layer softmax_16_204, operation 205/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0890, 0.9110, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0577, 0.6156, 0.3267, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0508, 0.4200, 0.2714, 0.2578, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0788, 0.2104, 0.2028, 0.2311, 0.2768, 0.0000, 0.0000, 0.0000],
        [0.0661, 0.3908, 0.0809, 0.1237, 0.2988, 0.0396, 0.0000, 0.0000],
        [0.0172, 0.2133, 0.0623, 0.0837, 0.1935, 0.0431, 0.3869, 0.0000],
        [0.0113, 0.1808, 0.0598, 0.0720, 0.0956, 0.0450, 0.3642, 0.1713]])...
	Related Layers:
		- parent layers: maskedfill_16_203
		- child layers: dropout_18_205
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.1.sa.heads.7:1
	Time elapsed:  6.771E-05s
	Output of modules: none
	Lookup keys: -477, 221, softmax_16, softmax_16:1, softmax_16_204, softmax_16_204:1
--------------------------------------------
Layer dropout_18_205, operation 206/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0890, 0.9110, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0577, 0.6156, 0.3267, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0508, 0.4200, 0.2714, 0.2578, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0788, 0.2104, 0.2028, 0.2311, 0.2768, 0.0000, 0.0000, 0.0000],
        [0.0661, 0.3908, 0.0809, 0.1237, 0.2988, 0.0396, 0.0000, 0.0000],
        [0.0172, 0.2133, 0.0623, 0.0837, 0.1935, 0.0431, 0.3869, 0.0000],
        [0.0113, 0.1808, 0.0598, 0.0720, 0.0956, 0.0450, 0.3642, 0.1713]])...
	Related Layers:
		- parent layers: softmax_16_204
		- child layers: matmul_32_207
		- shares parents with no other layers
		- shares children with layers: linear_51_206
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.1.sa.heads.7.dropout:1
	Time elapsed:  5.198E-05s
	Output of modules: blocks.1.sa.heads.7.dropout
	Output of bottom-level module: blocks.1.sa.heads.7.dropout:1
	Lookup keys: -476, 222, blocks.1.sa.heads.7.dropout, blocks.1.sa.heads.7.dropout:1, dropout_18, dropout_18:1, dropout_18_205, dropout_18_205:1
--------------------------------------------
Layer linear_51_206, operation 207/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.7142,  0.5181,  1.0227,  0.4711,  0.2062, -0.8850,  0.0229,  0.2262],
        [ 0.4953, -0.4040,  1.4802,  0.5209, -0.1587, -2.6301, -0.4144, -0.0532],
        [ 0.5786, -0.1471,  1.2523,  0.2251, -0.1822, -1.6747, -0.3362,  0.1177],
        [ 0.2432, -0.2093,  1.0637, -0.0226, -0.2826, -1.9808, -0.7550,  0.2358],
        [ 0.6298,  1.0797,  0.3232, -0.4972,  0.1949, -1.0618, -0.6630,  0.2700],
        [ 0.1674,  0.7901,  1.3037, -0.1110, -0.6315, -1.9905,  0.3742, -0.7377],
        [ 0.5668, -0.4325,  1.0179,  0.7442, -1.7043, -3.5999,  0.8530, -0.1081],
        [ 1.1589,  0.3991,  0.7976,  0.1094,  0.4720, -1.8456, -0.0158, -0.1678]])...
	Related Layers:
		- parent layers: layernorm_3_111
		- child layers: matmul_32_207
		- shares parents with layers: linear_28_112, linear_29_113, linear_30_122, linear_31_124, linear_32_125, linear_33_134, linear_34_136, linear_35_137, linear_36_146, linear_37_148, linear_38_149, linear_39_158, linear_40_160, linear_41_161, linear_42_170, linear_43_172, linear_44_173, linear_45_182, linear_46_184, linear_47_185, linear_48_194, linear_49_196, linear_50_197
		- shares children with layers: dropout_18_205
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.7.value:1
	Time elapsed:  1.187E-04s
	Output of modules: blocks.1.sa.heads.7.value
	Output of bottom-level module: blocks.1.sa.heads.7.value:1
	Lookup keys: -475, 223, blocks.1.sa.heads.7.value, blocks.1.sa.heads.7.value:1, linear_51, linear_51:1, linear_51_206, linear_51_206:1
--------------------------------------------
Layer matmul_32_207, operation 208/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 7.1425e-01,  5.1807e-01,  1.0227e+00,  4.7106e-01,  2.0622e-01,
         -8.8497e-01,  2.2891e-02,  2.2618e-01],
        [ 5.1477e-01, -3.2187e-01,  1.4394e+00,  5.1647e-01, -1.2622e-01,
         -2.4747e+00, -3.7545e-01, -2.8302e-02],
        [ 5.3512e-01, -2.6684e-01,  1.3793e+00,  4.2140e-01, -1.4533e-01,
         -2.2173e+00, -3.6360e-01,  1.8755e-02],
        [ 4.6401e-01, -2.3725e-01,  1.2877e+00,  2.9798e-01, -1.7850e-01,
         -2.1148e+00, -4.5876e-01,  8.1861e-02],
        [ 5.0840e-01,  1.7649e-01,  9.8138e-01,  4.9556e-02, -6.5459e-02,
         -1.7146e+00, -5.1159e-01,  1.5972e-01],
        [ 5.1253e-01,  1.9254e-01,  1.0272e+00,  9.7160e-02, -6.4865e-02,
         -1.8631e+00, -4.6434e-01,  8.4302e-02],
        [ 5.2276e-01, -2.8288e-02,  1.0129e+00,  3.1830e-01, -7.1417e-01,
         -2.5304e+00,  4.5717e-02, -1.7144e-03],
        [ 6.2245e-01, -4.1375e-02,  1.0275e+00,  3.4859e-01, -6.0720e-01,
         -2.5466e+00,  1.1229e-01, -5.8549e-02]])...
	Related Layers:
		- parent layers: dropout_18_205, linear_51_206
		- child layers: cat_2_208
		- shares parents with no other layers
		- shares children with layers: matmul_18_123, matmul_20_135, matmul_22_147, matmul_24_159, matmul_26_171, matmul_28_183, matmul_30_195
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.1.sa.heads.7:1
	Time elapsed:  8.273E-05s
	Output of modules: blocks.1.sa.heads.7
	Lookup keys: -474, 224, blocks.1.sa.heads.7, blocks.1.sa.heads.7:1, matmul_32, matmul_32:1, matmul_32_207, matmul_32_207:1
--------------------------------------------
Layer cat_2_208, operation 209/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-1.6575,  0.1352,  0.7676,  0.3312, -0.9955, -0.1754,  0.3324, -0.4713],
        [-1.2009,  0.3998,  0.7078,  0.2063, -0.8505, -0.3128, -0.0103,  0.4933],
        [-0.9445, -0.3015,  0.5858,  0.1607, -0.6361, -0.2058,  0.2365,  0.5505],
        [-0.7932, -0.4778,  0.5573,  0.0480, -0.7011, -0.1117,  0.2992,  0.5622],
        [-0.9049, -0.3552,  0.6385,  0.1396, -0.7808, -0.2816,  0.2594,  0.6201],
        [-0.7908, -0.3980,  0.5902,  0.0875, -0.7096, -0.2257,  0.2390,  0.7262],
        [-0.7980, -0.4933,  0.5985,  0.0958, -0.6986, -0.2207,  0.2784,  0.6842],
        [-0.9253, -0.1456,  0.6012,  0.0074, -0.7050,  0.0328,  0.1980,  0.5046]])...
	Related Layers:
		- parent layers: matmul_18_123, matmul_20_135, matmul_22_147, matmul_24_159, matmul_26_171, matmul_28_183, matmul_30_195, matmul_32_207
		- child layers: linear_52_209
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: cat (grad_fn: CatBackward0) 
	Computed inside module: blocks.1.sa:1
	Time elapsed:  1.016E-04s
	Output of modules: none
	Lookup keys: -473, 225, cat_2, cat_2:1, cat_2_208, cat_2_208:1
--------------------------------------------
Layer linear_52_209, operation 210/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[ 1.3400,  0.8271, -1.5619, -2.6520, -1.0974,  0.2974, -1.0069,  0.9567],
        [ 1.8595,  0.9129, -1.0355, -2.3873, -1.0014,  0.0420, -0.7694,  0.2942],
        [ 1.4958,  0.6447, -0.9347, -2.0617, -0.8659,  0.2485, -0.6887,  0.4735],
        [ 1.3637,  0.5721, -0.8312, -2.0437, -0.7473,  0.3592, -0.6739,  0.5141],
        [ 1.4160,  0.3464, -0.8366, -2.1412, -0.8217,  0.5066, -0.6486,  0.4633],
        [ 1.4358,  0.4061, -0.8871, -2.1020, -0.9264,  0.5329, -0.5553,  0.5072],
        [ 1.5556,  0.2913, -1.0664, -2.2139, -0.8279,  0.1115, -0.9193,  0.5243],
        [ 1.5993,  0.4533, -1.1069, -1.9744, -0.8318,  0.0187, -0.9681,  0.5409]])...
	Related Layers:
		- parent layers: cat_2_208
		- child layers: dropout_19_210
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (384, 384), (384,); 147840 params total (577.7 KB)
	Function: linear (grad_fn: ViewBackward0) 
	Computed inside module: blocks.1.sa.proj:1
	Time elapsed:  5.703E-04s
	Output of modules: blocks.1.sa.proj
	Output of bottom-level module: blocks.1.sa.proj:1
	Lookup keys: -472, 226, blocks.1.sa.proj, blocks.1.sa.proj:1, linear_52, linear_52:1, linear_52_209, linear_52_209:1
--------------------------------------------
Layer dropout_19_210, operation 211/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[ 1.3400,  0.8271, -1.5619, -2.6520, -1.0974,  0.2974, -1.0069,  0.9567],
        [ 1.8595,  0.9129, -1.0355, -2.3873, -1.0014,  0.0420, -0.7694,  0.2942],
        [ 1.4958,  0.6447, -0.9347, -2.0617, -0.8659,  0.2485, -0.6887,  0.4735],
        [ 1.3637,  0.5721, -0.8312, -2.0437, -0.7473,  0.3592, -0.6739,  0.5141],
        [ 1.4160,  0.3464, -0.8366, -2.1412, -0.8217,  0.5066, -0.6486,  0.4633],
        [ 1.4358,  0.4061, -0.8871, -2.1020, -0.9264,  0.5329, -0.5553,  0.5072],
        [ 1.5556,  0.2913, -1.0664, -2.2139, -0.8279,  0.1115, -0.9193,  0.5243],
        [ 1.5993,  0.4533, -1.1069, -1.9744, -0.8318,  0.0187, -0.9681,  0.5409]])...
	Related Layers:
		- parent layers: linear_52_209
		- child layers: add_3_211:1
		- shares parents with no other layers
		- shares children with layers: add_2_105:2
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: ViewBackward0) 
	Computed inside module: blocks.1.sa.dropout:1
	Time elapsed:  7.510E-05s
	Output of modules: blocks.1.sa.dropout, blocks.1.sa
	Output of bottom-level module: blocks.1.sa.dropout:1
	Lookup keys: -471, 227, blocks.1.sa, blocks.1.sa.dropout, blocks.1.sa.dropout:1, blocks.1.sa:1, dropout_19, dropout_19:1, dropout_19_210, dropout_19_210:1
--------------------------------------------
Layer add_3_211 (pass 1/2), operation 212/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-3.9094, -4.2240,  1.8379, -5.4863, -0.5869,  6.9193, -0.5634,  4.9044],
        [-0.8538, -6.6686,  3.4958, -3.8640, -1.7990,  4.1145, -1.6338,  2.0322],
        [-2.4219, -6.8265,  2.7005, -4.5804, -3.0523,  2.7034, -1.2994,  0.9082],
        [-2.0413, -6.6924,  4.8074, -6.2397, -1.6584,  2.5121, -0.7422,  2.7363],
        [-0.6283, -5.8089,  3.4494, -6.4769,  2.4696,  1.6267, -0.2882,  4.1026],
        [ 1.2214, -1.9914, -0.8582, -3.5778, -0.2847,  4.6757, -3.3348,  3.5968],
        [-2.0740, -2.8691,  2.4582, -4.5660,  1.8602,  9.0475, -2.0476,  0.3430],
        [-4.6934, -3.4285,  3.5095, -6.1442, -0.2094,  2.0177, -5.7766,  1.5632]])...
	Related Layers:
		- parent layers: add_2_105:2, dropout_19_210
		- child layers: layernorm_4_212, add_3_211:2
		- shares parents with layers: layernorm_3_111
		- shares children with layers: dropout_20_216
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __add__ (grad_fn: AddBackward0) 
	Computed inside module: blocks.1:1
	Time elapsed:  8.678E-05s
	Output of modules: none
	Lookup keys: -470, 228, add_3:1, add_3_211:1
--------------------------------------------
Layer layernorm_4_212, operation 213/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-1.1155, -1.2180,  0.4364, -1.5510, -0.2125,  1.8437, -0.2164,  1.2747],
        [-0.2640, -2.0572,  1.0482, -1.1961, -0.5617,  1.2553, -0.5271,  0.6056],
        [-0.7765, -2.2057,  0.8571, -1.4794, -0.9912,  0.8677, -0.4347,  0.2830],
        [-0.6733, -2.1839,  1.5141, -2.0325, -0.5568,  0.7865, -0.2693,  0.8555],
        [-0.1760, -1.8794,  1.1330, -2.0891,  0.8415,  0.5447, -0.0755,  1.3541],
        [ 0.4053, -0.6666, -0.2846, -1.1697, -0.0900,  1.5305, -1.1231,  1.1640],
        [-0.5785, -0.8300,  0.7156, -1.3080,  0.5647,  2.6606, -0.5964,  0.1059],
        [-1.6584, -1.2270,  1.2208, -2.1720, -0.0739,  0.7034, -2.1106,  0.5403]])...
	Related Layers:
		- parent layers: add_3_211:1
		- child layers: linear_53_213
		- shares parents with layers: add_3_211:2
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (384,), (384,); 768 params total (3.2 KB)
	Function: layer_norm (grad_fn: NativeLayerNormBackward0) 
	Computed inside module: blocks.1.ln2:1
	Time elapsed:  3.440E-04s
	Output of modules: blocks.1.ln2
	Output of bottom-level module: blocks.1.ln2:1
	Lookup keys: -469, 229, blocks.1.ln2, blocks.1.ln2:1, layernorm_4, layernorm_4:1, layernorm_4_212, layernorm_4_212:1
--------------------------------------------
Layer linear_53_213, operation 214/648:
	Output tensor: shape=1x11x1536(67584 bytes), dype=torch.float32, Tensor size=67728 bytes, Tensor storage size=67648
		tensor([[ 0.2655,  0.2304,  0.1884, -0.4583, -0.4528,  0.0854, -0.5814, -1.3987],
        [ 1.9019,  0.1918,  0.3798, -0.3144, -0.5127, -0.1345, -0.9078,  1.6408],
        [ 0.6392,  0.8140,  0.2096, -0.4997, -0.5911,  0.0771, -1.0992, -0.0463],
        [ 0.0957,  0.9616,  0.2554, -0.3159, -0.6816,  0.2550, -0.1033, -0.4266],
        [-0.2143,  0.2814,  0.1450, -0.3822, -0.7917, -0.2593, -0.9362, -1.1957],
        [ 0.7211,  0.0567, -0.1888, -0.4486,  0.5786, -0.5374, -0.1377, -1.6903],
        [ 0.7481, -0.7297,  0.2842, -0.4079,  0.4691, -0.2851, -0.2670,  1.5289],
        [ 0.3849,  0.5226,  1.1613, -0.8698, -0.9359,  1.6691, -0.0745, -0.2917]])...
	Related Layers:
		- parent layers: layernorm_4_212
		- child layers: relu_2_214
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (1536, 384), (1536,); 591360 params total (2.3 MB)
	Function: linear (grad_fn: ViewBackward0) 
	Computed inside module: blocks.1.ffwd.net.0:1
	Time elapsed:  8.023E-04s
	Output of modules: blocks.1.ffwd.net.0
	Output of bottom-level module: blocks.1.ffwd.net.0:1
	Lookup keys: -468, 230, blocks.1.ffwd.net.0, blocks.1.ffwd.net.0:1, linear_53, linear_53:1, linear_53_213, linear_53_213:1
--------------------------------------------
Layer relu_2_214, operation 215/648:
	Output tensor: shape=1x11x1536(67584 bytes), dype=torch.float32, Tensor size=67728 bytes, Tensor storage size=67648
		tensor([[0.2655, 0.2304, 0.1884, 0.0000, 0.0000, 0.0854, 0.0000, 0.0000],
        [1.9019, 0.1918, 0.3798, 0.0000, 0.0000, 0.0000, 0.0000, 1.6408],
        [0.6392, 0.8140, 0.2096, 0.0000, 0.0000, 0.0771, 0.0000, 0.0000],
        [0.0957, 0.9616, 0.2554, 0.0000, 0.0000, 0.2550, 0.0000, 0.0000],
        [0.0000, 0.2814, 0.1450, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.7211, 0.0567, 0.0000, 0.0000, 0.5786, 0.0000, 0.0000, 0.0000],
        [0.7481, 0.0000, 0.2842, 0.0000, 0.4691, 0.0000, 0.0000, 1.5289],
        [0.3849, 0.5226, 1.1613, 0.0000, 0.0000, 1.6691, 0.0000, 0.0000]])...
	Related Layers:
		- parent layers: linear_53_213
		- child layers: linear_54_215
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: relu (grad_fn: ReluBackward0) 
	Computed inside module: blocks.1.ffwd.net.1:1
	Time elapsed:  9.370E-05s
	Output of modules: blocks.1.ffwd.net.1
	Output of bottom-level module: blocks.1.ffwd.net.1:1
	Lookup keys: -467, 231, blocks.1.ffwd.net.1, blocks.1.ffwd.net.1:1, relu_2, relu_2:1, relu_2_214, relu_2_214:1
--------------------------------------------
Layer linear_54_215, operation 216/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-1.5139,  0.3185,  0.6868,  1.0144,  0.3297, -0.2366,  0.3504,  1.4877],
        [-1.4202,  0.6199,  1.3104,  0.9060,  1.2189,  0.5385,  0.9483,  2.4341],
        [-1.5732, -0.1735,  1.4476,  0.4526, -0.2312, -1.1022,  0.4139,  1.7184],
        [-1.6503, -0.3014,  1.6804,  0.6234, -0.2562, -1.0508,  0.4485,  1.8223],
        [-1.4565,  0.1482,  1.1580,  0.7566,  0.4201, -0.6067,  0.3342,  1.7619],
        [-0.6977,  0.1493,  1.7259,  0.8906,  0.6999,  1.8929,  1.5605,  2.3579],
        [-0.7214,  1.3689,  1.3530,  1.0794,  1.0662,  1.8048,  1.5006,  1.0747],
        [-1.5670,  0.4060,  1.5135,  0.9469,  1.5299, -0.2881,  0.5672,  1.6549]])...
	Related Layers:
		- parent layers: relu_2_214
		- child layers: dropout_20_216
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (384, 1536), (384,); 590208 params total (2.3 MB)
	Function: linear (grad_fn: ViewBackward0) 
	Computed inside module: blocks.1.ffwd.net.2:1
	Time elapsed:  8.993E-04s
	Output of modules: blocks.1.ffwd.net.2
	Output of bottom-level module: blocks.1.ffwd.net.2:1
	Lookup keys: -466, 232, blocks.1.ffwd.net.2, blocks.1.ffwd.net.2:1, linear_54, linear_54:1, linear_54_215, linear_54_215:1
--------------------------------------------
Layer dropout_20_216, operation 217/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-1.5139,  0.3185,  0.6868,  1.0144,  0.3297, -0.2366,  0.3504,  1.4877],
        [-1.4202,  0.6199,  1.3104,  0.9060,  1.2189,  0.5385,  0.9483,  2.4341],
        [-1.5732, -0.1735,  1.4476,  0.4526, -0.2312, -1.1022,  0.4139,  1.7184],
        [-1.6503, -0.3014,  1.6804,  0.6234, -0.2562, -1.0508,  0.4485,  1.8223],
        [-1.4565,  0.1482,  1.1580,  0.7566,  0.4201, -0.6067,  0.3342,  1.7619],
        [-0.6977,  0.1493,  1.7259,  0.8906,  0.6999,  1.8929,  1.5605,  2.3579],
        [-0.7214,  1.3689,  1.3530,  1.0794,  1.0662,  1.8048,  1.5006,  1.0747],
        [-1.5670,  0.4060,  1.5135,  0.9469,  1.5299, -0.2881,  0.5672,  1.6549]])...
	Related Layers:
		- parent layers: linear_54_215
		- child layers: add_3_211:2
		- shares parents with no other layers
		- shares children with layers: add_3_211:1
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: ViewBackward0) 
	Computed inside module: blocks.1.ffwd.net.3:1
	Time elapsed:  6.580E-05s
	Output of modules: blocks.1.ffwd.net.3, blocks.1.ffwd.net, blocks.1.ffwd
	Output of bottom-level module: blocks.1.ffwd.net.3:1
	Lookup keys: -465, 233, blocks.1.ffwd, blocks.1.ffwd.net, blocks.1.ffwd.net.3, blocks.1.ffwd.net.3:1, blocks.1.ffwd.net:1, blocks.1.ffwd:1, dropout_20, dropout_20:1, dropout_20_216, dropout_20_216:1
--------------------------------------------
Layer add_3_211 (pass 2/2), operation 218/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-5.4232, -3.9054,  2.5248, -4.4719, -0.2572,  6.6827, -0.2129,  6.3921],
        [-2.2740, -6.0487,  4.8062, -2.9580, -0.5801,  4.6530, -0.6854,  4.4663],
        [-3.9951, -7.0000,  4.1481, -4.1277, -3.2835,  1.6012, -0.8855,  2.6265],
        [-3.6916, -6.9939,  6.4878, -5.6163, -1.9146,  1.4612, -0.2937,  4.5586],
        [-2.0848, -5.6607,  4.6074, -5.7204,  2.8897,  1.0200,  0.0460,  5.8645],
        [ 0.5238, -1.8421,  0.8677, -2.6872,  0.4152,  6.5685, -1.7743,  5.9547],
        [-2.7954, -1.5002,  3.8112, -3.4865,  2.9263, 10.8523, -0.5470,  1.4177],
        [-6.2604, -3.0225,  5.0230, -5.1973,  1.3205,  1.7296, -5.2094,  3.2182]])...
	Related Layers:
		- parent layers: add_3_211:1, dropout_20_216
		- child layers: layernorm_5_217, add_4_317:1
		- shares parents with layers: layernorm_4_212
		- shares children with layers: dropout_29_316
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __add__ (grad_fn: AddBackward0) 
	Computed inside module: blocks.1:1
	Time elapsed:  7.463E-05s
	Output of modules: blocks.1
	Lookup keys: -464, 234, add_3:2, add_3_211:2, blocks.1, blocks.1:1
--------------------------------------------
Layer layernorm_5_217, operation 219/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-1.4786, -1.0963,  0.6016, -1.2492, -0.1324,  1.6834, -0.1142,  1.6174],
        [-0.6328, -1.6866,  1.3025, -0.8344, -0.1759,  1.2579, -0.1978,  1.2134],
        [-1.1790, -2.0865,  1.2111, -1.2375, -0.9780,  0.4664, -0.2651,  0.7676],
        [-1.0997, -2.0884,  1.8686, -1.6867, -0.5883,  0.4064, -0.1078,  1.3104],
        [-0.6086, -1.7020,  1.3962, -1.7246,  0.8787,  0.3252,  0.0303,  1.7777],
        [ 0.1262, -0.4855,  0.2134, -0.7043,  0.0925,  1.6580, -0.4582,  1.5115],
        [-0.6583, -0.3560,  0.9220, -0.8382,  0.7067,  2.5942, -0.1198,  0.3517],
        [-2.0274, -0.9970,  1.6103, -1.7101,  0.4128,  0.5511, -1.6846,  1.0323]])...
	Related Layers:
		- parent layers: add_3_211:2
		- child layers: linear_55_218, linear_56_219, linear_57_228, linear_58_230, linear_59_231, linear_60_240, linear_61_242, linear_62_243, linear_63_252, linear_64_254, linear_65_255, linear_66_264, linear_67_266, linear_68_267, linear_69_276, linear_70_278, linear_71_279, linear_72_288, linear_73_290, linear_74_291, linear_75_300, linear_76_302, linear_77_303, linear_78_312
		- shares parents with layers: add_4_317:1
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (384,), (384,); 768 params total (3.2 KB)
	Function: layer_norm (grad_fn: NativeLayerNormBackward0) 
	Computed inside module: blocks.2.ln1:1
	Time elapsed:  2.356E-04s
	Output of modules: blocks.2.ln1
	Output of bottom-level module: blocks.2.ln1:1
	Lookup keys: -463, 235, blocks.2.ln1, blocks.2.ln1:1, layernorm_5, layernorm_5:1, layernorm_5_217, layernorm_5_217:1
--------------------------------------------
Layer linear_55_218, operation 220/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-3.5462,  2.4972,  0.7737, -0.9526, -2.0230,  3.2379, -0.3951,  3.2972],
        [-3.6071,  2.6164,  0.3158, -0.2874, -2.1446,  3.1019, -0.4683,  3.3008],
        [-3.9336,  2.7282,  0.9220, -1.1659, -2.3113,  3.3455,  0.0173,  3.2189],
        [-3.6585,  2.9025,  0.7209, -0.9542, -2.4338,  3.5418, -0.0493,  3.4755],
        [-2.3821,  1.4347,  0.5023, -1.9779, -2.0739,  1.6553,  0.0543,  2.0799],
        [-2.8922,  2.9917,  0.2540, -1.6644, -1.3501,  4.0929,  0.5367,  2.9768],
        [-2.8658,  4.3668,  0.2473, -1.4723, -3.1613,  4.2468,  0.0278,  3.3039],
        [-3.1866,  3.1788,  0.7014, -0.5321, -2.8198,  2.7151, -0.6882,  3.0048]])...
	Related Layers:
		- parent layers: layernorm_5_217
		- child layers: transpose_17_220
		- shares parents with layers: linear_56_219, linear_57_228, linear_58_230, linear_59_231, linear_60_240, linear_61_242, linear_62_243, linear_63_252, linear_64_254, linear_65_255, linear_66_264, linear_67_266, linear_68_267, linear_69_276, linear_70_278, linear_71_279, linear_72_288, linear_73_290, linear_74_291, linear_75_300, linear_76_302, linear_77_303, linear_78_312
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.0.key:1
	Time elapsed:  1.178E-04s
	Output of modules: blocks.2.sa.heads.0.key
	Output of bottom-level module: blocks.2.sa.heads.0.key:1
	Lookup keys: -462, 236, blocks.2.sa.heads.0.key, blocks.2.sa.heads.0.key:1, linear_55, linear_55:1, linear_55_218, linear_55_218:1
--------------------------------------------
Layer linear_56_219, operation 221/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-1.1959, -0.8754,  1.1214,  0.3377,  2.8753, -0.7801, -2.2673, -1.5755],
        [-0.9394, -0.1231,  0.5595, -0.7485,  1.4353,  0.2752, -1.3967, -0.5326],
        [-1.0304,  0.3008,  0.6079,  0.5129,  2.5458,  0.0718, -1.5905, -0.8623],
        [-0.9472, -0.2056,  0.2941, -0.1458,  2.3039,  0.1102, -0.9902, -1.2107],
        [-1.3741, -0.4619,  0.2032,  1.0480,  2.9122, -0.0332, -1.3130, -1.3856],
        [-0.4652,  0.8544,  0.4863,  0.2619,  1.8274, -0.5228, -1.0833,  0.6479],
        [-0.0149,  0.7921,  0.7061,  0.1308,  1.3750, -0.5771, -1.8998,  0.2165],
        [-1.1955,  1.2139, -0.5556, -0.0364,  1.4870, -0.8080, -1.7706, -0.2540]])...
	Related Layers:
		- parent layers: layernorm_5_217
		- child layers: matmul_33_221
		- shares parents with layers: linear_55_218, linear_57_228, linear_58_230, linear_59_231, linear_60_240, linear_61_242, linear_62_243, linear_63_252, linear_64_254, linear_65_255, linear_66_264, linear_67_266, linear_68_267, linear_69_276, linear_70_278, linear_71_279, linear_72_288, linear_73_290, linear_74_291, linear_75_300, linear_76_302, linear_77_303, linear_78_312
		- shares children with layers: transpose_17_220
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.0.query:1
	Time elapsed:  1.113E-04s
	Output of modules: blocks.2.sa.heads.0.query
	Output of bottom-level module: blocks.2.sa.heads.0.query:1
	Lookup keys: -461, 237, blocks.2.sa.heads.0.query, blocks.2.sa.heads.0.query:1, linear_56, linear_56:1, linear_56_219, linear_56_219:1
--------------------------------------------
Layer transpose_17_220, operation 222/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-3.5462, -3.6071, -3.9336, -3.6585, -2.3821, -2.8922, -2.8658, -3.1866],
        [ 2.4972,  2.6164,  2.7282,  2.9025,  1.4347,  2.9917,  4.3668,  3.1788],
        [ 0.7737,  0.3158,  0.9220,  0.7209,  0.5023,  0.2540,  0.2473,  0.7014],
        [-0.9526, -0.2874, -1.1659, -0.9542, -1.9779, -1.6644, -1.4723, -0.5321],
        [-2.0230, -2.1446, -2.3113, -2.4338, -2.0739, -1.3501, -3.1613, -2.8198],
        [ 3.2379,  3.1019,  3.3455,  3.5418,  1.6553,  4.0929,  4.2468,  2.7151],
        [-0.3951, -0.4683,  0.0173, -0.0493,  0.0543,  0.5367,  0.0278, -0.6882],
        [ 3.2972,  3.3008,  3.2189,  3.4755,  2.0799,  2.9768,  3.3039,  3.0048]])...
	Related Layers:
		- parent layers: linear_55_218
		- child layers: matmul_33_221
		- shares parents with no other layers
		- shares children with layers: linear_56_219
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.2.sa.heads.0:1
	Time elapsed:  5.770E-05s
	Output of modules: none
	Lookup keys: -460, 238, transpose_17, transpose_17:1, transpose_17_220, transpose_17_220:1
--------------------------------------------
Layer matmul_33_221, operation 223/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[  6.4033,   7.9260,  -6.5053, -11.3600,  -4.7064,  25.0711,   0.2143,
         -11.2065],
        [ 83.5355,  98.0217,  75.8319,  75.9407,  52.8036, 131.1452, 107.7481,
          67.0507],
        [ 28.1521,  32.6974,  16.4948,  13.9883,   8.1744,  55.2979,  33.2458,
          18.6747],
        [ 12.9183,  13.2842,   2.7508,  -1.0321,   1.3258,  34.8732,  12.4303,
           2.3822],
        [-29.7525, -31.4686, -46.4302, -51.2180, -37.2569,  -8.2325, -39.3069,
         -48.7481],
        [ 84.3147, 107.5602,  77.7528,  77.7589,  51.3494, 126.6898, 112.8425,
          71.5556],
        [ 48.1257,  64.1824,  39.5327,  38.0627,  31.3902,  79.6276,  65.8204,
          38.9589],
        [ 17.6339,  18.2690,   5.0670,   2.2806,   6.3347,  26.7668,  14.1118,
           1.6487]])...
	Related Layers:
		- parent layers: linear_56_219, transpose_17_220
		- child layers: mul_17_222
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.0:1
	Time elapsed:  8.821E-05s
	Output of modules: none
	Lookup keys: -459, 239, matmul_33, matmul_33:1, matmul_33_221, matmul_33_221:1
--------------------------------------------
Layer mul_17_222, operation 224/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ 0.3268,  0.4045, -0.3320, -0.5797, -0.2402,  1.2794,  0.0109, -0.5719],
        [ 4.2629,  5.0021,  3.8698,  3.8753,  2.6946,  6.6925,  5.4985,  3.4217],
        [ 1.4366,  1.6686,  0.8417,  0.7138,  0.4171,  2.8219,  1.6966,  0.9530],
        [ 0.6592,  0.6779,  0.1404, -0.0527,  0.0677,  1.7796,  0.6343,  0.1216],
        [-1.5183, -1.6059, -2.3694, -2.6137, -1.9013, -0.4201, -2.0059, -2.4877],
        [ 4.3027,  5.4889,  3.9678,  3.9681,  2.6204,  6.4651,  5.7585,  3.6516],
        [ 2.4559,  3.2753,  2.0174,  1.9424,  1.6019,  4.0635,  3.3589,  1.9881],
        [ 0.8999,  0.9323,  0.2586,  0.1164,  0.3233,  1.3659,  0.7201,  0.0841]])...
	Related Layers:
		- parent layers: matmul_33_221
		- child layers: maskedfill_17_225
		- shares parents with no other layers
		- shares children with layers: eq_17_224
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.2.sa.heads.0:1
	Time elapsed:  6.914E-05s
	Output of modules: none
	Lookup keys: -458, 240, mul_17, mul_17:1, mul_17_222, mul_17_222:1
--------------------------------------------
Layer getitem_17_223, operation 225/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_17
		- child layers: eq_17_224
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.322E-05s
	Output of modules: none
	Lookup keys: -456, 242, getitem_17, getitem_17:1, getitem_17_223, getitem_17_223:1
--------------------------------------------
Layer eq_17_224, operation 226/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_17_223
		- child layers: maskedfill_17_225
		- shares parents with no other layers
		- shares children with layers: mul_17_222
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  7.915E-05s
	Output of modules: none
	Lookup keys: -455, 243, eq_17, eq_17:1, eq_17_224, eq_17_224:1
--------------------------------------------
Layer maskedfill_17_225, operation 227/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ 0.3268,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [ 4.2629,  5.0021,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [ 1.4366,  1.6686,  0.8417,    -inf,    -inf,    -inf,    -inf,    -inf],
        [ 0.6592,  0.6779,  0.1404, -0.0527,    -inf,    -inf,    -inf,    -inf],
        [-1.5183, -1.6059, -2.3694, -2.6137, -1.9013,    -inf,    -inf,    -inf],
        [ 4.3027,  5.4889,  3.9678,  3.9681,  2.6204,  6.4651,    -inf,    -inf],
        [ 2.4559,  3.2753,  2.0174,  1.9424,  1.6019,  4.0635,  3.3589,    -inf],
        [ 0.8999,  0.9323,  0.2586,  0.1164,  0.3233,  1.3659,  0.7201,  0.0841]])...
	Related Layers:
		- parent layers: mul_17_222, eq_17_224
		- child layers: softmax_17_226
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.2.sa.heads.0:1
	Time elapsed:  8.297E-05s
	Output of modules: none
	Lookup keys: -454, 244, maskedfill_17, maskedfill_17:1, maskedfill_17_225, maskedfill_17_225:1
--------------------------------------------
Layer softmax_17_226, operation 228/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3232, 0.6768, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3555, 0.4483, 0.1961, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3221, 0.3282, 0.1917, 0.1581, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2977, 0.2727, 0.1271, 0.0995, 0.2030, 0.0000, 0.0000, 0.0000],
        [0.0686, 0.2245, 0.0491, 0.0491, 0.0128, 0.5960, 0.0000, 0.0000],
        [0.0807, 0.1831, 0.0520, 0.0483, 0.0343, 0.4026, 0.1990, 0.0000],
        [0.1550, 0.1602, 0.0816, 0.0708, 0.0871, 0.2471, 0.1295, 0.0686]])...
	Related Layers:
		- parent layers: maskedfill_17_225
		- child layers: dropout_21_227
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.2.sa.heads.0:1
	Time elapsed:  7.176E-05s
	Output of modules: none
	Lookup keys: -453, 245, softmax_17, softmax_17:1, softmax_17_226, softmax_17_226:1
--------------------------------------------
Layer dropout_21_227, operation 229/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3232, 0.6768, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3555, 0.4483, 0.1961, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3221, 0.3282, 0.1917, 0.1581, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2977, 0.2727, 0.1271, 0.0995, 0.2030, 0.0000, 0.0000, 0.0000],
        [0.0686, 0.2245, 0.0491, 0.0491, 0.0128, 0.5960, 0.0000, 0.0000],
        [0.0807, 0.1831, 0.0520, 0.0483, 0.0343, 0.4026, 0.1990, 0.0000],
        [0.1550, 0.1602, 0.0816, 0.0708, 0.0871, 0.2471, 0.1295, 0.0686]])...
	Related Layers:
		- parent layers: softmax_17_226
		- child layers: matmul_34_229
		- shares parents with no other layers
		- shares children with layers: linear_57_228
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.2.sa.heads.0.dropout:1
	Time elapsed:  5.388E-05s
	Output of modules: blocks.2.sa.heads.0.dropout
	Output of bottom-level module: blocks.2.sa.heads.0.dropout:1
	Lookup keys: -452, 246, blocks.2.sa.heads.0.dropout, blocks.2.sa.heads.0.dropout:1, dropout_21, dropout_21:1, dropout_21_227, dropout_21_227:1
--------------------------------------------
Layer linear_57_228, operation 230/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.6453, -0.5102, -0.7570, -0.2688, -0.4013, -0.0191,  1.2619, -1.0336],
        [-0.7398,  0.0492,  0.4274,  1.0154, -0.6929, -0.3791,  1.2467,  0.6573],
        [-0.7768, -0.2937,  0.1838, -0.3253, -0.0514,  0.4723,  1.1917, -1.4863],
        [-0.6631, -0.3843,  0.0359, -0.0587, -0.0797,  0.1732,  1.1342, -1.1204],
        [-0.4211, -0.4236,  0.5474, -0.1966,  0.0884,  0.4985,  0.9813, -1.0615],
        [-2.7902,  1.3577,  1.6334, -0.3271, -0.1681, -2.8933,  1.4292,  1.3142],
        [-2.2441,  0.7566,  1.2982, -0.4968,  0.3666, -1.9771,  0.6933,  0.8661],
        [-1.0369,  0.4136,  0.6915, -0.4144,  0.3919,  0.1768,  0.9874,  0.0857]])...
	Related Layers:
		- parent layers: layernorm_5_217
		- child layers: matmul_34_229
		- shares parents with layers: linear_55_218, linear_56_219, linear_58_230, linear_59_231, linear_60_240, linear_61_242, linear_62_243, linear_63_252, linear_64_254, linear_65_255, linear_66_264, linear_67_266, linear_68_267, linear_69_276, linear_70_278, linear_71_279, linear_72_288, linear_73_290, linear_74_291, linear_75_300, linear_76_302, linear_77_303, linear_78_312
		- shares children with layers: dropout_21_227
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.0.value:1
	Time elapsed:  1.230E-04s
	Output of modules: blocks.2.sa.heads.0.value
	Output of bottom-level module: blocks.2.sa.heads.0.value:1
	Lookup keys: -451, 247, blocks.2.sa.heads.0.value, blocks.2.sa.heads.0.value:1, linear_57, linear_57:1, linear_57_228, linear_57_228:1
--------------------------------------------
Layer matmul_34_229, operation 231/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.6453, -0.5102, -0.7570, -0.2688, -0.4013, -0.0191,  1.2619, -1.0336],
        [-0.7092, -0.1316,  0.0447,  0.6004, -0.5986, -0.2627,  1.2516,  0.1108],
        [-0.7134, -0.2169, -0.0414,  0.2959, -0.4634, -0.0841,  1.2413, -0.3643],
        [-0.7043, -0.2652, -0.0626,  0.1750, -0.3791, -0.0126,  1.2233, -0.5792],
        [-0.6440, -0.3000,  0.0293,  0.1098, -0.3049,  0.0694,  1.1792, -0.6443],
        [-1.9494,  0.7466,  1.0353, -0.0067, -0.2886, -1.7729,  1.3449,  0.7185],
        [-1.8444,  0.6167,  0.9632, -0.0929, -0.1574, -1.5792,  1.1939,  0.5705],
        [-1.4169,  0.3025,  0.6355, -0.1005, -0.1425, -0.9283,  1.1691,  0.0946]])...
	Related Layers:
		- parent layers: dropout_21_227, linear_57_228
		- child layers: cat_3_314
		- shares parents with no other layers
		- shares children with layers: matmul_36_241, matmul_38_253, matmul_40_265, matmul_42_277, matmul_44_289, matmul_46_301, matmul_48_313
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.0:1
	Time elapsed:  8.368E-05s
	Output of modules: blocks.2.sa.heads.0
	Lookup keys: -450, 248, blocks.2.sa.heads.0, blocks.2.sa.heads.0:1, matmul_34, matmul_34:1, matmul_34_229, matmul_34_229:1
--------------------------------------------
Layer linear_58_230, operation 232/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-3.9864, -2.5590,  3.9322,  1.9378, -2.9869, -0.9230,  4.3823,  4.3536],
        [-4.1533, -2.6884,  3.0542,  2.1122, -3.8070, -2.0914,  4.5690,  4.0818],
        [-3.9777, -2.6159,  3.4288,  1.6337, -3.4760, -1.8723,  3.8212,  3.8853],
        [-3.5442, -1.7108,  3.0176,  1.3444, -3.7707, -1.6469,  4.0242,  3.3119],
        [-3.0712, -2.4190,  2.1648,  1.9953, -3.9461, -1.7991,  2.9486,  2.7455],
        [-5.1853, -3.1462,  5.4548,  1.8351,  0.5293, -1.2101,  5.1681,  4.9591],
        [-4.4782, -3.1167,  4.9213,  2.6011, -0.6500, -1.6072,  4.6677,  4.1878],
        [-4.2698, -3.0384,  4.0784,  2.9208, -3.6591, -1.1578,  4.5668,  4.2615]])...
	Related Layers:
		- parent layers: layernorm_5_217
		- child layers: transpose_18_232
		- shares parents with layers: linear_55_218, linear_56_219, linear_57_228, linear_59_231, linear_60_240, linear_61_242, linear_62_243, linear_63_252, linear_64_254, linear_65_255, linear_66_264, linear_67_266, linear_68_267, linear_69_276, linear_70_278, linear_71_279, linear_72_288, linear_73_290, linear_74_291, linear_75_300, linear_76_302, linear_77_303, linear_78_312
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.1.key:1
	Time elapsed:  1.135E-04s
	Output of modules: blocks.2.sa.heads.1.key
	Output of bottom-level module: blocks.2.sa.heads.1.key:1
	Lookup keys: -449, 249, blocks.2.sa.heads.1.key, blocks.2.sa.heads.1.key:1, linear_58, linear_58:1, linear_58_230, linear_58_230:1
--------------------------------------------
Layer linear_59_231, operation 233/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.7915, -0.5481, -0.5014,  0.8617, -1.2112,  0.0438, -0.8520, -0.0331],
        [ 2.0042, -0.5360, -2.1964,  0.6408, -1.4885, -0.6651, -2.4912, -2.5329],
        [ 0.2449, -0.3567, -0.7547,  1.1871, -0.6626,  0.2800, -0.5835,  0.2422],
        [ 0.8857, -0.4420, -0.6085,  0.8144, -0.4186,  0.4908, -0.5509, -0.0366],
        [ 0.0747, -0.5208, -0.1600,  1.2650, -0.5547,  1.0738, -1.2369,  0.3789],
        [ 2.8375,  0.5387, -3.0831, -3.1067, -0.7735, -1.7586, -1.8013, -3.8938],
        [ 3.9256,  0.4210, -2.8913, -1.9781, -1.4212, -2.7865, -2.0499, -4.0822],
        [ 2.9154,  0.6487, -2.7816,  0.0875, -1.7045, -0.0703, -2.1985, -1.2130]])...
	Related Layers:
		- parent layers: layernorm_5_217
		- child layers: matmul_35_233
		- shares parents with layers: linear_55_218, linear_56_219, linear_57_228, linear_58_230, linear_60_240, linear_61_242, linear_62_243, linear_63_252, linear_64_254, linear_65_255, linear_66_264, linear_67_266, linear_68_267, linear_69_276, linear_70_278, linear_71_279, linear_72_288, linear_73_290, linear_74_291, linear_75_300, linear_76_302, linear_77_303, linear_78_312
		- shares children with layers: transpose_18_232
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.1.query:1
	Time elapsed:  1.142E-04s
	Output of modules: blocks.2.sa.heads.1.query
	Output of bottom-level module: blocks.2.sa.heads.1.query:1
	Lookup keys: -448, 250, blocks.2.sa.heads.1.query, blocks.2.sa.heads.1.query:1, linear_59, linear_59:1, linear_59_231, linear_59_231:1
--------------------------------------------
Layer transpose_18_232, operation 234/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-3.9864, -4.1533, -3.9777, -3.5442, -3.0712, -5.1853, -4.4782, -4.2698],
        [-2.5590, -2.6884, -2.6159, -1.7108, -2.4190, -3.1462, -3.1167, -3.0384],
        [ 3.9322,  3.0542,  3.4288,  3.0176,  2.1648,  5.4548,  4.9213,  4.0784],
        [ 1.9378,  2.1122,  1.6337,  1.3444,  1.9953,  1.8351,  2.6011,  2.9208],
        [-2.9869, -3.8070, -3.4760, -3.7707, -3.9461,  0.5293, -0.6500, -3.6591],
        [-0.9230, -2.0914, -1.8723, -1.6469, -1.7991, -1.2101, -1.6072, -1.1578],
        [ 4.3823,  4.5690,  3.8212,  4.0242,  2.9486,  5.1681,  4.6677,  4.5668],
        [ 4.3536,  4.0818,  3.8853,  3.3119,  2.7455,  4.9591,  4.1878,  4.2615]])...
	Related Layers:
		- parent layers: linear_58_230
		- child layers: matmul_35_233
		- shares parents with no other layers
		- shares children with layers: linear_59_231
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.2.sa.heads.1:1
	Time elapsed:  6.056E-05s
	Output of modules: none
	Lookup keys: -447, 251, transpose_18, transpose_18:1, transpose_18_232, transpose_18_232:1
--------------------------------------------
Layer matmul_35_233, operation 235/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[  -8.2910,  -10.0545,   -7.2570,   -7.6099,   -4.4348,  -22.7914,
          -27.1856,   -3.4677],
        [-146.5927, -140.9669, -123.3566, -112.0942,  -77.2382, -209.5846,
         -190.7338, -132.3364],
        [ -24.3079,  -14.9067,  -21.6763,  -20.5550,  -18.7021,  -38.0404,
          -26.2285,  -13.6700],
        [ -36.3504,  -28.0741,  -32.4824,  -30.6604,  -27.9847,  -47.2966,
          -36.4656,  -25.7182],
        [ -12.7096,  -25.1490,  -17.9003,  -21.3989,  -12.4494,  -21.2724,
          -38.2721,  -14.0974],
        [-227.0454, -194.4620, -190.1439, -172.5061, -143.2233, -294.5998,
         -244.8342, -210.2977],
        [-262.8896, -235.3048, -217.2632, -195.5708, -149.4747, -353.7096,
         -296.5337, -237.1035],
        [-130.9422, -136.2920, -113.9482, -103.9860,  -68.4362, -222.5866,
         -209.8328, -132.8247]])...
	Related Layers:
		- parent layers: linear_59_231, transpose_18_232
		- child layers: mul_18_234
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.1:1
	Time elapsed:  8.798E-05s
	Output of modules: none
	Lookup keys: -446, 252, matmul_35, matmul_35:1, matmul_35_233, matmul_35_233:1
--------------------------------------------
Layer mul_18_234, operation 236/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -0.4231,  -0.5131,  -0.3703,  -0.3883,  -0.2263,  -1.1631,  -1.3873,
          -0.1770],
        [ -7.4808,  -7.1937,  -6.2950,  -5.7203,  -3.9415, -10.6953,  -9.7333,
          -6.7533],
        [ -1.2405,  -0.7607,  -1.1062,  -1.0489,  -0.9544,  -1.9412,  -1.3385,
          -0.6976],
        [ -1.8550,  -1.4327,  -1.6576,  -1.5646,  -1.4281,  -2.4136,  -1.8609,
          -1.3124],
        [ -0.6486,  -1.2834,  -0.9135,  -1.0920,  -0.6353,  -1.0856,  -1.9531,
          -0.7194],
        [-11.5864,  -9.9236,  -9.7032,  -8.8032,  -7.3088, -15.0337, -12.4941,
         -10.7317],
        [-13.4155, -12.0078, -11.0872,  -9.9802,  -7.6279, -18.0502, -15.1324,
         -12.0996],
        [ -6.6821,  -6.9551,  -5.8149,  -5.3065,  -3.4924, -11.3588, -10.7080,
          -6.7782]])...
	Related Layers:
		- parent layers: matmul_35_233
		- child layers: maskedfill_18_237
		- shares parents with no other layers
		- shares children with layers: eq_18_236
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.2.sa.heads.1:1
	Time elapsed:  6.986E-05s
	Output of modules: none
	Lookup keys: -445, 253, mul_18, mul_18:1, mul_18_234, mul_18_234:1
--------------------------------------------
Layer getitem_18_235, operation 237/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_18
		- child layers: eq_18_236
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.893E-05s
	Output of modules: none
	Lookup keys: -443, 255, getitem_18, getitem_18:1, getitem_18_235, getitem_18_235:1
--------------------------------------------
Layer eq_18_236, operation 238/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_18_235
		- child layers: maskedfill_18_237
		- shares parents with no other layers
		- shares children with layers: mul_18_234
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  7.796E-05s
	Output of modules: none
	Lookup keys: -442, 256, eq_18, eq_18:1, eq_18_236, eq_18_236:1
--------------------------------------------
Layer maskedfill_18_237, operation 239/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -0.4231,     -inf,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -7.4808,  -7.1937,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -1.2405,  -0.7607,  -1.1062,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -1.8550,  -1.4327,  -1.6576,  -1.5646,     -inf,     -inf,     -inf,
             -inf],
        [ -0.6486,  -1.2834,  -0.9135,  -1.0920,  -0.6353,     -inf,     -inf,
             -inf],
        [-11.5864,  -9.9236,  -9.7032,  -8.8032,  -7.3088, -15.0337,     -inf,
             -inf],
        [-13.4155, -12.0078, -11.0872,  -9.9802,  -7.6279, -18.0502, -15.1324,
             -inf],
        [ -6.6821,  -6.9551,  -5.8149,  -5.3065,  -3.4924, -11.3588, -10.7080,
          -6.7782]])...
	Related Layers:
		- parent layers: mul_18_234, eq_18_236
		- child layers: softmax_18_238
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.2.sa.heads.1:1
	Time elapsed:  8.297E-05s
	Output of modules: none
	Lookup keys: -441, 257, maskedfill_18, maskedfill_18:1, maskedfill_18_237, maskedfill_18_237:1
--------------------------------------------
Layer softmax_18_238, operation 240/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [4.2872e-01, 5.7128e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.6600e-01, 4.2977e-01, 3.0423e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.9682e-01, 3.0026e-01, 2.3977e-01, 2.6314e-01, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.5300e-01, 1.3410e-01, 1.9413e-01, 1.6239e-01, 2.5638e-01, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [9.8900e-03, 5.2159e-02, 6.5017e-02, 1.5993e-01, 7.1269e-01, 3.1479e-04,
         0.0000e+00, 0.0000e+00],
        [2.6822e-03, 1.0961e-02, 2.7522e-02, 8.3260e-02, 8.7507e-01, 2.6043e-05,
         4.8178e-04, 0.0000e+00],
        [3.0015e-02, 2.2844e-02, 7.1444e-02, 1.1878e-01, 7.2883e-01, 2.7943e-04,
         5.3570e-04, 2.7266e-02]])...
	Related Layers:
		- parent layers: maskedfill_18_237
		- child layers: dropout_22_239
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.2.sa.heads.1:1
	Time elapsed:  6.890E-05s
	Output of modules: none
	Lookup keys: -440, 258, softmax_18, softmax_18:1, softmax_18_238, softmax_18_238:1
--------------------------------------------
Layer dropout_22_239, operation 241/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [4.2872e-01, 5.7128e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.6600e-01, 4.2977e-01, 3.0423e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.9682e-01, 3.0026e-01, 2.3977e-01, 2.6314e-01, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.5300e-01, 1.3410e-01, 1.9413e-01, 1.6239e-01, 2.5638e-01, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [9.8900e-03, 5.2159e-02, 6.5017e-02, 1.5993e-01, 7.1269e-01, 3.1479e-04,
         0.0000e+00, 0.0000e+00],
        [2.6822e-03, 1.0961e-02, 2.7522e-02, 8.3260e-02, 8.7507e-01, 2.6043e-05,
         4.8178e-04, 0.0000e+00],
        [3.0015e-02, 2.2844e-02, 7.1444e-02, 1.1878e-01, 7.2883e-01, 2.7943e-04,
         5.3570e-04, 2.7266e-02]])...
	Related Layers:
		- parent layers: softmax_18_238
		- child layers: matmul_36_241
		- shares parents with no other layers
		- shares children with layers: linear_60_240
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.2.sa.heads.1.dropout:1
	Time elapsed:  5.388E-05s
	Output of modules: blocks.2.sa.heads.1.dropout
	Output of bottom-level module: blocks.2.sa.heads.1.dropout:1
	Lookup keys: -439, 259, blocks.2.sa.heads.1.dropout, blocks.2.sa.heads.1.dropout:1, dropout_22, dropout_22:1, dropout_22_239, dropout_22_239:1
--------------------------------------------
Layer linear_60_240, operation 242/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.4132,  0.3855,  0.5652,  1.0806,  0.7287, -0.1169, -0.2561,  0.3940],
        [-0.2202,  0.6944,  1.5259, -0.0552,  0.8288,  0.5066,  0.7312, -0.0357],
        [-0.5416,  0.6100,  0.9667,  0.8700,  0.7373, -0.0948,  0.5116,  0.4623],
        [-0.2087,  0.6750,  1.0938,  1.1030,  0.4170,  0.0073, -0.0567,  0.6511],
        [ 0.0183,  0.8578, -0.0763,  0.6870, -0.0765, -0.4096, -0.4354, -0.2376],
        [-0.4048,  0.3287,  1.3109,  0.1625,  1.0189,  1.1679,  1.1730, -0.1049],
        [-0.4018, -0.4538,  1.0473, -0.4110,  0.5966,  1.4523,  0.8465,  0.4680],
        [-0.5649,  0.0094,  1.3395,  0.1958,  0.3089,  0.3995,  0.0024,  0.7291]])...
	Related Layers:
		- parent layers: layernorm_5_217
		- child layers: matmul_36_241
		- shares parents with layers: linear_55_218, linear_56_219, linear_57_228, linear_58_230, linear_59_231, linear_61_242, linear_62_243, linear_63_252, linear_64_254, linear_65_255, linear_66_264, linear_67_266, linear_68_267, linear_69_276, linear_70_278, linear_71_279, linear_72_288, linear_73_290, linear_74_291, linear_75_300, linear_76_302, linear_77_303, linear_78_312
		- shares children with layers: dropout_22_239
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.1.value:1
	Time elapsed:  1.192E-04s
	Output of modules: blocks.2.sa.heads.1.value
	Output of bottom-level module: blocks.2.sa.heads.1.value:1
	Lookup keys: -438, 260, blocks.2.sa.heads.1.value, blocks.2.sa.heads.1.value:1, linear_60, linear_60:1, linear_60_240, linear_60_240:1
--------------------------------------------
Layer matmul_36_241, operation 243/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 4.1318e-01,  3.8550e-01,  5.6523e-01,  1.0806e+00,  7.2868e-01,
         -1.1687e-01, -2.5606e-01,  3.9397e-01],
        [ 5.1361e-02,  5.6196e-01,  1.1141e+00,  4.3171e-01,  7.8590e-01,
          2.3933e-01,  3.0794e-01,  1.4848e-01],
        [-1.4947e-01,  5.8653e-01,  1.1002e+00,  5.2838e-01,  7.7435e-01,
          1.5780e-01,  4.0178e-01,  2.3009e-01],
        [-1.6954e-01,  6.0823e-01,  1.0890e+00,  6.9495e-01,  6.7880e-01,
          1.0831e-01,  2.7691e-01,  3.4899e-01],
        [-5.9313e-02,  6.3860e-01,  6.9335e-01,  7.9014e-01,  4.8673e-01,
         -8.3862e-02,  1.1767e-02,  2.2944e-01],
        [-6.3065e-02,  7.9911e-01,  2.6901e-01,  7.3047e-01,  1.1084e-01,
         -2.7128e-01, -2.5010e-01, -3.3159e-02],
        [-1.7776e-02,  8.3209e-01,  6.9706e-02,  7.1909e-01, -6.0351e-04,
         -3.5446e-01, -3.6384e-01, -1.4010e-01],
        [-5.8499e-02,  7.7651e-01,  2.3267e-01,  7.3025e-01,  9.6261e-02,
         -2.8437e-01, -2.7762e-01, -3.1699e-02]])...
	Related Layers:
		- parent layers: dropout_22_239, linear_60_240
		- child layers: cat_3_314
		- shares parents with no other layers
		- shares children with layers: matmul_34_229, matmul_38_253, matmul_40_265, matmul_42_277, matmul_44_289, matmul_46_301, matmul_48_313
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.1:1
	Time elapsed:  8.297E-05s
	Output of modules: blocks.2.sa.heads.1
	Lookup keys: -437, 261, blocks.2.sa.heads.1, blocks.2.sa.heads.1:1, matmul_36, matmul_36:1, matmul_36_241, matmul_36_241:1
--------------------------------------------
Layer linear_61_242, operation 244/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 2.6144,  4.7885, -2.2727, -3.2615,  4.2193,  2.8292, -0.5996, -5.5019],
        [ 2.1686,  4.1528, -1.4444, -3.3750,  3.8235,  2.1082,  0.8911, -4.4650],
        [ 1.7868,  4.2641, -0.6507, -2.9109,  3.2866,  3.7462,  2.0299, -3.9269],
        [ 1.4678,  3.7392, -0.7987, -3.0625,  2.3710,  3.3757,  2.3479, -4.2046],
        [ 2.0861,  3.8850, -1.7850, -2.6191,  3.6294,  2.2781, -0.1839, -4.4939],
        [ 0.9675,  1.8679, -0.5388, -2.5823,  0.9578,  0.5780,  1.5384, -2.1777],
        [ 1.4893,  2.5468, -1.3546, -3.0345,  2.3939,  0.7614,  0.4233, -2.9017],
        [ 0.8572,  2.9228, -2.7772, -2.3209,  4.1649,  2.0996,  0.3936, -4.0692]])...
	Related Layers:
		- parent layers: layernorm_5_217
		- child layers: transpose_19_244
		- shares parents with layers: linear_55_218, linear_56_219, linear_57_228, linear_58_230, linear_59_231, linear_60_240, linear_62_243, linear_63_252, linear_64_254, linear_65_255, linear_66_264, linear_67_266, linear_68_267, linear_69_276, linear_70_278, linear_71_279, linear_72_288, linear_73_290, linear_74_291, linear_75_300, linear_76_302, linear_77_303, linear_78_312
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.2.key:1
	Time elapsed:  1.128E-04s
	Output of modules: blocks.2.sa.heads.2.key
	Output of bottom-level module: blocks.2.sa.heads.2.key:1
	Lookup keys: -436, 262, blocks.2.sa.heads.2.key, blocks.2.sa.heads.2.key:1, linear_61, linear_61:1, linear_61_242, linear_61_242:1
--------------------------------------------
Layer linear_62_243, operation 245/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.3457,  0.6280,  0.2625,  1.4438, -3.1543,  0.0415,  0.6508,  4.4911],
        [-0.9745,  1.7483,  0.6691,  0.2897, -4.2557, -0.5594,  1.8923,  4.7765],
        [-0.5390,  1.0748,  0.5102,  1.0508, -4.1407, -0.6125,  0.6350,  5.6664],
        [-0.7218,  0.8540,  0.7014,  1.0254, -4.0394, -0.5711,  0.8390,  5.7016],
        [-0.4724,  0.8947,  0.9710,  0.7845, -3.6348,  0.7417,  1.8848,  5.0658],
        [-1.3846, -0.9443,  0.3282,  0.8530, -2.2026, -0.2131,  1.9335,  3.4625],
        [-2.0441,  0.3594,  0.3524,  1.0870, -5.0968, -1.9975,  2.4012,  4.7958],
        [-0.5836,  0.6217,  0.8789,  1.4034, -5.0449, -0.8875,  0.8020,  6.3080]])...
	Related Layers:
		- parent layers: layernorm_5_217
		- child layers: matmul_37_245
		- shares parents with layers: linear_55_218, linear_56_219, linear_57_228, linear_58_230, linear_59_231, linear_60_240, linear_61_242, linear_63_252, linear_64_254, linear_65_255, linear_66_264, linear_67_266, linear_68_267, linear_69_276, linear_70_278, linear_71_279, linear_72_288, linear_73_290, linear_74_291, linear_75_300, linear_76_302, linear_77_303, linear_78_312
		- shares children with layers: transpose_19_244
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.2.query:1
	Time elapsed:  1.113E-04s
	Output of modules: blocks.2.sa.heads.2.query
	Output of bottom-level module: blocks.2.sa.heads.2.query:1
	Lookup keys: -435, 263, blocks.2.sa.heads.2.query, blocks.2.sa.heads.2.query:1, linear_62, linear_62:1, linear_62_243, linear_62_243:1
--------------------------------------------
Layer transpose_19_244, operation 246/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 2.6144,  2.1686,  1.7868,  1.4678,  2.0861,  0.9675,  1.4893,  0.8572],
        [ 4.7885,  4.1528,  4.2641,  3.7392,  3.8850,  1.8679,  2.5468,  2.9228],
        [-2.2727, -1.4444, -0.6507, -0.7987, -1.7850, -0.5388, -1.3546, -2.7772],
        [-3.2615, -3.3750, -2.9109, -3.0625, -2.6191, -2.5823, -3.0345, -2.3209],
        [ 4.2193,  3.8235,  3.2866,  2.3710,  3.6294,  0.9578,  2.3939,  4.1649],
        [ 2.8292,  2.1082,  3.7462,  3.3757,  2.2781,  0.5780,  0.7614,  2.0996],
        [-0.5996,  0.8911,  2.0299,  2.3479, -0.1839,  1.5384,  0.4233,  0.3936],
        [-5.5019, -4.4650, -3.9269, -4.2046, -4.4939, -2.1777, -2.9017, -4.0692]])...
	Related Layers:
		- parent layers: linear_61_242
		- child layers: matmul_37_245
		- shares parents with no other layers
		- shares children with layers: linear_62_243
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.2.sa.heads.2:1
	Time elapsed:  5.913E-05s
	Output of modules: none
	Lookup keys: -434, 264, transpose_19, transpose_19:1, transpose_19_244, transpose_19_244:1
--------------------------------------------
Layer matmul_37_245, operation 247/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-125.8510,  -97.4085,  -81.3434,  -72.9543,  -95.7662,  -47.9428,
          -92.2107,  -94.0138],
        [ -98.7000,  -66.0707,  -46.7086,  -36.4226,  -63.2637,  -24.5594,
          -75.5003,  -71.2963],
        [-200.9911, -162.4483, -141.1999, -126.2579, -167.5099,  -65.2076,
         -124.4446, -158.0238],
        [-214.2290, -171.2772, -147.4586, -131.9037, -175.7052,  -69.2667,
         -133.6672, -165.7204],
        [-133.2860, -103.4759,  -79.1630,  -68.8606, -103.6401,  -45.3083,
          -92.2958, -104.2307],
        [-195.0192, -145.0331, -120.8089, -110.6037, -142.4972,  -55.2439,
         -123.0113, -123.8669],
        [-243.9524, -190.9297, -167.5913, -151.0570, -190.4515,  -66.3174,
         -144.1840, -173.6665],
        [-198.6390, -155.6351, -136.9252, -122.0195, -156.6605,  -71.4916,
         -133.2346, -154.4525]])...
	Related Layers:
		- parent layers: linear_62_243, transpose_19_244
		- child layers: mul_19_246
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.2:1
	Time elapsed:  8.821E-05s
	Output of modules: none
	Lookup keys: -433, 265, matmul_37, matmul_37:1, matmul_37_245, matmul_37_245:1
--------------------------------------------
Layer mul_19_246, operation 248/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -6.4223,  -4.9709,  -4.1510,  -3.7229,  -4.8871,  -2.4466,  -4.7056,
          -4.7976],
        [ -5.0368,  -3.3717,  -2.3836,  -1.8587,  -3.2284,  -1.2533,  -3.8529,
          -3.6383],
        [-10.2568,  -8.2899,  -7.2056,  -6.4431,  -8.5482,  -3.3276,  -6.3505,
          -8.0641],
        [-10.9323,  -8.7405,  -7.5250,  -6.7312,  -8.9664,  -3.5347,  -6.8212,
          -8.4569],
        [ -6.8017,  -5.2805,  -4.0398,  -3.5140,  -5.2889,  -2.3121,  -4.7100,
          -5.3190],
        [ -9.9520,  -7.4012,  -6.1650,  -5.6442,  -7.2718,  -2.8192,  -6.2774,
          -6.3211],
        [-12.4491,  -9.7433,  -8.5524,  -7.7086,  -9.7189,  -3.3842,  -7.3579,
          -8.8624],
        [-10.1368,  -7.9422,  -6.9874,  -6.2268,  -7.9945,  -3.6483,  -6.7991,
          -7.8819]])...
	Related Layers:
		- parent layers: matmul_37_245
		- child layers: maskedfill_19_249
		- shares parents with no other layers
		- shares children with layers: eq_19_248
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.2.sa.heads.2:1
	Time elapsed:  6.795E-05s
	Output of modules: none
	Lookup keys: -432, 266, mul_19, mul_19:1, mul_19_246, mul_19_246:1
--------------------------------------------
Layer getitem_19_247, operation 249/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_19
		- child layers: eq_19_248
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.869E-05s
	Output of modules: none
	Lookup keys: -430, 268, getitem_19, getitem_19:1, getitem_19_247, getitem_19_247:1
--------------------------------------------
Layer eq_19_248, operation 250/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_19_247
		- child layers: maskedfill_19_249
		- shares parents with no other layers
		- shares children with layers: mul_19_246
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  7.963E-05s
	Output of modules: none
	Lookup keys: -429, 269, eq_19, eq_19:1, eq_19_248, eq_19_248:1
--------------------------------------------
Layer maskedfill_19_249, operation 251/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -6.4223,     -inf,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -5.0368,  -3.3717,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [-10.2568,  -8.2899,  -7.2056,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [-10.9323,  -8.7405,  -7.5250,  -6.7312,     -inf,     -inf,     -inf,
             -inf],
        [ -6.8017,  -5.2805,  -4.0398,  -3.5140,  -5.2889,     -inf,     -inf,
             -inf],
        [ -9.9520,  -7.4012,  -6.1650,  -5.6442,  -7.2718,  -2.8192,     -inf,
             -inf],
        [-12.4491,  -9.7433,  -8.5524,  -7.7086,  -9.7189,  -3.3842,  -7.3579,
             -inf],
        [-10.1368,  -7.9422,  -6.9874,  -6.2268,  -7.9945,  -3.6483,  -6.7991,
          -7.8819]])...
	Related Layers:
		- parent layers: mul_19_246, eq_19_248
		- child layers: softmax_19_250
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.2.sa.heads.2:1
	Time elapsed:  8.416E-05s
	Output of modules: none
	Lookup keys: -428, 270, maskedfill_19, maskedfill_19:1, maskedfill_19_249, maskedfill_19_249:1
--------------------------------------------
Layer softmax_19_250, operation 252/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.5908e-01, 8.4092e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [3.4142e-02, 2.4406e-01, 7.2180e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [9.3545e-03, 8.3741e-02, 2.8237e-01, 6.2453e-01, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.8965e-02, 8.6819e-02, 3.0023e-01, 5.0790e-01, 8.6094e-02, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [7.1465e-04, 9.1603e-03, 3.1534e-02, 5.3083e-02, 1.0426e-02, 8.9508e-01,
         0.0000e+00, 0.0000e+00],
        [1.1106e-04, 1.6622e-03, 5.4691e-03, 1.2716e-02, 1.7032e-03, 9.6028e-01,
         1.8058e-02, 0.0000e+00],
        [1.2708e-03, 1.1406e-02, 2.9635e-02, 6.3409e-02, 1.0825e-02, 8.3556e-01,
         3.5777e-02, 1.2116e-02]])...
	Related Layers:
		- parent layers: maskedfill_19_249
		- child layers: dropout_23_251
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.2.sa.heads.2:1
	Time elapsed:  7.486E-05s
	Output of modules: none
	Lookup keys: -427, 271, softmax_19, softmax_19:1, softmax_19_250, softmax_19_250:1
--------------------------------------------
Layer dropout_23_251, operation 253/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.5908e-01, 8.4092e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [3.4142e-02, 2.4406e-01, 7.2180e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [9.3545e-03, 8.3741e-02, 2.8237e-01, 6.2453e-01, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.8965e-02, 8.6819e-02, 3.0023e-01, 5.0790e-01, 8.6094e-02, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [7.1465e-04, 9.1603e-03, 3.1534e-02, 5.3083e-02, 1.0426e-02, 8.9508e-01,
         0.0000e+00, 0.0000e+00],
        [1.1106e-04, 1.6622e-03, 5.4691e-03, 1.2716e-02, 1.7032e-03, 9.6028e-01,
         1.8058e-02, 0.0000e+00],
        [1.2708e-03, 1.1406e-02, 2.9635e-02, 6.3409e-02, 1.0825e-02, 8.3556e-01,
         3.5777e-02, 1.2116e-02]])...
	Related Layers:
		- parent layers: softmax_19_250
		- child layers: matmul_38_253
		- shares parents with no other layers
		- shares children with layers: linear_63_252
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.2.sa.heads.2.dropout:1
	Time elapsed:  5.436E-05s
	Output of modules: blocks.2.sa.heads.2.dropout
	Output of bottom-level module: blocks.2.sa.heads.2.dropout:1
	Lookup keys: -426, 272, blocks.2.sa.heads.2.dropout, blocks.2.sa.heads.2.dropout:1, dropout_23, dropout_23:1, dropout_23_251, dropout_23_251:1
--------------------------------------------
Layer linear_63_252, operation 254/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.4609,  0.2630,  1.0002, -0.7791,  0.7447,  0.2699,  0.3321, -0.8225],
        [-0.9596, -0.2208,  0.1500, -1.0087,  1.6135, -0.4144,  0.6379, -1.0468],
        [-0.9277,  0.4541,  0.8834, -0.6693,  0.5440, -0.0877, -0.0071, -1.1571],
        [-0.6563,  0.5795,  0.5439, -0.2324,  0.4863, -0.7063, -0.0043, -1.0459],
        [-0.3942,  1.0380,  1.0323, -0.1683,  0.6576,  0.2046, -0.4913, -0.8746],
        [-0.1440, -2.3361, -0.2613, -0.4284,  0.6013, -0.2539,  0.6618,  0.5537],
        [-0.1829, -0.7886, -0.1634, -0.4338,  1.4210, -0.6454,  0.1523, -0.2335],
        [-1.3576, -0.2434,  0.6813, -0.5251,  1.3098, -0.1834, -0.2771, -0.3015]])...
	Related Layers:
		- parent layers: layernorm_5_217
		- child layers: matmul_38_253
		- shares parents with layers: linear_55_218, linear_56_219, linear_57_228, linear_58_230, linear_59_231, linear_60_240, linear_61_242, linear_62_243, linear_64_254, linear_65_255, linear_66_264, linear_67_266, linear_68_267, linear_69_276, linear_70_278, linear_71_279, linear_72_288, linear_73_290, linear_74_291, linear_75_300, linear_76_302, linear_77_303, linear_78_312
		- shares children with layers: dropout_23_251
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.2.value:1
	Time elapsed:  1.199E-04s
	Output of modules: blocks.2.sa.heads.2.value
	Output of bottom-level module: blocks.2.sa.heads.2.value:1
	Lookup keys: -425, 273, blocks.2.sa.heads.2.value, blocks.2.sa.heads.2.value:1, linear_63, linear_63:1, linear_63_252, linear_63_252:1
--------------------------------------------
Layer matmul_38_253, operation 255/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.4609,  0.2630,  1.0002, -0.7791,  0.7447,  0.2699,  0.3321, -0.8225],
        [-0.8803, -0.1438,  0.2852, -0.9722,  1.4753, -0.3055,  0.5892, -1.0111],
        [-0.9195,  0.2829,  0.7084, -0.7559,  0.8119, -0.1552,  0.1619, -1.1188],
        [-0.7565,  0.4741,  0.6110, -0.4259,  0.5994, -0.4981,  0.0518, -1.0753],
        [-0.7379,  0.5059,  0.6623, -0.4358,  0.6211, -0.3983,  0.0151, -1.0604],
        [-0.2062, -2.0369, -0.1643, -0.4284,  0.6033, -0.2690,  0.5929,  0.3843],
        [-0.1573, -2.2463, -0.2400, -0.4279,  0.6161, -0.2652,  0.6384,  0.5045],
        [-0.2282, -1.9239, -0.1411, -0.4287,  0.6425, -0.2870,  0.5570,  0.3276]])...
	Related Layers:
		- parent layers: dropout_23_251, linear_63_252
		- child layers: cat_3_314
		- shares parents with no other layers
		- shares children with layers: matmul_34_229, matmul_36_241, matmul_40_265, matmul_42_277, matmul_44_289, matmul_46_301, matmul_48_313
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.2:1
	Time elapsed:  8.368E-05s
	Output of modules: blocks.2.sa.heads.2
	Lookup keys: -424, 274, blocks.2.sa.heads.2, blocks.2.sa.heads.2:1, matmul_38, matmul_38:1, matmul_38_253, matmul_38_253:1
--------------------------------------------
Layer linear_64_254, operation 256/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.4933,  4.9192, -4.7456, -5.0968, -6.4061,  3.3777, -5.6597, -4.9758],
        [-0.3546,  4.7193, -3.8638, -5.1730, -5.4195,  3.0491, -4.3618, -4.3125],
        [-0.3346,  4.1108, -3.1867, -4.1729, -4.6855,  2.0716, -4.3176, -3.4348],
        [-0.3973,  4.2248, -3.2067, -3.9080, -4.7944,  1.4170, -4.1374, -3.2645],
        [ 0.0521,  3.5682, -3.5263, -3.5544, -3.7019,  2.2795, -3.7917, -3.0949],
        [-0.9966,  6.3575, -3.9696, -6.4371, -6.5946,  4.8782, -5.3063, -6.6917],
        [-0.5120,  4.7762, -3.9369, -5.5952, -4.2808,  3.0874, -3.7711, -4.7154],
        [-1.9058,  2.2125, -2.7328, -2.7912, -2.9035,  1.9681, -3.8285, -2.0008]])...
	Related Layers:
		- parent layers: layernorm_5_217
		- child layers: transpose_20_256
		- shares parents with layers: linear_55_218, linear_56_219, linear_57_228, linear_58_230, linear_59_231, linear_60_240, linear_61_242, linear_62_243, linear_63_252, linear_65_255, linear_66_264, linear_67_266, linear_68_267, linear_69_276, linear_70_278, linear_71_279, linear_72_288, linear_73_290, linear_74_291, linear_75_300, linear_76_302, linear_77_303, linear_78_312
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.3.key:1
	Time elapsed:  1.147E-04s
	Output of modules: blocks.2.sa.heads.3.key
	Output of bottom-level module: blocks.2.sa.heads.3.key:1
	Lookup keys: -423, 275, blocks.2.sa.heads.3.key, blocks.2.sa.heads.3.key:1, linear_64, linear_64:1, linear_64_254, linear_64_254:1
--------------------------------------------
Layer linear_65_255, operation 257/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-1.6045, -0.9834,  0.1504,  0.6030,  0.3645,  1.4973,  1.5412,  1.0589],
        [-0.9024, -1.5858, -0.3044,  0.4902,  1.1072,  0.2810,  0.7989,  1.0921],
        [-1.0330, -0.9565, -0.8066,  0.8442,  0.1650,  0.7651,  1.4545,  1.3024],
        [-0.7971, -0.8054, -0.6729,  0.3264, -0.1922,  0.6801,  1.3993,  1.7020],
        [-1.3009, -1.6045, -0.0338,  0.5671,  0.7598,  0.3370,  1.1051,  1.5203],
        [-0.1459, -2.0984,  2.8416,  1.3057,  1.4889,  1.3795,  1.2485,  0.5161],
        [-0.1104, -3.6990,  2.4391,  2.1769,  2.4024,  0.0717,  1.6469,  2.0904],
        [-1.1562, -2.1115,  0.1826,  0.7172,  1.1555, -0.6423,  0.8558,  0.8702]])...
	Related Layers:
		- parent layers: layernorm_5_217
		- child layers: matmul_39_257
		- shares parents with layers: linear_55_218, linear_56_219, linear_57_228, linear_58_230, linear_59_231, linear_60_240, linear_61_242, linear_62_243, linear_63_252, linear_64_254, linear_66_264, linear_67_266, linear_68_267, linear_69_276, linear_70_278, linear_71_279, linear_72_288, linear_73_290, linear_74_291, linear_75_300, linear_76_302, linear_77_303, linear_78_312
		- shares children with layers: transpose_20_256
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.3.query:1
	Time elapsed:  1.147E-04s
	Output of modules: blocks.2.sa.heads.3.query
	Output of bottom-level module: blocks.2.sa.heads.3.query:1
	Lookup keys: -422, 276, blocks.2.sa.heads.3.query, blocks.2.sa.heads.3.query:1, linear_65, linear_65:1, linear_65_255, linear_65_255:1
--------------------------------------------
Layer transpose_20_256, operation 258/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.4933, -0.3546, -0.3346, -0.3973,  0.0521, -0.9966, -0.5120, -1.9058],
        [ 4.9192,  4.7193,  4.1108,  4.2248,  3.5682,  6.3575,  4.7762,  2.2125],
        [-4.7456, -3.8638, -3.1867, -3.2067, -3.5263, -3.9696, -3.9369, -2.7328],
        [-5.0968, -5.1730, -4.1729, -3.9080, -3.5544, -6.4371, -5.5952, -2.7912],
        [-6.4061, -5.4195, -4.6855, -4.7944, -3.7019, -6.5946, -4.2808, -2.9035],
        [ 3.3777,  3.0491,  2.0716,  1.4170,  2.2795,  4.8782,  3.0874,  1.9681],
        [-5.6597, -4.3618, -4.3176, -4.1374, -3.7917, -5.3063, -3.7711, -3.8285],
        [-4.9758, -4.3125, -3.4348, -3.2645, -3.0949, -6.6917, -4.7154, -2.0008]])...
	Related Layers:
		- parent layers: linear_64_254
		- child layers: matmul_39_257
		- shares parents with no other layers
		- shares children with layers: linear_65_255
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.2.sa.heads.3:1
	Time elapsed:  5.889E-05s
	Output of modules: none
	Lookup keys: -421, 277, transpose_20, transpose_20:1, transpose_20_256, transpose_20_256:1
--------------------------------------------
Layer matmul_39_257, operation 259/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-3.7292e+01, -2.6140e+01, -3.3599e+01, -3.6004e+01, -2.4733e+01,
         -2.5157e+01, -1.3448e+01, -7.8945e+00],
        [-6.5655e+01, -4.8123e+01, -4.1864e+01, -4.4427e+01, -3.6779e+01,
         -5.4434e+01, -4.7629e+01, -1.9825e+01],
        [-1.3936e+01, -3.3731e+00, -1.1794e+01, -1.3624e+01, -8.9965e+00,
         -1.2316e+00, -9.3184e-03,  3.9628e+00],
        [-1.5123e+01, -4.9385e+00, -1.5038e+01, -1.6836e+01, -1.1641e+01,
         -4.6345e+00,  1.6180e+00,  1.6276e+00],
        [-2.5767e+01, -1.5508e+01, -2.5502e+01, -2.7468e+01, -2.2002e+01,
         -1.2293e+01, -2.3749e+00, -3.0018e+00],
        [-1.4841e+02, -1.2363e+02, -1.0452e+02, -1.0498e+02, -8.9991e+01,
         -1.3086e+02, -1.1233e+02, -6.7233e+01],
        [-2.1058e+02, -1.8223e+02, -1.5188e+02, -1.4883e+02, -1.2915e+02,
         -2.1924e+02, -1.7913e+02, -1.0444e+02],
        [-5.6926e+01, -4.2732e+01, -4.2199e+01, -4.2379e+01, -3.4878e+01,
         -5.8841e+01, -3.8623e+01, -2.1700e+01]])...
	Related Layers:
		- parent layers: linear_65_255, transpose_20_256
		- child layers: mul_20_258
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.3:1
	Time elapsed:  8.678E-05s
	Output of modules: none
	Lookup keys: -420, 278, matmul_39, matmul_39:1, matmul_39_257, matmul_39_257:1
--------------------------------------------
Layer mul_20_258, operation 260/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-1.9031e+00, -1.3339e+00, -1.7146e+00, -1.8373e+00, -1.2622e+00,
         -1.2838e+00, -6.8625e-01, -4.0286e-01],
        [-3.3504e+00, -2.4557e+00, -2.1364e+00, -2.2672e+00, -1.8769e+00,
         -2.7778e+00, -2.4306e+00, -1.0117e+00],
        [-7.1118e-01, -1.7213e-01, -6.0185e-01, -6.9524e-01, -4.5910e-01,
         -6.2849e-02, -4.7553e-04,  2.0223e-01],
        [-7.7172e-01, -2.5202e-01, -7.6742e-01, -8.5917e-01, -5.9406e-01,
         -2.3650e-01,  8.2569e-02,  8.3057e-02],
        [-1.3149e+00, -7.9139e-01, -1.3014e+00, -1.4017e+00, -1.1228e+00,
         -6.2734e-01, -1.2119e-01, -1.5318e-01],
        [-7.5736e+00, -6.3091e+00, -5.3336e+00, -5.3574e+00, -4.5923e+00,
         -6.6780e+00, -5.7325e+00, -3.4310e+00],
        [-1.0746e+01, -9.2996e+00, -7.7504e+00, -7.5952e+00, -6.5907e+00,
         -1.1188e+01, -9.1414e+00, -5.3296e+00],
        [-2.9050e+00, -2.1806e+00, -2.1535e+00, -2.1626e+00, -1.7799e+00,
         -3.0027e+00, -1.9710e+00, -1.1074e+00]])...
	Related Layers:
		- parent layers: matmul_39_257
		- child layers: maskedfill_20_261
		- shares parents with no other layers
		- shares children with layers: eq_20_260
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.2.sa.heads.3:1
	Time elapsed:  6.771E-05s
	Output of modules: none
	Lookup keys: -419, 279, mul_20, mul_20:1, mul_20_258, mul_20_258:1
--------------------------------------------
Layer getitem_20_259, operation 261/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_20
		- child layers: eq_20_260
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.441E-05s
	Output of modules: none
	Lookup keys: -417, 281, getitem_20, getitem_20:1, getitem_20_259, getitem_20_259:1
--------------------------------------------
Layer eq_20_260, operation 262/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_20_259
		- child layers: maskedfill_20_261
		- shares parents with no other layers
		- shares children with layers: mul_20_258
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  7.820E-05s
	Output of modules: none
	Lookup keys: -416, 282, eq_20, eq_20:1, eq_20_260, eq_20_260:1
--------------------------------------------
Layer maskedfill_20_261, operation 263/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -1.9031,     -inf,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -3.3504,  -2.4557,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -0.7112,  -0.1721,  -0.6018,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -0.7717,  -0.2520,  -0.7674,  -0.8592,     -inf,     -inf,     -inf,
             -inf],
        [ -1.3149,  -0.7914,  -1.3014,  -1.4017,  -1.1228,     -inf,     -inf,
             -inf],
        [ -7.5736,  -6.3091,  -5.3336,  -5.3574,  -4.5923,  -6.6780,     -inf,
             -inf],
        [-10.7462,  -9.2996,  -7.7504,  -7.5952,  -6.5907, -11.1881,  -9.1414,
             -inf],
        [ -2.9050,  -2.1806,  -2.1535,  -2.1626,  -1.7799,  -3.0027,  -1.9710,
          -1.1074]])...
	Related Layers:
		- parent layers: mul_20_258, eq_20_260
		- child layers: softmax_20_262
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.2.sa.heads.3:1
	Time elapsed:  8.249E-05s
	Output of modules: none
	Lookup keys: -415, 283, maskedfill_20, maskedfill_20:1, maskedfill_20_261, maskedfill_20_261:1
--------------------------------------------
Layer softmax_20_262, operation 264/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2901, 0.7099, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2611, 0.4476, 0.2913, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2173, 0.3654, 0.2182, 0.1991, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1715, 0.2895, 0.1739, 0.1573, 0.2078, 0.0000, 0.0000, 0.0000],
        [0.0221, 0.0782, 0.2075, 0.2026, 0.4355, 0.0541, 0.0000, 0.0000],
        [0.0085, 0.0360, 0.1695, 0.1979, 0.5405, 0.0054, 0.0422, 0.0000],
        [0.0504, 0.1039, 0.1068, 0.1058, 0.1552, 0.0457, 0.1282, 0.3040]])...
	Related Layers:
		- parent layers: maskedfill_20_261
		- child layers: dropout_24_263
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.2.sa.heads.3:1
	Time elapsed:  6.986E-05s
	Output of modules: none
	Lookup keys: -414, 284, softmax_20, softmax_20:1, softmax_20_262, softmax_20_262:1
--------------------------------------------
Layer dropout_24_263, operation 265/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2901, 0.7099, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2611, 0.4476, 0.2913, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2173, 0.3654, 0.2182, 0.1991, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1715, 0.2895, 0.1739, 0.1573, 0.2078, 0.0000, 0.0000, 0.0000],
        [0.0221, 0.0782, 0.2075, 0.2026, 0.4355, 0.0541, 0.0000, 0.0000],
        [0.0085, 0.0360, 0.1695, 0.1979, 0.5405, 0.0054, 0.0422, 0.0000],
        [0.0504, 0.1039, 0.1068, 0.1058, 0.1552, 0.0457, 0.1282, 0.3040]])...
	Related Layers:
		- parent layers: softmax_20_262
		- child layers: matmul_40_265
		- shares parents with no other layers
		- shares children with layers: linear_66_264
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.2.sa.heads.3.dropout:1
	Time elapsed:  5.603E-05s
	Output of modules: blocks.2.sa.heads.3.dropout
	Output of bottom-level module: blocks.2.sa.heads.3.dropout:1
	Lookup keys: -413, 285, blocks.2.sa.heads.3.dropout, blocks.2.sa.heads.3.dropout:1, dropout_24, dropout_24:1, dropout_24_263, dropout_24_263:1
--------------------------------------------
Layer linear_66_264, operation 266/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 9.9727e-01,  6.1670e-01, -2.3224e-01,  1.1600e+00, -1.6748e-01,
          3.7262e-01, -1.0120e+00,  9.9909e-02],
        [ 3.8622e-01,  4.2290e-01, -4.0442e-01,  1.1610e+00, -1.1129e+00,
          7.8720e-01, -1.1570e+00, -6.4364e-02],
        [ 1.3681e+00,  1.5645e-01, -7.5031e-02,  7.0253e-01, -3.0028e-01,
          4.7047e-01, -8.2939e-01,  1.5254e-01],
        [ 1.6915e+00,  7.5028e-02,  1.2025e-04,  1.1079e+00, -3.0409e-01,
          5.0304e-01, -1.2068e+00, -1.8249e-01],
        [ 1.9418e+00, -1.5384e-01, -1.1803e-01,  1.0569e+00, -5.4157e-01,
         -6.0500e-01, -3.3464e-01, -8.0559e-01],
        [-8.8318e-01,  8.5625e-01,  3.4595e-01,  1.5823e+00,  6.4810e-01,
          1.3863e+00, -2.6105e-02,  1.4825e+00],
        [-1.0479e-01,  1.2091e+00, -9.4688e-02,  1.2893e+00,  7.7628e-01,
          1.3017e+00,  3.3931e-01,  7.0518e-01],
        [ 2.3810e-01,  6.3696e-01, -8.0793e-01,  4.1299e-01, -1.2033e-01,
          1.5088e-01, -8.2193e-01, -1.0099e+00]])...
	Related Layers:
		- parent layers: layernorm_5_217
		- child layers: matmul_40_265
		- shares parents with layers: linear_55_218, linear_56_219, linear_57_228, linear_58_230, linear_59_231, linear_60_240, linear_61_242, linear_62_243, linear_63_252, linear_64_254, linear_65_255, linear_67_266, linear_68_267, linear_69_276, linear_70_278, linear_71_279, linear_72_288, linear_73_290, linear_74_291, linear_75_300, linear_76_302, linear_77_303, linear_78_312
		- shares children with layers: dropout_24_263
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.3.value:1
	Time elapsed:  1.190E-04s
	Output of modules: blocks.2.sa.heads.3.value
	Output of bottom-level module: blocks.2.sa.heads.3.value:1
	Lookup keys: -412, 286, blocks.2.sa.heads.3.value, blocks.2.sa.heads.3.value:1, linear_66, linear_66:1, linear_66_264, linear_66_264:1
--------------------------------------------
Layer matmul_40_265, operation 267/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.9973,  0.6167, -0.2322,  1.1600, -0.1675,  0.3726, -1.0120,  0.0999],
        [ 0.5635,  0.4791, -0.3545,  1.1607, -0.8386,  0.6669, -1.1149, -0.0167],
        [ 0.8318,  0.3959, -0.2635,  1.0272, -0.6294,  0.5867, -1.0237,  0.0417],
        [ 0.9931,  0.3376, -0.2146,  1.0501, -0.5691,  0.5714, -1.0639, -0.0049],
        [ 1.1903,  0.2352, -0.1945,  1.0511, -0.5635,  0.3270, -0.9121, -0.1711],
        [ 1.4767,  0.0737, -0.0850,  1.0326, -0.4155,  0.0809, -0.6766, -0.2788],
        [ 1.6293,  0.0343, -0.0951,  1.0242, -0.4090, -0.0537, -0.5964, -0.4093],
        [ 0.7354,  0.4635, -0.3220,  0.8985, -0.1799,  0.3863, -0.6470, -0.2786]])...
	Related Layers:
		- parent layers: dropout_24_263, linear_66_264
		- child layers: cat_3_314
		- shares parents with no other layers
		- shares children with layers: matmul_34_229, matmul_36_241, matmul_38_253, matmul_42_277, matmul_44_289, matmul_46_301, matmul_48_313
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.3:1
	Time elapsed:  8.392E-05s
	Output of modules: blocks.2.sa.heads.3
	Lookup keys: -411, 287, blocks.2.sa.heads.3, blocks.2.sa.heads.3:1, matmul_40, matmul_40:1, matmul_40_265, matmul_40_265:1
--------------------------------------------
Layer linear_67_266, operation 268/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-2.6091, -4.5986, -3.7034, -2.4651, -5.1870, -1.2907,  3.8126,  4.5725],
        [-3.5575, -4.0494, -4.3719, -4.0263, -5.8664, -2.1406,  3.3321,  4.4700],
        [-2.3926, -4.3218, -3.4815, -2.7432, -4.2845, -1.3080,  2.2276,  3.9906],
        [-1.9159, -4.1312, -3.3896, -2.4073, -4.0541, -0.9624,  1.8951,  3.7876],
        [-1.8379, -3.2919, -3.5932, -1.8283, -3.6201, -0.5005,  2.7184,  4.0256],
        [-4.6834, -4.0903, -3.5988, -4.7665, -3.5102, -3.4611,  3.8471,  3.0564],
        [-4.7229, -4.2550, -3.0534, -5.6854, -3.1214, -3.0455,  3.8633,  2.3647],
        [-1.3182, -2.7809, -3.1687, -2.2114, -3.8615, -1.0797,  1.8504,  2.4119]])...
	Related Layers:
		- parent layers: layernorm_5_217
		- child layers: transpose_21_268
		- shares parents with layers: linear_55_218, linear_56_219, linear_57_228, linear_58_230, linear_59_231, linear_60_240, linear_61_242, linear_62_243, linear_63_252, linear_64_254, linear_65_255, linear_66_264, linear_68_267, linear_69_276, linear_70_278, linear_71_279, linear_72_288, linear_73_290, linear_74_291, linear_75_300, linear_76_302, linear_77_303, linear_78_312
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.4.key:1
	Time elapsed:  1.118E-04s
	Output of modules: blocks.2.sa.heads.4.key
	Output of bottom-level module: blocks.2.sa.heads.4.key:1
	Lookup keys: -410, 288, blocks.2.sa.heads.4.key, blocks.2.sa.heads.4.key:1, linear_67, linear_67:1, linear_67_266, linear_67_266:1
--------------------------------------------
Layer linear_68_267, operation 269/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 2.0166,  1.4531,  3.0878,  1.3142,  0.8607,  0.8020, -0.6224, -1.4687],
        [ 1.8263,  0.6067,  3.7748,  1.0489,  2.3119,  1.2997, -1.5022, -2.0180],
        [ 2.4779,  1.3221,  3.2456,  1.2713,  0.2506,  0.7920, -1.3946, -1.0979],
        [ 2.4167,  1.0959,  3.1408,  1.8367,  0.2910,  1.3209, -1.3170, -0.9298],
        [ 2.3528,  1.0725,  3.2431,  2.7248,  1.6901,  0.7051, -1.7894, -1.1803],
        [ 1.3608,  1.3321,  1.9239, -0.5679, -0.5365,  0.6420, -0.7088, -0.7388],
        [ 1.8596,  2.0385,  3.5505,  0.4294,  1.3840,  0.2136, -1.3569, -1.1043],
        [ 1.5740,  1.1239,  3.0847,  2.4013,  0.8405,  1.4953, -1.8955, -1.0453]])...
	Related Layers:
		- parent layers: layernorm_5_217
		- child layers: matmul_41_269
		- shares parents with layers: linear_55_218, linear_56_219, linear_57_228, linear_58_230, linear_59_231, linear_60_240, linear_61_242, linear_62_243, linear_63_252, linear_64_254, linear_65_255, linear_66_264, linear_67_266, linear_69_276, linear_70_278, linear_71_279, linear_72_288, linear_73_290, linear_74_291, linear_75_300, linear_76_302, linear_77_303, linear_78_312
		- shares children with layers: transpose_21_268
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.4.query:1
	Time elapsed:  1.111E-04s
	Output of modules: blocks.2.sa.heads.4.query
	Output of bottom-level module: blocks.2.sa.heads.4.query:1
	Lookup keys: -409, 289, blocks.2.sa.heads.4.query, blocks.2.sa.heads.4.query:1, linear_68, linear_68:1, linear_68_267, linear_68_267:1
--------------------------------------------
Layer transpose_21_268, operation 270/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-2.6091, -3.5575, -2.3926, -1.9159, -1.8379, -4.6834, -4.7229, -1.3182],
        [-4.5986, -4.0494, -4.3218, -4.1312, -3.2919, -4.0903, -4.2550, -2.7809],
        [-3.7034, -4.3719, -3.4815, -3.3896, -3.5932, -3.5988, -3.0534, -3.1687],
        [-2.4651, -4.0263, -2.7432, -2.4073, -1.8283, -4.7665, -5.6854, -2.2114],
        [-5.1870, -5.8664, -4.2845, -4.0541, -3.6201, -3.5102, -3.1214, -3.8615],
        [-1.2907, -2.1406, -1.3080, -0.9624, -0.5005, -3.4611, -3.0455, -1.0797],
        [ 3.8126,  3.3321,  2.2276,  1.8951,  2.7184,  3.8471,  3.8633,  1.8504],
        [ 4.5725,  4.4700,  3.9906,  3.7876,  4.0256,  3.0564,  2.3647,  2.4119]])...
	Related Layers:
		- parent layers: linear_67_266
		- child layers: matmul_41_269
		- shares parents with no other layers
		- shares children with layers: linear_68_267
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.2.sa.heads.4:1
	Time elapsed:  5.889E-05s
	Output of modules: none
	Lookup keys: -408, 290, transpose_21, transpose_21:1, transpose_21_268, transpose_21_268:1
--------------------------------------------
Layer matmul_41_269, operation 271/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-116.7665, -120.8618, -101.9922,  -93.9021,  -96.0306, -117.2115,
         -111.5580,  -76.2510],
        [-159.2766, -164.6879, -133.2892, -123.1809, -124.7587, -159.5550,
         -144.5282, -115.0847],
        [-128.6819, -136.4425, -105.8897,  -95.0090, -104.7356, -167.7393,
         -161.1713,  -94.4646],
        [-127.5880, -140.7485, -104.9275,  -93.8586, -103.9552, -182.9403,
         -174.9819,  -97.5768],
        [-214.4664, -228.7728, -191.0863, -177.3093, -163.2782, -237.3263,
         -232.2940, -165.2826],
        [ -32.1567,  -33.5765,  -15.1712,  -11.7386,  -37.3092,  -72.5734,
          -67.5222,  -23.4003],
        [-158.3979, -162.5374, -132.3573, -122.3180, -126.3351, -175.9837,
         -167.2755, -120.5222],
        [-185.7938, -207.1791, -159.0617, -146.0517, -143.1922, -268.0339,
         -260.6301, -159.7157]])...
	Related Layers:
		- parent layers: linear_68_267, transpose_21_268
		- child layers: mul_21_270
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.4:1
	Time elapsed:  8.631E-05s
	Output of modules: none
	Lookup keys: -407, 291, matmul_41, matmul_41:1, matmul_41_269, matmul_41_269:1
--------------------------------------------
Layer mul_21_270, operation 272/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -5.9587,  -6.1677,  -5.2048,  -4.7919,  -4.9005,  -5.9814,  -5.6929,
          -3.8912],
        [ -8.1281,  -8.4042,  -6.8019,  -6.2860,  -6.3666,  -8.1423,  -7.3754,
          -5.8729],
        [ -6.5668,  -6.9628,  -5.4037,  -4.8484,  -5.3448,  -8.5599,  -8.2247,
          -4.8206],
        [ -6.5109,  -7.1825,  -5.3546,  -4.7897,  -5.3049,  -9.3356,  -8.9295,
          -4.9794],
        [-10.9444, -11.6745,  -9.7513,  -9.0483,  -8.3323, -12.1110, -11.8542,
          -8.4345],
        [ -1.6410,  -1.7134,  -0.7742,  -0.5990,  -1.9039,  -3.7035,  -3.4457,
          -1.1941],
        [ -8.0832,  -8.2945,  -6.7543,  -6.2420,  -6.4470,  -8.9806,  -8.5362,
          -6.1504],
        [ -9.4813, -10.5726,  -8.1171,  -7.4532,  -7.3072, -13.6781, -13.3002,
          -8.1505]])...
	Related Layers:
		- parent layers: matmul_41_269
		- child layers: maskedfill_21_273
		- shares parents with no other layers
		- shares children with layers: eq_21_272
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.2.sa.heads.4:1
	Time elapsed:  6.795E-05s
	Output of modules: none
	Lookup keys: -406, 292, mul_21, mul_21:1, mul_21_270, mul_21_270:1
--------------------------------------------
Layer getitem_21_271, operation 273/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_21
		- child layers: eq_21_272
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.394E-05s
	Output of modules: none
	Lookup keys: -404, 294, getitem_21, getitem_21:1, getitem_21_271, getitem_21_271:1
--------------------------------------------
Layer eq_21_272, operation 274/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_21_271
		- child layers: maskedfill_21_273
		- shares parents with no other layers
		- shares children with layers: mul_21_270
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  7.939E-05s
	Output of modules: none
	Lookup keys: -403, 295, eq_21, eq_21:1, eq_21_272, eq_21_272:1
--------------------------------------------
Layer maskedfill_21_273, operation 275/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -5.9587,     -inf,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -8.1281,  -8.4042,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -6.5668,  -6.9628,  -5.4037,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -6.5109,  -7.1825,  -5.3546,  -4.7897,     -inf,     -inf,     -inf,
             -inf],
        [-10.9444, -11.6745,  -9.7513,  -9.0483,  -8.3323,     -inf,     -inf,
             -inf],
        [ -1.6410,  -1.7134,  -0.7742,  -0.5990,  -1.9039,  -3.7035,     -inf,
             -inf],
        [ -8.0832,  -8.2945,  -6.7543,  -6.2420,  -6.4470,  -8.9806,  -8.5362,
             -inf],
        [ -9.4813, -10.5726,  -8.1171,  -7.4532,  -7.3072, -13.6781, -13.3002,
          -8.1505]])...
	Related Layers:
		- parent layers: mul_21_270, eq_21_272
		- child layers: softmax_21_274
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.2.sa.heads.4:1
	Time elapsed:  8.392E-05s
	Output of modules: none
	Lookup keys: -402, 296, maskedfill_21, maskedfill_21:1, maskedfill_21_273, maskedfill_21_273:1
--------------------------------------------
Layer softmax_21_274, operation 276/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [5.6860e-01, 4.3140e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.0522e-01, 1.3811e-01, 6.5667e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [9.7268e-02, 4.9694e-02, 3.0916e-01, 5.4388e-01, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [3.9891e-02, 1.9223e-02, 1.3153e-01, 2.6569e-01, 5.4367e-01, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.2438e-01, 1.1568e-01, 2.9593e-01, 3.5258e-01, 9.5620e-02, 1.5813e-02,
         0.0000e+00, 0.0000e+00],
        [5.5342e-02, 4.4804e-02, 2.0902e-01, 3.4888e-01, 2.8422e-01, 2.2559e-02,
         3.5181e-02, 0.0000e+00],
        [3.9274e-02, 1.3187e-02, 1.5366e-01, 2.9846e-01, 3.4535e-01, 5.9082e-04,
         8.6207e-04, 1.4861e-01]])...
	Related Layers:
		- parent layers: maskedfill_21_273
		- child layers: dropout_25_275
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.2.sa.heads.4:1
	Time elapsed:  7.248E-05s
	Output of modules: none
	Lookup keys: -401, 297, softmax_21, softmax_21:1, softmax_21_274, softmax_21_274:1
--------------------------------------------
Layer dropout_25_275, operation 277/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [5.6860e-01, 4.3140e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.0522e-01, 1.3811e-01, 6.5667e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [9.7268e-02, 4.9694e-02, 3.0916e-01, 5.4388e-01, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [3.9891e-02, 1.9223e-02, 1.3153e-01, 2.6569e-01, 5.4367e-01, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.2438e-01, 1.1568e-01, 2.9593e-01, 3.5258e-01, 9.5620e-02, 1.5813e-02,
         0.0000e+00, 0.0000e+00],
        [5.5342e-02, 4.4804e-02, 2.0902e-01, 3.4888e-01, 2.8422e-01, 2.2559e-02,
         3.5181e-02, 0.0000e+00],
        [3.9274e-02, 1.3187e-02, 1.5366e-01, 2.9846e-01, 3.4535e-01, 5.9082e-04,
         8.6207e-04, 1.4861e-01]])...
	Related Layers:
		- parent layers: softmax_21_274
		- child layers: matmul_42_277
		- shares parents with no other layers
		- shares children with layers: linear_69_276
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.2.sa.heads.4.dropout:1
	Time elapsed:  5.746E-05s
	Output of modules: blocks.2.sa.heads.4.dropout
	Output of bottom-level module: blocks.2.sa.heads.4.dropout:1
	Lookup keys: -400, 298, blocks.2.sa.heads.4.dropout, blocks.2.sa.heads.4.dropout:1, dropout_25, dropout_25:1, dropout_25_275, dropout_25_275:1
--------------------------------------------
Layer linear_69_276, operation 278/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.0268,  0.1778,  0.3324,  0.0397, -0.1482,  0.4618,  1.6423,  0.0567],
        [ 0.8605,  1.0713,  0.3541, -0.2049,  0.0784, -0.3830,  1.0527, -0.5660],
        [ 0.2943,  0.6559,  0.5518,  0.9180,  0.2866,  0.1112,  0.5690,  0.3099],
        [ 0.4829,  0.6906,  0.6782,  0.4925,  0.1278,  0.0793,  0.7416,  0.3597],
        [-0.0598, -0.6612,  1.0671,  0.3340,  0.7838,  0.1850,  0.5598,  0.0206],
        [ 0.2952,  1.0446,  0.0214, -0.2059, -1.0364,  0.7438,  2.0841, -0.4736],
        [-0.2160,  0.8796, -0.1963,  0.5649, -0.4922,  0.3741,  1.6146, -0.8766],
        [ 1.2176,  0.3387,  0.2886,  0.1831,  0.5356, -0.5232,  0.8078, -0.7118]])...
	Related Layers:
		- parent layers: layernorm_5_217
		- child layers: matmul_42_277
		- shares parents with layers: linear_55_218, linear_56_219, linear_57_228, linear_58_230, linear_59_231, linear_60_240, linear_61_242, linear_62_243, linear_63_252, linear_64_254, linear_65_255, linear_66_264, linear_67_266, linear_68_267, linear_70_278, linear_71_279, linear_72_288, linear_73_290, linear_74_291, linear_75_300, linear_76_302, linear_77_303, linear_78_312
		- shares children with layers: dropout_25_275
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.4.value:1
	Time elapsed:  1.197E-04s
	Output of modules: blocks.2.sa.heads.4.value
	Output of bottom-level module: blocks.2.sa.heads.4.value:1
	Lookup keys: -399, 299, blocks.2.sa.heads.4.value, blocks.2.sa.heads.4.value:1, linear_69, linear_69:1, linear_69_276, linear_69_276:1
--------------------------------------------
Layer matmul_42_277, operation 279/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.0268,  0.1778,  0.3324,  0.0397, -0.1482,  0.4618,  1.6423,  0.0567],
        [ 0.3865,  0.5633,  0.3418, -0.0658, -0.0504,  0.0974,  1.3879, -0.2119],
        [ 0.3176,  0.6152,  0.4795,  0.5827,  0.1687,  0.1149,  0.8560,  0.1370],
        [ 0.3990,  0.6489,  0.5894,  0.5453,  0.1476,  0.1034,  0.7913,  0.2688],
        [ 0.1521, -0.0620,  0.8530,  0.4309,  0.4934,  0.1474,  0.6620,  0.1389],
        [ 0.3592,  0.5369,  0.5871,  0.4552,  0.1791,  0.1035,  0.8423,  0.1546],
        [ 0.2521,  0.3025,  0.6831,  0.4669,  0.2819,  0.1419,  0.7786,  0.1324],
        [ 0.3620,  0.1514,  0.7162,  0.4298,  0.4267,  0.0408,  0.7031,  0.0500]])...
	Related Layers:
		- parent layers: dropout_25_275, linear_69_276
		- child layers: cat_3_314
		- shares parents with no other layers
		- shares children with layers: matmul_34_229, matmul_36_241, matmul_38_253, matmul_40_265, matmul_44_289, matmul_46_301, matmul_48_313
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.4:1
	Time elapsed:  8.345E-05s
	Output of modules: blocks.2.sa.heads.4
	Lookup keys: -398, 300, blocks.2.sa.heads.4, blocks.2.sa.heads.4:1, matmul_42, matmul_42:1, matmul_42_277, matmul_42_277:1
--------------------------------------------
Layer linear_70_278, operation 280/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 4.2605,  1.9108, -3.5143, -4.9380, -1.6395,  3.9266, -1.1882, -5.0895],
        [ 4.0726,  1.1297, -4.7195, -4.6760, -0.4470,  3.5077, -2.4587, -6.1058],
        [ 4.5123,  1.4035, -3.0081, -4.4333, -0.6095,  3.6386, -2.2145, -4.4655],
        [ 3.5916,  1.2381, -3.2405, -4.2285, -1.0572,  3.6310, -1.8394, -4.3284],
        [ 3.7088,  0.5962, -1.1227, -3.4262, -1.2932,  3.3374, -1.6547, -3.1958],
        [ 2.8964,  4.0243, -6.3999, -4.6416, -2.7854,  1.1860, -2.4528, -6.3370],
        [ 2.8021,  2.2667, -5.2643, -3.8344, -0.6475,  0.2400, -3.8896, -6.6609],
        [ 3.1157,  1.3526, -2.2792, -3.5071,  0.4521,  1.8807, -2.2157, -4.8922]])...
	Related Layers:
		- parent layers: layernorm_5_217
		- child layers: transpose_22_280
		- shares parents with layers: linear_55_218, linear_56_219, linear_57_228, linear_58_230, linear_59_231, linear_60_240, linear_61_242, linear_62_243, linear_63_252, linear_64_254, linear_65_255, linear_66_264, linear_67_266, linear_68_267, linear_69_276, linear_71_279, linear_72_288, linear_73_290, linear_74_291, linear_75_300, linear_76_302, linear_77_303, linear_78_312
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.5.key:1
	Time elapsed:  1.121E-04s
	Output of modules: blocks.2.sa.heads.5.key
	Output of bottom-level module: blocks.2.sa.heads.5.key:1
	Lookup keys: -397, 301, blocks.2.sa.heads.5.key, blocks.2.sa.heads.5.key:1, linear_70, linear_70:1, linear_70_278, linear_70_278:1
--------------------------------------------
Layer linear_71_279, operation 281/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.3439, -0.9941,  1.5913, -0.3458,  2.4806, -1.1816, -0.9145,  0.9467],
        [-0.8890, -1.7387,  0.8275,  1.9089,  0.9850,  0.2672, -0.1992,  1.7128],
        [-1.1492, -1.7633,  0.0911,  0.7632,  0.7935,  0.6957, -0.3856,  0.9745],
        [-1.4086, -1.4499, -0.1391,  0.2114,  1.0479,  0.3299, -0.4216,  0.5146],
        [-0.9993, -2.3922,  0.7737,  0.0693,  2.3365,  0.5863,  0.2348,  0.8679],
        [-0.5401, -0.0123,  1.1560,  1.1919,  0.4041, -1.0246, -0.0769,  2.4079],
        [-1.4185, -0.9442,  0.8168,  2.6328,  1.2409, -0.2457, -0.0726,  1.5509],
        [-2.9542, -2.4767, -0.0397,  2.4334,  1.0795,  0.1446,  1.1566,  1.8510]])...
	Related Layers:
		- parent layers: layernorm_5_217
		- child layers: matmul_43_281
		- shares parents with layers: linear_55_218, linear_56_219, linear_57_228, linear_58_230, linear_59_231, linear_60_240, linear_61_242, linear_62_243, linear_63_252, linear_64_254, linear_65_255, linear_66_264, linear_67_266, linear_68_267, linear_69_276, linear_70_278, linear_72_288, linear_73_290, linear_74_291, linear_75_300, linear_76_302, linear_77_303, linear_78_312
		- shares children with layers: transpose_22_280
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.5.query:1
	Time elapsed:  1.116E-04s
	Output of modules: blocks.2.sa.heads.5.query
	Output of bottom-level module: blocks.2.sa.heads.5.query:1
	Lookup keys: -396, 302, blocks.2.sa.heads.5.query, blocks.2.sa.heads.5.query:1, linear_71, linear_71:1, linear_71_279, linear_71_279:1
--------------------------------------------
Layer transpose_22_280, operation 282/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 4.2605,  4.0726,  4.5123,  3.5916,  3.7088,  2.8964,  2.8021,  3.1157],
        [ 1.9108,  1.1297,  1.4035,  1.2381,  0.5962,  4.0243,  2.2667,  1.3526],
        [-3.5143, -4.7195, -3.0081, -3.2405, -1.1227, -6.3999, -5.2643, -2.2792],
        [-4.9380, -4.6760, -4.4333, -4.2285, -3.4262, -4.6416, -3.8344, -3.5071],
        [-1.6395, -0.4470, -0.6095, -1.0572, -1.2932, -2.7854, -0.6475,  0.4521],
        [ 3.9266,  3.5077,  3.6386,  3.6310,  3.3374,  1.1860,  0.2400,  1.8807],
        [-1.1882, -2.4587, -2.2145, -1.8394, -1.6547, -2.4528, -3.8896, -2.2157],
        [-5.0895, -6.1058, -4.4655, -4.3284, -3.1958, -6.3370, -6.6609, -4.8922]])...
	Related Layers:
		- parent layers: linear_70_278
		- child layers: matmul_43_281
		- shares parents with no other layers
		- shares children with layers: linear_71_279
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.2.sa.heads.5:1
	Time elapsed:  5.913E-05s
	Output of modules: none
	Lookup keys: -395, 303, transpose_22, transpose_22:1, transpose_22_280, transpose_22_280:1
--------------------------------------------
Layer matmul_43_281, operation 283/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-149.8085, -131.8733, -132.0760, -130.7438, -107.7133, -160.6738,
         -117.2951,  -86.7651],
        [-309.8712, -263.1996, -265.8352, -251.8166, -223.5507, -307.9718,
         -219.9200, -212.3837],
        [-226.0098, -187.8166, -196.6452, -185.7305, -163.5069, -224.1034,
         -158.4664, -153.1547],
        [-220.5353, -177.9718, -190.2871, -179.3333, -159.4611, -214.3723,
         -147.3970, -145.2201],
        [-180.2640, -155.4616, -160.2823, -153.7825, -131.3209, -194.1060,
         -142.5600, -114.4213],
        [-244.9205, -213.2211, -211.2646, -200.3457, -166.8210, -249.2030,
         -184.4541, -168.2034],
        [-318.8011, -278.5511, -277.5015, -262.5614, -224.5616, -322.1559,
         -238.9218, -219.2925],
        [-306.8442, -276.0042, -282.8931, -266.6471, -228.5546, -307.6669,
         -245.7942, -214.5919]])...
	Related Layers:
		- parent layers: linear_71_279, transpose_22_280
		- child layers: mul_22_282
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.5:1
	Time elapsed:  8.821E-05s
	Output of modules: none
	Lookup keys: -394, 304, matmul_43, matmul_43:1, matmul_43_281, matmul_43_281:1
--------------------------------------------
Layer mul_22_282, operation 284/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -7.6449,  -6.7296,  -6.7400,  -6.6720,  -5.4967,  -8.1993,  -5.9857,
          -4.4277],
        [-15.8130, -13.4313, -13.5658, -12.8505, -11.4080, -15.7161, -11.2227,
         -10.8382],
        [-11.5335,  -9.5845, -10.0350,  -9.4780,  -8.3439, -11.4362,  -8.0867,
          -7.8156],
        [-11.2541,  -9.0821,  -9.7106,  -9.1516,  -8.1375, -10.9396,  -7.5218,
          -7.4107],
        [ -9.1991,  -7.9334,  -8.1794,  -7.8477,  -6.7014,  -9.9054,  -7.2750,
          -5.8390],
        [-12.4985, -10.8809, -10.7811, -10.2239,  -8.5130, -12.7171,  -9.4129,
          -8.5836],
        [-16.2687, -14.2148, -14.1612, -13.3988, -11.4596, -16.4399, -12.1924,
         -11.1907],
        [-15.6586, -14.0848, -14.4363, -13.6073, -11.6634, -15.7006, -12.5431,
         -10.9508]])...
	Related Layers:
		- parent layers: matmul_43_281
		- child layers: maskedfill_22_285
		- shares parents with no other layers
		- shares children with layers: eq_22_284
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.2.sa.heads.5:1
	Time elapsed:  6.819E-05s
	Output of modules: none
	Lookup keys: -393, 305, mul_22, mul_22:1, mul_22_282, mul_22_282:1
--------------------------------------------
Layer getitem_22_283, operation 285/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_22
		- child layers: eq_22_284
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.322E-05s
	Output of modules: none
	Lookup keys: -391, 307, getitem_22, getitem_22:1, getitem_22_283, getitem_22_283:1
--------------------------------------------
Layer eq_22_284, operation 286/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_22_283
		- child layers: maskedfill_22_285
		- shares parents with no other layers
		- shares children with layers: mul_22_282
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  7.796E-05s
	Output of modules: none
	Lookup keys: -390, 308, eq_22, eq_22:1, eq_22_284, eq_22_284:1
--------------------------------------------
Layer maskedfill_22_285, operation 287/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -7.6449,     -inf,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [-15.8130, -13.4313,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [-11.5335,  -9.5845, -10.0350,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [-11.2541,  -9.0821,  -9.7106,  -9.1516,     -inf,     -inf,     -inf,
             -inf],
        [ -9.1991,  -7.9334,  -8.1794,  -7.8477,  -6.7014,     -inf,     -inf,
             -inf],
        [-12.4985, -10.8809, -10.7811, -10.2239,  -8.5130, -12.7171,     -inf,
             -inf],
        [-16.2687, -14.2148, -14.1612, -13.3988, -11.4596, -16.4399, -12.1924,
             -inf],
        [-15.6586, -14.0848, -14.4363, -13.6073, -11.6634, -15.7006, -12.5431,
         -10.9508]])...
	Related Layers:
		- parent layers: mul_22_282, eq_22_284
		- child layers: softmax_22_286
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.2.sa.heads.5:1
	Time elapsed:  8.416E-05s
	Output of modules: none
	Lookup keys: -389, 309, maskedfill_22, maskedfill_22:1, maskedfill_22_285, maskedfill_22_285:1
--------------------------------------------
Layer softmax_22_286, operation 288/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0846, 0.9154, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0800, 0.5619, 0.3581, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0442, 0.3876, 0.2067, 0.3615, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0429, 0.1519, 0.1188, 0.1655, 0.5208, 0.0000, 0.0000, 0.0000],
        [0.0132, 0.0664, 0.0733, 0.1280, 0.7085, 0.0106, 0.0000, 0.0000],
        [0.0046, 0.0359, 0.0379, 0.0813, 0.5649, 0.0039, 0.2715, 0.0000],
        [0.0049, 0.0235, 0.0165, 0.0378, 0.2642, 0.0047, 0.1096, 0.5388]])...
	Related Layers:
		- parent layers: maskedfill_22_285
		- child layers: dropout_26_287
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.2.sa.heads.5:1
	Time elapsed:  6.938E-05s
	Output of modules: none
	Lookup keys: -388, 310, softmax_22, softmax_22:1, softmax_22_286, softmax_22_286:1
--------------------------------------------
Layer dropout_26_287, operation 289/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0846, 0.9154, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0800, 0.5619, 0.3581, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0442, 0.3876, 0.2067, 0.3615, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0429, 0.1519, 0.1188, 0.1655, 0.5208, 0.0000, 0.0000, 0.0000],
        [0.0132, 0.0664, 0.0733, 0.1280, 0.7085, 0.0106, 0.0000, 0.0000],
        [0.0046, 0.0359, 0.0379, 0.0813, 0.5649, 0.0039, 0.2715, 0.0000],
        [0.0049, 0.0235, 0.0165, 0.0378, 0.2642, 0.0047, 0.1096, 0.5388]])...
	Related Layers:
		- parent layers: softmax_22_286
		- child layers: matmul_44_289
		- shares parents with no other layers
		- shares children with layers: linear_72_288
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.2.sa.heads.5.dropout:1
	Time elapsed:  5.341E-05s
	Output of modules: blocks.2.sa.heads.5.dropout
	Output of bottom-level module: blocks.2.sa.heads.5.dropout:1
	Lookup keys: -387, 311, blocks.2.sa.heads.5.dropout, blocks.2.sa.heads.5.dropout:1, dropout_26, dropout_26:1, dropout_26_287, dropout_26_287:1
--------------------------------------------
Layer linear_72_288, operation 290/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.6991, -0.0640,  0.4011,  0.0938, -0.9778,  0.1813,  1.3622, -0.7862],
        [ 0.5733,  0.9612,  0.7778,  0.0385, -1.7366,  0.8051,  1.5308,  0.0522],
        [ 1.0345,  0.3582,  0.7805, -0.0773, -0.0647, -0.0366,  1.4282, -0.4810],
        [ 1.1490,  0.5966,  0.8933, -0.1543,  0.1008, -0.1099,  1.1123, -0.3992],
        [ 0.9463,  0.2579,  0.5559, -0.5260,  0.4300,  0.2661,  1.5899,  0.5220],
        [ 0.4368,  1.3298, -0.5247, -0.3991, -1.8661, -0.6184,  1.0235, -0.4871],
        [-0.1537,  1.2281,  0.5859, -0.6523, -1.5963,  0.4973,  0.8490, -0.1600],
        [ 0.3331,  0.7236,  1.1066, -0.0659,  0.3616, -0.2525,  0.8652,  0.2118]])...
	Related Layers:
		- parent layers: layernorm_5_217
		- child layers: matmul_44_289
		- shares parents with layers: linear_55_218, linear_56_219, linear_57_228, linear_58_230, linear_59_231, linear_60_240, linear_61_242, linear_62_243, linear_63_252, linear_64_254, linear_65_255, linear_66_264, linear_67_266, linear_68_267, linear_69_276, linear_70_278, linear_71_279, linear_73_290, linear_74_291, linear_75_300, linear_76_302, linear_77_303, linear_78_312
		- shares children with layers: dropout_26_287
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.5.value:1
	Time elapsed:  1.452E-04s
	Output of modules: blocks.2.sa.heads.5.value
	Output of bottom-level module: blocks.2.sa.heads.5.value:1
	Lookup keys: -386, 312, blocks.2.sa.heads.5.value, blocks.2.sa.heads.5.value:1, linear_72, linear_72:1, linear_72_288, linear_72_288:1
--------------------------------------------
Layer matmul_44_289, operation 291/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 6.9907e-01, -6.3989e-02,  4.0113e-01,  9.3826e-02, -9.7776e-01,
          1.8133e-01,  1.3622e+00, -7.8622e-01],
        [ 5.8395e-01,  8.7447e-01,  7.4594e-01,  4.3146e-02, -1.6724e+00,
          7.5238e-01,  1.5165e+00, -1.8754e-02],
        [ 7.4853e-01,  6.6323e-01,  7.4861e-01,  1.4313e-03, -1.0772e+00,
          4.5382e-01,  1.4806e+00, -2.0585e-01],
        [ 8.8234e-01,  6.5944e-01,  8.0347e-01, -5.2716e-02, -6.9316e-01,
          2.7277e-01,  1.3508e+00, -2.5828e-01],
        [ 9.2306e-01,  4.1892e-01,  6.6554e-01, -2.9882e-01, -7.2839e-02,
          2.4618e-01,  1.4729e+00,  1.2288e-01],
        [ 9.4531e-01,  3.6237e-01,  6.1686e-01, -3.9852e-01,  1.6489e-01,
          2.2108e-01,  1.5040e+00,  2.7139e-01],
        [ 6.5095e-01,  5.8054e-01,  6.0306e-01, -4.8943e-01, -2.5887e-01,
          3.0240e-01,  1.3384e+00,  1.9715e-01],
        [ 4.9210e-01,  6.4960e-01,  8.7183e-01, -2.5359e-01,  8.1966e-02,
          8.9642e-04,  1.0923e+00,  2.0660e-01]])...
	Related Layers:
		- parent layers: dropout_26_287, linear_72_288
		- child layers: cat_3_314
		- shares parents with no other layers
		- shares children with layers: matmul_34_229, matmul_36_241, matmul_38_253, matmul_40_265, matmul_42_277, matmul_46_301, matmul_48_313
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.5:1
	Time elapsed:  8.607E-05s
	Output of modules: blocks.2.sa.heads.5
	Lookup keys: -385, 313, blocks.2.sa.heads.5, blocks.2.sa.heads.5:1, matmul_44, matmul_44:1, matmul_44_289, matmul_44_289:1
--------------------------------------------
Layer linear_73_290, operation 292/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 5.9745,  0.6274,  2.7633,  2.2467, -1.1893, -0.9026,  5.5145, -1.2523],
        [ 5.1855, -0.1008,  3.3393,  1.2600, -2.2861,  0.0187,  5.7904, -1.6195],
        [ 4.6083,  2.2610,  1.8979,  2.6661, -1.0678, -1.5404,  6.1847, -1.2928],
        [ 4.5716,  2.2531,  2.1447,  2.9432, -0.9899, -1.6726,  6.2622, -1.1437],
        [ 4.1687,  1.8860,  2.4812,  2.1003, -1.0751, -1.3640,  5.3441, -0.7529],
        [ 5.4610, -1.8259,  4.3198,  1.3858, -4.2904,  2.5466,  5.3339, -1.4918],
        [ 4.9488, -2.5531,  2.9244,  0.4736, -2.5426,  1.9423,  4.0583, -0.9236],
        [ 4.4056,  0.1353,  2.9136,  1.4909, -1.3678, -0.8174,  5.3825, -1.6369]])...
	Related Layers:
		- parent layers: layernorm_5_217
		- child layers: transpose_23_292
		- shares parents with layers: linear_55_218, linear_56_219, linear_57_228, linear_58_230, linear_59_231, linear_60_240, linear_61_242, linear_62_243, linear_63_252, linear_64_254, linear_65_255, linear_66_264, linear_67_266, linear_68_267, linear_69_276, linear_70_278, linear_71_279, linear_72_288, linear_74_291, linear_75_300, linear_76_302, linear_77_303, linear_78_312
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.6.key:1
	Time elapsed:  1.149E-04s
	Output of modules: blocks.2.sa.heads.6.key
	Output of bottom-level module: blocks.2.sa.heads.6.key:1
	Lookup keys: -384, 314, blocks.2.sa.heads.6.key, blocks.2.sa.heads.6.key:1, linear_73, linear_73:1, linear_73_290, linear_73_290:1
--------------------------------------------
Layer linear_74_291, operation 293/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.1515, -1.3074, -0.4546,  0.1087,  0.0639, -0.4473, -0.8657,  0.8324],
        [-1.2934, -2.0344, -2.1363,  0.1949,  1.0951, -0.5138, -2.9811,  2.2137],
        [-0.2995, -1.6825, -0.3840, -0.7382,  0.1681, -0.1253, -0.6077,  0.5415],
        [ 0.3457, -1.8541, -0.1439, -0.7669,  0.3808, -0.2377, -0.7888,  0.5069],
        [-0.1210, -2.8558, -0.4914, -0.7897, -0.4134,  0.7677, -1.2731,  0.5990],
        [-2.6452,  1.8875, -2.5842,  2.8343,  2.6440, -3.6040, -1.4789,  1.8719],
        [-2.4974,  1.8820, -2.7579,  2.1565,  2.6368, -3.1819, -2.4959,  1.9912],
        [-0.6449, -1.9959, -0.2912, -1.0357,  0.1101, -0.2786, -0.8924,  0.9599]])...
	Related Layers:
		- parent layers: layernorm_5_217
		- child layers: matmul_45_293
		- shares parents with layers: linear_55_218, linear_56_219, linear_57_228, linear_58_230, linear_59_231, linear_60_240, linear_61_242, linear_62_243, linear_63_252, linear_64_254, linear_65_255, linear_66_264, linear_67_266, linear_68_267, linear_69_276, linear_70_278, linear_71_279, linear_72_288, linear_73_290, linear_75_300, linear_76_302, linear_77_303, linear_78_312
		- shares children with layers: transpose_23_292
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.6.query:1
	Time elapsed:  1.132E-04s
	Output of modules: blocks.2.sa.heads.6.query
	Output of bottom-level module: blocks.2.sa.heads.6.query:1
	Lookup keys: -383, 315, blocks.2.sa.heads.6.query, blocks.2.sa.heads.6.query:1, linear_74, linear_74:1, linear_74_291, linear_74_291:1
--------------------------------------------
Layer transpose_23_292, operation 294/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 5.9745,  5.1855,  4.6083,  4.5716,  4.1687,  5.4610,  4.9488,  4.4056],
        [ 0.6274, -0.1008,  2.2610,  2.2531,  1.8860, -1.8259, -2.5531,  0.1353],
        [ 2.7633,  3.3393,  1.8979,  2.1447,  2.4812,  4.3198,  2.9244,  2.9136],
        [ 2.2467,  1.2600,  2.6661,  2.9432,  2.1003,  1.3858,  0.4736,  1.4909],
        [-1.1893, -2.2861, -1.0678, -0.9899, -1.0751, -4.2904, -2.5426, -1.3678],
        [-0.9026,  0.0187, -1.5404, -1.6726, -1.3640,  2.5466,  1.9423, -0.8174],
        [ 5.5145,  5.7904,  6.1847,  6.2622,  5.3441,  5.3339,  4.0583,  5.3825],
        [-1.2523, -1.6195, -1.2928, -1.1437, -0.7529, -1.4918, -0.9236, -1.6369]])...
	Related Layers:
		- parent layers: linear_73_290
		- child layers: matmul_45_293
		- shares parents with no other layers
		- shares children with layers: linear_74_291
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.2.sa.heads.6:1
	Time elapsed:  5.960E-05s
	Output of modules: none
	Lookup keys: -382, 316, transpose_23, transpose_23:1, transpose_23_292, transpose_23_292:1
--------------------------------------------
Layer matmul_45_293, operation 295/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -84.3126,  -79.8827,  -88.9632,  -85.5685,  -90.0767,  -55.2148,
          -19.0964,  -76.9233],
        [-280.6788, -276.0692, -272.9118, -261.6396, -270.3772, -235.2902,
         -142.2599, -260.6384],
        [-106.2857,  -92.6742, -111.2807, -105.8236, -109.7244,  -55.0143,
          -12.8721,  -95.2949],
        [ -76.2253,  -65.4525,  -84.7712,  -81.3044,  -86.4766,  -30.4116,
            8.5931,  -71.0481],
        [ -96.2839,  -78.0172, -110.8355, -107.2378, -110.6993,  -22.8587,
           14.3175,  -87.6163],
        [-194.7672, -230.2208, -153.7207, -144.5527, -147.1741, -290.9759,
         -230.7403, -171.6991],
        [-293.7691, -323.1816, -250.5446, -238.2212, -240.6976, -364.5083,
         -278.4770, -264.0600],
        [-176.7141, -166.8348, -176.4601, -170.0902, -171.8644, -130.2130,
          -72.4892, -157.6417]])...
	Related Layers:
		- parent layers: linear_74_291, transpose_23_292
		- child layers: mul_23_294
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.6:1
	Time elapsed:  8.583E-05s
	Output of modules: none
	Lookup keys: -381, 317, matmul_45, matmul_45:1, matmul_45_293, matmul_45_293:1
--------------------------------------------
Layer mul_23_294, operation 296/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -4.3026,  -4.0765,  -4.5399,  -4.3667,  -4.5967,  -2.8177,  -0.9745,
          -3.9255],
        [-14.3233, -14.0881, -13.9270, -13.3517, -13.7976, -12.0071,  -7.2597,
         -13.3006],
        [ -5.4239,  -4.7293,  -5.6788,  -5.4003,  -5.5994,  -2.8074,  -0.6569,
          -4.8630],
        [ -3.8899,  -3.3401,  -4.3260,  -4.1490,  -4.4130,  -1.5519,   0.4385,
          -3.6257],
        [ -4.9135,  -3.9813,  -5.6561,  -5.4725,  -5.6491,  -1.1665,   0.7306,
          -4.4711],
        [ -9.9392, -11.7484,  -7.8445,  -7.3767,  -7.5104, -14.8488, -11.7749,
          -8.7620],
        [-14.9913, -16.4923, -12.7856, -12.1567, -12.2830, -18.6012, -14.2110,
         -13.4753],
        [ -9.0179,  -8.5138,  -9.0049,  -8.6799,  -8.7704,  -6.6449,  -3.6992,
          -8.0446]])...
	Related Layers:
		- parent layers: matmul_45_293
		- child layers: maskedfill_23_297
		- shares parents with no other layers
		- shares children with layers: eq_23_296
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.2.sa.heads.6:1
	Time elapsed:  7.176E-05s
	Output of modules: none
	Lookup keys: -380, 318, mul_23, mul_23:1, mul_23_294, mul_23_294:1
--------------------------------------------
Layer getitem_23_295, operation 297/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_23
		- child layers: eq_23_296
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.561E-05s
	Output of modules: none
	Lookup keys: -378, 320, getitem_23, getitem_23:1, getitem_23_295, getitem_23_295:1
--------------------------------------------
Layer eq_23_296, operation 298/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_23_295
		- child layers: maskedfill_23_297
		- shares parents with no other layers
		- shares children with layers: mul_23_294
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.178E-05s
	Output of modules: none
	Lookup keys: -377, 321, eq_23, eq_23:1, eq_23_296, eq_23_296:1
--------------------------------------------
Layer maskedfill_23_297, operation 299/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -4.3026,     -inf,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [-14.3233, -14.0881,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -5.4239,  -4.7293,  -5.6788,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -3.8899,  -3.3401,  -4.3260,  -4.1490,     -inf,     -inf,     -inf,
             -inf],
        [ -4.9135,  -3.9813,  -5.6561,  -5.4725,  -5.6491,     -inf,     -inf,
             -inf],
        [ -9.9392, -11.7484,  -7.8445,  -7.3767,  -7.5104, -14.8488,     -inf,
             -inf],
        [-14.9913, -16.4923, -12.7856, -12.1567, -12.2830, -18.6012, -14.2110,
             -inf],
        [ -9.0179,  -8.5138,  -9.0049,  -8.6799,  -8.7704,  -6.6449,  -3.6992,
          -8.0446]])...
	Related Layers:
		- parent layers: mul_23_294, eq_23_296
		- child layers: softmax_23_298
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.2.sa.heads.6:1
	Time elapsed:  9.227E-05s
	Output of modules: none
	Lookup keys: -376, 322, maskedfill_23, maskedfill_23:1, maskedfill_23_297, maskedfill_23_297:1
--------------------------------------------
Layer softmax_23_298, operation 300/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [4.4146e-01, 5.5854e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.6470e-01, 5.3017e-01, 2.0514e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.4090e-01, 4.1744e-01, 1.5576e-01, 1.8590e-01, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.9736e-01, 5.0130e-01, 9.3920e-02, 1.1285e-01, 9.4575e-02, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.9756e-02, 4.8735e-03, 2.4170e-01, 3.8588e-01, 3.3757e-01, 2.1946e-04,
         0.0000e+00, 0.0000e+00],
        [2.2453e-02, 5.0051e-03, 2.0381e-01, 3.8225e-01, 3.3687e-01, 6.0745e-04,
         4.8998e-02, 0.0000e+00],
        [4.4673e-03, 7.3960e-03, 4.5256e-03, 6.2640e-03, 5.7218e-03, 4.7932e-02,
         9.1187e-01, 1.1823e-02]])...
	Related Layers:
		- parent layers: maskedfill_23_297
		- child layers: dropout_27_299
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.2.sa.heads.6:1
	Time elapsed:  7.176E-05s
	Output of modules: none
	Lookup keys: -375, 323, softmax_23, softmax_23:1, softmax_23_298, softmax_23_298:1
--------------------------------------------
Layer dropout_27_299, operation 301/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [4.4146e-01, 5.5854e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.6470e-01, 5.3017e-01, 2.0514e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.4090e-01, 4.1744e-01, 1.5576e-01, 1.8590e-01, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.9736e-01, 5.0130e-01, 9.3920e-02, 1.1285e-01, 9.4575e-02, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.9756e-02, 4.8735e-03, 2.4170e-01, 3.8588e-01, 3.3757e-01, 2.1946e-04,
         0.0000e+00, 0.0000e+00],
        [2.2453e-02, 5.0051e-03, 2.0381e-01, 3.8225e-01, 3.3687e-01, 6.0745e-04,
         4.8998e-02, 0.0000e+00],
        [4.4673e-03, 7.3960e-03, 4.5256e-03, 6.2640e-03, 5.7218e-03, 4.7932e-02,
         9.1187e-01, 1.1823e-02]])...
	Related Layers:
		- parent layers: softmax_23_298
		- child layers: matmul_46_301
		- shares parents with no other layers
		- shares children with layers: linear_75_300
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.2.sa.heads.6.dropout:1
	Time elapsed:  5.412E-05s
	Output of modules: blocks.2.sa.heads.6.dropout
	Output of bottom-level module: blocks.2.sa.heads.6.dropout:1
	Lookup keys: -374, 324, blocks.2.sa.heads.6.dropout, blocks.2.sa.heads.6.dropout:1, dropout_27, dropout_27:1, dropout_27_299, dropout_27_299:1
--------------------------------------------
Layer linear_75_300, operation 302/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.4978, -0.0178, -0.7275,  0.8052,  0.3869, -0.5591, -0.0522, -0.0934],
        [-0.5727,  0.1358,  0.2091,  0.9351,  0.5097, -0.8954, -0.0426, -0.4121],
        [-1.0507, -0.1870,  0.1674,  0.6322,  0.3458, -0.6441, -0.2356, -0.6720],
        [-0.7944,  0.1374,  0.1896,  0.4821,  0.3171, -0.5005, -0.1139, -0.5271],
        [-0.7141,  0.3425,  0.2820,  1.0271, -0.0058, -0.8266, -0.1499, -0.2735],
        [-0.8269,  0.1936, -0.2858,  0.1232, -1.0276,  0.5084,  0.1373,  0.1159],
        [-0.6348,  0.2617, -0.9803,  0.2291,  0.5264, -0.0353, -0.0395,  0.0445],
        [-0.7795,  0.0225, -0.8276,  1.0231,  0.2474,  0.1193,  0.2805, -0.4259]])...
	Related Layers:
		- parent layers: layernorm_5_217
		- child layers: matmul_46_301
		- shares parents with layers: linear_55_218, linear_56_219, linear_57_228, linear_58_230, linear_59_231, linear_60_240, linear_61_242, linear_62_243, linear_63_252, linear_64_254, linear_65_255, linear_66_264, linear_67_266, linear_68_267, linear_69_276, linear_70_278, linear_71_279, linear_72_288, linear_73_290, linear_74_291, linear_76_302, linear_77_303, linear_78_312
		- shares children with layers: dropout_27_299
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.6.value:1
	Time elapsed:  1.187E-04s
	Output of modules: blocks.2.sa.heads.6.value
	Output of bottom-level module: blocks.2.sa.heads.6.value:1
	Lookup keys: -373, 325, blocks.2.sa.heads.6.value, blocks.2.sa.heads.6.value:1, linear_75, linear_75:1, linear_75_300, linear_75_300:1
--------------------------------------------
Layer matmul_46_301, operation 303/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.4978, -0.0178, -0.7275,  0.8052,  0.3869, -0.5591, -0.0522, -0.0934],
        [-0.5396,  0.0680, -0.2044,  0.8778,  0.4555, -0.7469, -0.0468, -0.2714],
        [-0.6509,  0.0289, -0.0474,  0.8386,  0.4436, -0.7548, -0.0847, -0.3810],
        [-0.6703,  0.0488, -0.0267,  0.7724,  0.4188, -0.7018, -0.0882, -0.3972],
        [-0.6412,  0.0949,  0.0250,  0.8386,  0.3996, -0.7543, -0.0808, -0.3735],
        [-0.8194,  0.1236,  0.1882,  0.7141,  0.2178, -0.6487, -0.1532, -0.4629],
        [-0.8040,  0.1430,  0.1381,  0.6932,  0.2261, -0.6195, -0.1453, -0.4325],
        [-0.6480,  0.2511, -0.9155,  0.2492,  0.4427, -0.0263, -0.0293,  0.0297]])...
	Related Layers:
		- parent layers: dropout_27_299, linear_75_300
		- child layers: cat_3_314
		- shares parents with no other layers
		- shares children with layers: matmul_34_229, matmul_36_241, matmul_38_253, matmul_40_265, matmul_42_277, matmul_44_289, matmul_48_313
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.6:1
	Time elapsed:  8.488E-05s
	Output of modules: blocks.2.sa.heads.6
	Lookup keys: -372, 326, blocks.2.sa.heads.6, blocks.2.sa.heads.6:1, matmul_46, matmul_46:1, matmul_46_301, matmul_46_301:1
--------------------------------------------
Layer linear_76_302, operation 304/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 2.2201, -4.8267,  4.2600, -4.0368,  4.9117,  3.5437, -0.9474,  0.3504],
        [ 3.6425, -4.7804,  3.4098, -3.8961,  5.2526,  4.2398, -0.6085,  1.6805],
        [ 2.3787, -3.2387,  3.0753, -3.2951,  5.6618,  3.1712, -2.9521, -0.0396],
        [ 2.1247, -2.5751,  2.9001, -2.5142,  4.7164,  2.6736, -2.4517, -0.2727],
        [ 0.9525, -3.3236,  2.2638, -2.3886,  3.1834,  2.1745, -2.2044,  0.7565],
        [ 3.4215, -3.7368,  2.5720, -2.2007,  3.7894,  3.5559, -0.5251, -0.7076],
        [ 2.3089, -3.9950,  2.7776, -1.5329,  5.2896,  2.7615,  0.5022, -0.8598],
        [ 2.6859, -4.1808,  3.7255, -3.2942,  4.8531,  3.7171, -1.7832,  0.0071]])...
	Related Layers:
		- parent layers: layernorm_5_217
		- child layers: transpose_24_304
		- shares parents with layers: linear_55_218, linear_56_219, linear_57_228, linear_58_230, linear_59_231, linear_60_240, linear_61_242, linear_62_243, linear_63_252, linear_64_254, linear_65_255, linear_66_264, linear_67_266, linear_68_267, linear_69_276, linear_70_278, linear_71_279, linear_72_288, linear_73_290, linear_74_291, linear_75_300, linear_77_303, linear_78_312
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.7.key:1
	Time elapsed:  1.128E-04s
	Output of modules: blocks.2.sa.heads.7.key
	Output of bottom-level module: blocks.2.sa.heads.7.key:1
	Lookup keys: -371, 327, blocks.2.sa.heads.7.key, blocks.2.sa.heads.7.key:1, linear_76, linear_76:1, linear_76_302, linear_76_302:1
--------------------------------------------
Layer linear_77_303, operation 305/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-1.4465,  1.3135, -0.7389, -0.7922, -0.7021,  0.3638,  0.4554,  0.9420],
        [-2.1957,  1.4006,  0.2929, -0.2869, -0.7017, -0.7120,  0.1229,  1.0813],
        [-1.7629,  1.2222,  0.5468, -1.2510,  0.1473, -0.0225,  1.5265, -0.3687],
        [-1.6697,  0.7228,  0.1339, -0.9925, -0.0373, -0.5120,  1.5863, -0.5995],
        [-2.1499,  0.7886,  0.3525, -0.4891, -0.1734, -0.1114,  1.5957, -0.4507],
        [-1.2322, -0.0306,  1.0685, -0.5986, -1.0609,  0.2760,  0.6910,  0.9293],
        [-2.7255,  1.4472,  1.6308,  0.0820, -1.2182, -0.9959,  0.1670, -0.5952],
        [-2.6067,  1.2554,  0.1961,  0.6369, -0.4204, -0.7426,  0.9300, -0.4544]])...
	Related Layers:
		- parent layers: layernorm_5_217
		- child layers: matmul_47_305
		- shares parents with layers: linear_55_218, linear_56_219, linear_57_228, linear_58_230, linear_59_231, linear_60_240, linear_61_242, linear_62_243, linear_63_252, linear_64_254, linear_65_255, linear_66_264, linear_67_266, linear_68_267, linear_69_276, linear_70_278, linear_71_279, linear_72_288, linear_73_290, linear_74_291, linear_75_300, linear_76_302, linear_78_312
		- shares children with layers: transpose_24_304
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.7.query:1
	Time elapsed:  1.111E-04s
	Output of modules: blocks.2.sa.heads.7.query
	Output of bottom-level module: blocks.2.sa.heads.7.query:1
	Lookup keys: -370, 328, blocks.2.sa.heads.7.query, blocks.2.sa.heads.7.query:1, linear_77, linear_77:1, linear_77_303, linear_77_303:1
--------------------------------------------
Layer transpose_24_304, operation 306/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 2.2201,  3.6425,  2.3787,  2.1247,  0.9525,  3.4215,  2.3089,  2.6859],
        [-4.8267, -4.7804, -3.2387, -2.5751, -3.3236, -3.7368, -3.9950, -4.1808],
        [ 4.2600,  3.4098,  3.0753,  2.9001,  2.2638,  2.5720,  2.7776,  3.7255],
        [-4.0368, -3.8961, -3.2951, -2.5142, -2.3886, -2.2007, -1.5329, -3.2942],
        [ 4.9117,  5.2526,  5.6618,  4.7164,  3.1834,  3.7894,  5.2896,  4.8531],
        [ 3.5437,  4.2398,  3.1712,  2.6736,  2.1745,  3.5559,  2.7615,  3.7171],
        [-0.9474, -0.6085, -2.9521, -2.4517, -2.2044, -0.5251,  0.5022, -1.7832],
        [ 0.3504,  1.6805, -0.0396, -0.2727,  0.7565, -0.7076, -0.8598,  0.0071]])...
	Related Layers:
		- parent layers: linear_76_302
		- child layers: matmul_47_305
		- shares parents with no other layers
		- shares children with layers: linear_77_303
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.2.sa.heads.7:1
	Time elapsed:  5.817E-05s
	Output of modules: none
	Lookup keys: -369, 329, transpose_24, transpose_24:1, transpose_24_304, transpose_24_304:1
--------------------------------------------
Layer matmul_47_305, operation 307/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -66.0440,  -55.3893,  -51.3422,  -42.4474,  -45.7638,  -44.4908,
          -38.6129,  -50.3031],
        [-118.8813, -125.9007,  -96.6637,  -80.8365,  -75.2608, -114.4162,
          -78.2926, -108.4327],
        [ -86.9851,  -88.8084,  -76.7748,  -64.0566,  -61.7068,  -77.1304,
          -51.1100,  -77.8069],
        [ -87.5145,  -90.9641,  -82.8326,  -70.2906,  -64.1033,  -77.7843,
          -52.4315,  -79.1282],
        [ -65.9854,  -70.7835,  -65.8258,  -54.9021,  -49.3359,  -60.4458,
          -41.8991,  -60.3652],
        [  60.7722,   62.2513,   33.3839,   32.7831,   41.5181,   55.4222,
           35.2853,   51.3623],
        [ -60.0146,  -80.6637,  -61.2003,  -48.0473,  -36.8647,  -67.3858,
          -42.9349,  -63.9284],
        [-112.7767, -125.5130, -100.0717,  -83.2892,  -75.6617, -110.0512,
          -74.4250, -106.7014]])...
	Related Layers:
		- parent layers: linear_77_303, transpose_24_304
		- child layers: mul_24_306
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.7:1
	Time elapsed:  8.655E-05s
	Output of modules: none
	Lookup keys: -368, 330, matmul_47, matmul_47:1, matmul_47_305, matmul_47_305:1
--------------------------------------------
Layer mul_24_306, operation 308/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-3.3703, -2.8266, -2.6200, -2.1661, -2.3354, -2.2704, -1.9705, -2.5670],
        [-6.0666, -6.4248, -4.9328, -4.1252, -3.8406, -5.8388, -3.9954, -5.5334],
        [-4.4389, -4.5320, -3.9179, -3.2689, -3.1490, -3.9360, -2.6082, -3.9706],
        [-4.4660, -4.6420, -4.2270, -3.5870, -3.2713, -3.9694, -2.6756, -4.0380],
        [-3.3673, -3.6122, -3.3592, -2.8017, -2.5177, -3.0846, -2.1382, -3.0805],
        [ 3.1013,  3.1767,  1.7036,  1.6730,  2.1187,  2.8283,  1.8006,  2.6211],
        [-3.0626, -4.1163, -3.1231, -2.4519, -1.8812, -3.4388, -2.1910, -3.2623],
        [-5.7551, -6.4051, -5.1068, -4.2503, -3.8611, -5.6160, -3.7980, -5.4451]])...
	Related Layers:
		- parent layers: matmul_47_305
		- child layers: maskedfill_24_309
		- shares parents with no other layers
		- shares children with layers: eq_24_308
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.2.sa.heads.7:1
	Time elapsed:  6.795E-05s
	Output of modules: none
	Lookup keys: -367, 331, mul_24, mul_24:1, mul_24_306, mul_24_306:1
--------------------------------------------
Layer getitem_24_307, operation 309/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_24
		- child layers: eq_24_308
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.489E-05s
	Output of modules: none
	Lookup keys: -365, 333, getitem_24, getitem_24:1, getitem_24_307, getitem_24_307:1
--------------------------------------------
Layer eq_24_308, operation 310/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_24_307
		- child layers: maskedfill_24_309
		- shares parents with no other layers
		- shares children with layers: mul_24_306
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.035E-05s
	Output of modules: none
	Lookup keys: -364, 334, eq_24, eq_24:1, eq_24_308, eq_24_308:1
--------------------------------------------
Layer maskedfill_24_309, operation 311/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-3.3703,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-6.0666, -6.4248,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-4.4389, -4.5320, -3.9179,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-4.4660, -4.6420, -4.2270, -3.5870,    -inf,    -inf,    -inf,    -inf],
        [-3.3673, -3.6122, -3.3592, -2.8017, -2.5177,    -inf,    -inf,    -inf],
        [ 3.1013,  3.1767,  1.7036,  1.6730,  2.1187,  2.8283,    -inf,    -inf],
        [-3.0626, -4.1163, -3.1231, -2.4519, -1.8812, -3.4388, -2.1910,    -inf],
        [-5.7551, -6.4051, -5.1068, -4.2503, -3.8611, -5.6160, -3.7980, -5.4451]])...
	Related Layers:
		- parent layers: mul_24_306, eq_24_308
		- child layers: softmax_24_310
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.2.sa.heads.7:1
	Time elapsed:  8.368E-05s
	Output of modules: none
	Lookup keys: -363, 335, maskedfill_24, maskedfill_24:1, maskedfill_24_309, maskedfill_24_309:1
--------------------------------------------
Layer softmax_24_310, operation 312/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5886, 0.4114, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2782, 0.2535, 0.4684, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1813, 0.1520, 0.2302, 0.4365, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1451, 0.1136, 0.1463, 0.2555, 0.3394, 0.0000, 0.0000, 0.0000],
        [0.2702, 0.2914, 0.0668, 0.0648, 0.1012, 0.2057, 0.0000, 0.0000],
        [0.0955, 0.0333, 0.0899, 0.1759, 0.3113, 0.0656, 0.2284, 0.0000],
        [0.0414, 0.0216, 0.0791, 0.1863, 0.2749, 0.0475, 0.2928, 0.0564]])...
	Related Layers:
		- parent layers: maskedfill_24_309
		- child layers: dropout_28_311
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.2.sa.heads.7:1
	Time elapsed:  7.224E-05s
	Output of modules: none
	Lookup keys: -362, 336, softmax_24, softmax_24:1, softmax_24_310, softmax_24_310:1
--------------------------------------------
Layer dropout_28_311, operation 313/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5886, 0.4114, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2782, 0.2535, 0.4684, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1813, 0.1520, 0.2302, 0.4365, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1451, 0.1136, 0.1463, 0.2555, 0.3394, 0.0000, 0.0000, 0.0000],
        [0.2702, 0.2914, 0.0668, 0.0648, 0.1012, 0.2057, 0.0000, 0.0000],
        [0.0955, 0.0333, 0.0899, 0.1759, 0.3113, 0.0656, 0.2284, 0.0000],
        [0.0414, 0.0216, 0.0791, 0.1863, 0.2749, 0.0475, 0.2928, 0.0564]])...
	Related Layers:
		- parent layers: softmax_24_310
		- child layers: matmul_48_313
		- shares parents with no other layers
		- shares children with layers: linear_78_312
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.2.sa.heads.7.dropout:1
	Time elapsed:  5.364E-05s
	Output of modules: blocks.2.sa.heads.7.dropout
	Output of bottom-level module: blocks.2.sa.heads.7.dropout:1
	Lookup keys: -361, 337, blocks.2.sa.heads.7.dropout, blocks.2.sa.heads.7.dropout:1, dropout_28, dropout_28:1, dropout_28_311, dropout_28_311:1
--------------------------------------------
Layer linear_78_312, operation 314/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.2607, -0.2262,  0.4588,  0.3611, -0.0986,  0.5401,  0.9305, -1.1236],
        [-0.3285, -0.9391,  0.1001,  1.1061, -0.3005,  0.0440,  1.6756, -0.4206],
        [ 0.4837, -0.3834,  0.6856,  0.4158, -0.3725,  0.6357,  1.1212, -0.7630],
        [ 0.5335,  0.2094,  0.5825,  0.2262, -0.3826,  0.8712,  0.8646, -1.0306],
        [ 0.1580, -0.8706,  0.3678,  0.1826, -0.7454,  0.3442,  1.2553, -0.3774],
        [-0.6702, -0.2092,  0.2637,  0.0313,  0.1383, -0.2304,  1.9229, -0.0250],
        [-0.2121, -0.1735, -0.0984,  0.1417,  0.5308, -0.3279,  0.9893, -0.6972],
        [ 0.7288, -0.7188,  0.7176,  0.0750, -0.2419, -0.2175,  0.9638, -0.2763]])...
	Related Layers:
		- parent layers: layernorm_5_217
		- child layers: matmul_48_313
		- shares parents with layers: linear_55_218, linear_56_219, linear_57_228, linear_58_230, linear_59_231, linear_60_240, linear_61_242, linear_62_243, linear_63_252, linear_64_254, linear_65_255, linear_66_264, linear_67_266, linear_68_267, linear_69_276, linear_70_278, linear_71_279, linear_72_288, linear_73_290, linear_74_291, linear_75_300, linear_76_302, linear_77_303
		- shares children with layers: dropout_28_311
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.7.value:1
	Time elapsed:  1.206E-04s
	Output of modules: blocks.2.sa.heads.7.value
	Output of bottom-level module: blocks.2.sa.heads.7.value:1
	Lookup keys: -360, 338, blocks.2.sa.heads.7.value, blocks.2.sa.heads.7.value:1, linear_78, linear_78:1, linear_78_312, linear_78_312:1
--------------------------------------------
Layer matmul_48_313, operation 315/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.2607, -0.2262,  0.4588,  0.3611, -0.0986,  0.5401,  0.9305, -1.1236],
        [ 0.0183, -0.5195,  0.3112,  0.6676, -0.1816,  0.3360,  1.2370, -0.8344],
        [ 0.2158, -0.4805,  0.4741,  0.5755, -0.2780,  0.4591,  1.2086, -0.7765],
        [ 0.3416, -0.1806,  0.5105,  0.4280, -0.3163,  0.6312,  1.0589, -0.8931],
        [ 0.2612, -0.4376,  0.4519,  0.3587, -0.4537,  0.5158,  1.1365, -0.7139],
        [-0.0803, -0.4779,  0.3281,  0.4872, -0.2108,  0.2451,  1.3930, -0.5872],
        [ 0.1081, -0.3749,  0.3206,  0.2398, -0.2220,  0.2807,  1.1405, -0.6497],
        [ 0.1319, -0.3616,  0.3092,  0.2112, -0.1679,  0.2112,  1.1050, -0.6326]])...
	Related Layers:
		- parent layers: dropout_28_311, linear_78_312
		- child layers: cat_3_314
		- shares parents with no other layers
		- shares children with layers: matmul_34_229, matmul_36_241, matmul_38_253, matmul_40_265, matmul_42_277, matmul_44_289, matmul_46_301
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.2.sa.heads.7:1
	Time elapsed:  8.345E-05s
	Output of modules: blocks.2.sa.heads.7
	Lookup keys: -359, 339, blocks.2.sa.heads.7, blocks.2.sa.heads.7:1, matmul_48, matmul_48:1, matmul_48_313, matmul_48_313:1
--------------------------------------------
Layer cat_3_314, operation 316/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-0.6453, -0.5102, -0.7570, -0.2688, -0.4013, -0.0191,  1.2619, -1.0336],
        [-0.7092, -0.1316,  0.0447,  0.6004, -0.5986, -0.2627,  1.2516,  0.1108],
        [-0.7134, -0.2169, -0.0414,  0.2959, -0.4634, -0.0841,  1.2413, -0.3643],
        [-0.7043, -0.2652, -0.0626,  0.1750, -0.3791, -0.0126,  1.2233, -0.5792],
        [-0.6440, -0.3000,  0.0293,  0.1098, -0.3049,  0.0694,  1.1792, -0.6443],
        [-1.9494,  0.7466,  1.0353, -0.0067, -0.2886, -1.7729,  1.3449,  0.7185],
        [-1.8444,  0.6167,  0.9632, -0.0929, -0.1574, -1.5792,  1.1939,  0.5705],
        [-1.4169,  0.3025,  0.6355, -0.1005, -0.1425, -0.9283,  1.1691,  0.0946]])...
	Related Layers:
		- parent layers: matmul_34_229, matmul_36_241, matmul_38_253, matmul_40_265, matmul_42_277, matmul_44_289, matmul_46_301, matmul_48_313
		- child layers: linear_79_315
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: cat (grad_fn: CatBackward0) 
	Computed inside module: blocks.2.sa:1
	Time elapsed:  8.702E-05s
	Output of modules: none
	Lookup keys: -358, 340, cat_3, cat_3:1, cat_3_314, cat_3_314:1
--------------------------------------------
Layer linear_79_315, operation 317/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-0.3913,  0.1735, -1.4020, -0.0953,  0.8444, -0.1100, -0.4204, -0.5759],
        [-0.0058,  0.8576, -1.6833,  0.0182,  0.4733,  0.2932, -0.3610, -1.0805],
        [-0.2262,  0.5015, -1.5830, -0.0610,  0.5822, -0.0116, -0.3188, -0.9223],
        [-0.3931,  0.3609, -1.4341, -0.1464,  0.6493, -0.1745, -0.3887, -0.9126],
        [-0.5491,  0.1805, -1.2674, -0.2648,  0.7351, -0.3311, -0.4373, -1.0062],
        [-0.1969,  0.4302, -0.9158, -0.0847,  0.6429, -0.3778, -0.0802, -0.9369],
        [-0.2503,  0.6497, -0.8981, -0.0528,  0.6273, -0.5982, -0.2711, -0.9768],
        [-0.2334,  0.6110, -1.0059, -0.1582,  0.6211, -0.6772, -0.0904, -0.8790]])...
	Related Layers:
		- parent layers: cat_3_314
		- child layers: dropout_29_316
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (384, 384), (384,); 147840 params total (577.7 KB)
	Function: linear (grad_fn: ViewBackward0) 
	Computed inside module: blocks.2.sa.proj:1
	Time elapsed:  7.446E-04s
	Output of modules: blocks.2.sa.proj
	Output of bottom-level module: blocks.2.sa.proj:1
	Lookup keys: -357, 341, blocks.2.sa.proj, blocks.2.sa.proj:1, linear_79, linear_79:1, linear_79_315, linear_79_315:1
--------------------------------------------
Layer dropout_29_316, operation 318/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-0.3913,  0.1735, -1.4020, -0.0953,  0.8444, -0.1100, -0.4204, -0.5759],
        [-0.0058,  0.8576, -1.6833,  0.0182,  0.4733,  0.2932, -0.3610, -1.0805],
        [-0.2262,  0.5015, -1.5830, -0.0610,  0.5822, -0.0116, -0.3188, -0.9223],
        [-0.3931,  0.3609, -1.4341, -0.1464,  0.6493, -0.1745, -0.3887, -0.9126],
        [-0.5491,  0.1805, -1.2674, -0.2648,  0.7351, -0.3311, -0.4373, -1.0062],
        [-0.1969,  0.4302, -0.9158, -0.0847,  0.6429, -0.3778, -0.0802, -0.9369],
        [-0.2503,  0.6497, -0.8981, -0.0528,  0.6273, -0.5982, -0.2711, -0.9768],
        [-0.2334,  0.6110, -1.0059, -0.1582,  0.6211, -0.6772, -0.0904, -0.8790]])...
	Related Layers:
		- parent layers: linear_79_315
		- child layers: add_4_317:1
		- shares parents with no other layers
		- shares children with layers: add_3_211:2
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: ViewBackward0) 
	Computed inside module: blocks.2.sa.dropout:1
	Time elapsed:  7.868E-05s
	Output of modules: blocks.2.sa.dropout, blocks.2.sa
	Output of bottom-level module: blocks.2.sa.dropout:1
	Lookup keys: -356, 342, blocks.2.sa, blocks.2.sa.dropout, blocks.2.sa.dropout:1, blocks.2.sa:1, dropout_29, dropout_29:1, dropout_29_316, dropout_29_316:1
--------------------------------------------
Layer add_4_317 (pass 1/2), operation 319/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-5.8145, -3.7320,  1.1228, -4.5672,  0.5872,  6.5727, -0.6334,  5.8162],
        [-2.2798, -5.1911,  3.1230, -2.9398, -0.1068,  4.9462, -1.0465,  3.3858],
        [-4.2213, -6.4985,  2.5651, -4.1888, -2.7013,  1.5896, -1.2043,  1.7042],
        [-4.0847, -6.6330,  5.0537, -5.7627, -1.2653,  1.2867, -0.6824,  3.6460],
        [-2.6340, -5.4802,  3.3400, -5.9851,  3.6248,  0.6889, -0.3913,  4.8583],
        [ 0.3269, -1.4119, -0.0481, -2.7719,  1.0581,  6.1907, -1.8545,  5.0178],
        [-3.0457, -0.8505,  2.9132, -3.5393,  3.5536, 10.2541, -0.8181,  0.4409],
        [-6.4938, -2.4114,  4.0171, -5.3555,  1.9415,  1.0524, -5.2998,  2.3392]])...
	Related Layers:
		- parent layers: add_3_211:2, dropout_29_316
		- child layers: layernorm_6_318, add_4_317:2
		- shares parents with layers: layernorm_5_217
		- shares children with layers: dropout_30_322
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __add__ (grad_fn: AddBackward0) 
	Computed inside module: blocks.2:1
	Time elapsed:  7.701E-05s
	Output of modules: none
	Lookup keys: -355, 343, add_4:1, add_4_317:1
--------------------------------------------
Layer layernorm_6_318, operation 320/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-1.5865, -1.0559,  0.2195, -1.2684,  0.0913,  1.6889, -0.2509,  1.4673],
        [-0.6814, -1.5469,  0.8778, -0.8816, -0.0479,  1.4486, -0.3434,  0.9722],
        [-1.2938, -2.0132,  0.7561, -1.2915, -0.8412,  0.4839, -0.4001,  0.5088],
        [-1.2516, -2.0424,  1.4696, -1.7588, -0.4114,  0.3669, -0.2533,  1.0692],
        [-0.7834, -1.6616,  1.0037, -1.7958,  1.1172,  0.2263, -0.1250,  1.4829],
        [ 0.0662, -0.3746, -0.0340, -0.7252,  0.2589,  1.5840, -0.5070,  1.2641],
        [-0.7176, -0.1982,  0.6831, -0.8431,  0.8581,  2.4786, -0.2083,  0.1089],
        [-2.1655, -0.8220,  1.3103, -1.7942,  0.6439,  0.3495, -1.8186,  0.7712]])...
	Related Layers:
		- parent layers: add_4_317:1
		- child layers: linear_80_319
		- shares parents with layers: add_4_317:2
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (384,), (384,); 768 params total (3.2 KB)
	Function: layer_norm (grad_fn: NativeLayerNormBackward0) 
	Computed inside module: blocks.2.ln2:1
	Time elapsed:  2.425E-04s
	Output of modules: blocks.2.ln2
	Output of bottom-level module: blocks.2.ln2:1
	Lookup keys: -354, 344, blocks.2.ln2, blocks.2.ln2:1, layernorm_6, layernorm_6:1, layernorm_6_318, layernorm_6_318:1
--------------------------------------------
Layer linear_80_319, operation 321/648:
	Output tensor: shape=1x11x1536(67584 bytes), dype=torch.float32, Tensor size=67728 bytes, Tensor storage size=67648
		tensor([[ 0.2558, -0.7529,  0.0745,  0.4550, -0.5113, -0.0109, -0.0060, -0.3250],
        [ 1.3633,  0.0862,  0.6138, -1.0871, -0.7395,  0.4519, -0.0622, -0.0375],
        [ 0.6176, -1.1186, -0.3196,  0.1230,  0.1233,  0.6445,  0.6283,  0.2729],
        [ 1.0708, -0.8905, -0.3853,  0.4316,  0.0946,  0.5968,  0.8574, -0.0139],
        [ 0.1337, -0.6826, -0.7321,  0.2576, -0.8005, -0.0705,  0.0916, -0.9409],
        [-0.7569, -0.8157,  0.9272, -1.1048, -0.5644,  1.1300, -0.0243,  0.3527],
        [-0.5469, -0.1035,  1.2313, -0.4580, -0.4730,  0.6912, -0.3834,  0.2551],
        [-0.5796, -0.4199, -0.7429, -1.4277, -1.2041,  0.7046, -0.1999,  0.2069]])...
	Related Layers:
		- parent layers: layernorm_6_318
		- child layers: relu_3_320
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (1536, 384), (1536,); 591360 params total (2.3 MB)
	Function: linear (grad_fn: ViewBackward0) 
	Computed inside module: blocks.2.ffwd.net.0:1
	Time elapsed:  6.397E-04s
	Output of modules: blocks.2.ffwd.net.0
	Output of bottom-level module: blocks.2.ffwd.net.0:1
	Lookup keys: -353, 345, blocks.2.ffwd.net.0, blocks.2.ffwd.net.0:1, linear_80, linear_80:1, linear_80_319, linear_80_319:1
--------------------------------------------
Layer relu_3_320, operation 322/648:
	Output tensor: shape=1x11x1536(67584 bytes), dype=torch.float32, Tensor size=67728 bytes, Tensor storage size=67648
		tensor([[0.2558, 0.0000, 0.0745, 0.4550, 0.0000, 0.0000, 0.0000, 0.0000],
        [1.3633, 0.0862, 0.6138, 0.0000, 0.0000, 0.4519, 0.0000, 0.0000],
        [0.6176, 0.0000, 0.0000, 0.1230, 0.1233, 0.6445, 0.6283, 0.2729],
        [1.0708, 0.0000, 0.0000, 0.4316, 0.0946, 0.5968, 0.8574, 0.0000],
        [0.1337, 0.0000, 0.0000, 0.2576, 0.0000, 0.0000, 0.0916, 0.0000],
        [0.0000, 0.0000, 0.9272, 0.0000, 0.0000, 1.1300, 0.0000, 0.3527],
        [0.0000, 0.0000, 1.2313, 0.0000, 0.0000, 0.6912, 0.0000, 0.2551],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7046, 0.0000, 0.2069]])...
	Related Layers:
		- parent layers: linear_80_319
		- child layers: linear_81_321
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: relu (grad_fn: ReluBackward0) 
	Computed inside module: blocks.2.ffwd.net.1:1
	Time elapsed:  9.727E-05s
	Output of modules: blocks.2.ffwd.net.1
	Output of bottom-level module: blocks.2.ffwd.net.1:1
	Lookup keys: -352, 346, blocks.2.ffwd.net.1, blocks.2.ffwd.net.1:1, relu_3, relu_3:1, relu_3_320, relu_3_320:1
--------------------------------------------
Layer linear_81_321, operation 323/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-1.1972e+00, -1.5580e-03,  1.2172e+00,  1.1293e+00,  1.0996e+00,
         -6.4651e-01,  6.3200e-01,  8.9388e-01],
        [-1.1837e+00, -1.8769e-01,  1.7446e+00,  1.1543e+00,  1.4838e+00,
         -5.0568e-01,  1.5274e+00,  1.5323e+00],
        [-1.1068e+00, -1.4995e-01,  1.9382e+00,  1.2568e+00,  5.5143e-01,
         -1.3631e+00,  7.2257e-01,  1.0577e+00],
        [-1.3317e+00, -1.9058e-01,  2.0535e+00,  1.5116e+00,  6.6469e-01,
         -1.3989e+00,  4.8716e-01,  1.0370e+00],
        [-1.3297e+00, -1.8110e-01,  1.5826e+00,  1.4222e+00,  1.2635e+00,
         -1.3445e+00,  5.3710e-01,  9.7234e-01],
        [-6.5830e-01,  4.4104e-01,  2.6744e+00,  1.3971e+00,  8.4541e-01,
          5.6741e-01,  1.6339e+00,  1.6067e+00],
        [-6.8929e-01,  4.8141e-01,  1.7226e+00,  1.1487e+00,  3.8355e-01,
          2.0628e-01,  1.1432e+00,  4.0854e-01],
        [-1.4492e+00, -3.7861e-01,  1.8646e+00,  4.3137e-01,  1.0941e+00,
         -1.1530e+00,  5.2668e-01,  1.1718e+00]])...
	Related Layers:
		- parent layers: relu_3_320
		- child layers: dropout_30_322
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (384, 1536), (384,); 590208 params total (2.3 MB)
	Function: linear (grad_fn: ViewBackward0) 
	Computed inside module: blocks.2.ffwd.net.2:1
	Time elapsed:  7.372E-04s
	Output of modules: blocks.2.ffwd.net.2
	Output of bottom-level module: blocks.2.ffwd.net.2:1
	Lookup keys: -351, 347, blocks.2.ffwd.net.2, blocks.2.ffwd.net.2:1, linear_81, linear_81:1, linear_81_321, linear_81_321:1
--------------------------------------------
Layer dropout_30_322, operation 324/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-1.1972e+00, -1.5580e-03,  1.2172e+00,  1.1293e+00,  1.0996e+00,
         -6.4651e-01,  6.3200e-01,  8.9388e-01],
        [-1.1837e+00, -1.8769e-01,  1.7446e+00,  1.1543e+00,  1.4838e+00,
         -5.0568e-01,  1.5274e+00,  1.5323e+00],
        [-1.1068e+00, -1.4995e-01,  1.9382e+00,  1.2568e+00,  5.5143e-01,
         -1.3631e+00,  7.2257e-01,  1.0577e+00],
        [-1.3317e+00, -1.9058e-01,  2.0535e+00,  1.5116e+00,  6.6469e-01,
         -1.3989e+00,  4.8716e-01,  1.0370e+00],
        [-1.3297e+00, -1.8110e-01,  1.5826e+00,  1.4222e+00,  1.2635e+00,
         -1.3445e+00,  5.3710e-01,  9.7234e-01],
        [-6.5830e-01,  4.4104e-01,  2.6744e+00,  1.3971e+00,  8.4541e-01,
          5.6741e-01,  1.6339e+00,  1.6067e+00],
        [-6.8929e-01,  4.8141e-01,  1.7226e+00,  1.1487e+00,  3.8355e-01,
          2.0628e-01,  1.1432e+00,  4.0854e-01],
        [-1.4492e+00, -3.7861e-01,  1.8646e+00,  4.3137e-01,  1.0941e+00,
         -1.1530e+00,  5.2668e-01,  1.1718e+00]])...
	Related Layers:
		- parent layers: linear_81_321
		- child layers: add_4_317:2
		- shares parents with no other layers
		- shares children with layers: add_4_317:1
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: ViewBackward0) 
	Computed inside module: blocks.2.ffwd.net.3:1
	Time elapsed:  6.652E-05s
	Output of modules: blocks.2.ffwd.net.3, blocks.2.ffwd.net, blocks.2.ffwd
	Output of bottom-level module: blocks.2.ffwd.net.3:1
	Lookup keys: -350, 348, blocks.2.ffwd, blocks.2.ffwd.net, blocks.2.ffwd.net.3, blocks.2.ffwd.net.3:1, blocks.2.ffwd.net:1, blocks.2.ffwd:1, dropout_30, dropout_30:1, dropout_30_322, dropout_30_322:1
--------------------------------------------
Layer add_4_317 (pass 2/2), operation 325/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-7.0117e+00, -3.7335e+00,  2.3400e+00, -3.4379e+00,  1.6868e+00,
          5.9262e+00, -1.3784e-03,  6.7101e+00],
        [-3.4635e+00, -5.3788e+00,  4.8675e+00, -1.7855e+00,  1.3770e+00,
          4.4406e+00,  4.8097e-01,  4.9181e+00],
        [-5.3282e+00, -6.6484e+00,  4.5033e+00, -2.9319e+00, -2.1498e+00,
          2.2650e-01, -4.8169e-01,  2.7619e+00],
        [-5.4165e+00, -6.8236e+00,  7.1072e+00, -4.2511e+00, -6.0060e-01,
         -1.1218e-01, -1.9523e-01,  4.6831e+00],
        [-3.9637e+00, -5.6613e+00,  4.9226e+00, -4.5630e+00,  4.8883e+00,
         -6.5556e-01,  1.4583e-01,  5.8307e+00],
        [-3.3143e-01, -9.7083e-01,  2.6264e+00, -1.3748e+00,  1.9035e+00,
          6.7581e+00, -2.2066e-01,  6.6245e+00],
        [-3.7350e+00, -3.6912e-01,  4.6357e+00, -2.3906e+00,  3.9372e+00,
          1.0460e+01,  3.2511e-01,  8.4940e-01],
        [-7.9431e+00, -2.7901e+00,  5.8817e+00, -4.9241e+00,  3.0356e+00,
         -1.0063e-01, -4.7731e+00,  3.5110e+00]])...
	Related Layers:
		- parent layers: add_4_317:1, dropout_30_322
		- child layers: layernorm_7_323, add_5_423:1
		- shares parents with layers: layernorm_6_318
		- shares children with layers: dropout_39_422
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __add__ (grad_fn: AddBackward0) 
	Computed inside module: blocks.2:1
	Time elapsed:  7.343E-05s
	Output of modules: blocks.2
	Lookup keys: -349, 349, add_4:2, add_4_317:2, blocks.2, blocks.2:1
--------------------------------------------
Layer layernorm_7_323, operation 326/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-1.8300, -1.0258,  0.5346, -0.9424,  0.3665,  1.4416, -0.0686,  1.6383],
        [-0.9126, -1.4501,  1.2710, -0.4916,  0.3493,  1.1527,  0.1069,  1.2784],
        [-1.4951, -1.9060,  1.2539, -0.8429, -0.6144,  0.0445, -0.1541,  0.7615],
        [-1.4916, -1.9164,  1.9186, -1.1924, -0.1887, -0.0629, -0.0852,  1.2501],
        [-1.0755, -1.5821,  1.3816, -1.2601,  1.3671, -0.1767,  0.0463,  1.6261],
        [-0.0688, -0.2128,  0.5187, -0.2995,  0.3708,  1.3478, -0.0638,  1.3200],
        [-0.7669, -0.0727,  0.9850, -0.5045,  0.8345,  2.2082,  0.0689,  0.1875],
        [-2.3735, -0.8563,  1.7590, -1.4880,  0.9015, -0.0439, -1.4415,  1.0430]])...
	Related Layers:
		- parent layers: add_4_317:2
		- child layers: linear_82_324, linear_83_325, linear_84_334, linear_85_336, linear_86_337, linear_87_346, linear_88_348, linear_89_349, linear_90_358, linear_91_360, linear_92_361, linear_93_370, linear_94_372, linear_95_373, linear_96_382, linear_97_384, linear_98_385, linear_99_394, linear_100_396, linear_101_397, linear_102_406, linear_103_408, linear_104_409, linear_105_418
		- shares parents with layers: add_5_423:1
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (384,), (384,); 768 params total (3.2 KB)
	Function: layer_norm (grad_fn: NativeLayerNormBackward0) 
	Computed inside module: blocks.3.ln1:1
	Time elapsed:  2.408E-04s
	Output of modules: blocks.3.ln1
	Output of bottom-level module: blocks.3.ln1:1
	Lookup keys: -348, 350, blocks.3.ln1, blocks.3.ln1:1, layernorm_7, layernorm_7:1, layernorm_7_323, layernorm_7_323:1
--------------------------------------------
Layer linear_82_324, operation 327/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.2638,  3.6328,  3.6076, -2.5535,  1.6227,  2.6277,  2.3178, -3.7745],
        [ 2.8755,  3.9751,  3.9711, -2.5633,  2.5126,  4.3967,  5.1077, -3.6204],
        [ 0.0316,  3.9655,  3.1226, -2.1776,  1.9427,  1.8418,  2.2028, -3.9065],
        [ 0.1446,  4.0310,  2.8330, -1.9511,  1.7076,  2.3444,  1.9775, -4.0480],
        [-1.1557,  2.4632,  1.9017, -2.3180,  2.1474,  0.1539, -0.3001, -2.9870],
        [ 5.2730,  3.4199,  4.4218, -0.7276,  1.0180,  5.0234,  7.0581, -3.3878],
        [ 4.2792,  3.5091,  3.5343, -0.1202,  0.9227,  5.0441,  6.0519, -3.6348],
        [ 1.1099,  3.6028,  2.1327, -0.8412,  1.7346,  2.7278,  3.0458, -2.3873]])...
	Related Layers:
		- parent layers: layernorm_7_323
		- child layers: transpose_25_326
		- shares parents with layers: linear_83_325, linear_84_334, linear_85_336, linear_86_337, linear_87_346, linear_88_348, linear_89_349, linear_90_358, linear_91_360, linear_92_361, linear_93_370, linear_94_372, linear_95_373, linear_96_382, linear_97_384, linear_98_385, linear_99_394, linear_100_396, linear_101_397, linear_102_406, linear_103_408, linear_104_409, linear_105_418
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.0.key:1
	Time elapsed:  1.159E-04s
	Output of modules: blocks.3.sa.heads.0.key
	Output of bottom-level module: blocks.3.sa.heads.0.key:1
	Lookup keys: -347, 351, blocks.3.sa.heads.0.key, blocks.3.sa.heads.0.key:1, linear_82, linear_82:1, linear_82_324, linear_82_324:1
--------------------------------------------
Layer linear_83_325, operation 328/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.1086, -0.9745,  0.8311,  0.6374, -0.1671,  1.0513,  1.2870, -0.0668],
        [-1.2877,  0.5540,  1.2396,  0.9422,  0.3477,  0.6736,  0.9613, -1.1620],
        [ 0.6019, -0.3502,  1.1107,  0.8899,  0.3783,  1.1929,  2.4308, -0.2820],
        [ 0.6875, -0.2589,  1.4155,  0.3973,  0.3918,  1.7313,  2.6466, -0.4181],
        [ 1.4721, -0.1552,  0.7722,  1.1793, -0.0309,  1.5081,  2.2231, -0.1114],
        [-3.0429, -1.4071, -0.6223,  1.0703, -0.0099, -2.3487, -1.4455,  1.3820],
        [-1.1653,  0.3725,  0.5206,  1.2252,  0.9305,  0.2238,  0.7368,  0.2846],
        [-0.2148,  0.4500,  0.1415,  1.0111,  0.1360,  1.2928,  1.9137, -0.5521]])...
	Related Layers:
		- parent layers: layernorm_7_323
		- child layers: matmul_49_327
		- shares parents with layers: linear_82_324, linear_84_334, linear_85_336, linear_86_337, linear_87_346, linear_88_348, linear_89_349, linear_90_358, linear_91_360, linear_92_361, linear_93_370, linear_94_372, linear_95_373, linear_96_382, linear_97_384, linear_98_385, linear_99_394, linear_100_396, linear_101_397, linear_102_406, linear_103_408, linear_104_409, linear_105_418
		- shares children with layers: transpose_25_326
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.0.query:1
	Time elapsed:  1.130E-04s
	Output of modules: blocks.3.sa.heads.0.query
	Output of bottom-level module: blocks.3.sa.heads.0.query:1
	Lookup keys: -346, 352, blocks.3.sa.heads.0.query, blocks.3.sa.heads.0.query:1, linear_83, linear_83:1, linear_83_325, linear_83_325:1
--------------------------------------------
Layer transpose_25_326, operation 329/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.2638,  2.8755,  0.0316,  0.1446, -1.1557,  5.2730,  4.2792,  1.1099],
        [ 3.6328,  3.9751,  3.9655,  4.0310,  2.4632,  3.4199,  3.5091,  3.6028],
        [ 3.6076,  3.9711,  3.1226,  2.8330,  1.9017,  4.4218,  3.5343,  2.1327],
        [-2.5535, -2.5633, -2.1776, -1.9511, -2.3180, -0.7276, -0.1202, -0.8412],
        [ 1.6227,  2.5126,  1.9427,  1.7076,  2.1474,  1.0180,  0.9227,  1.7346],
        [ 2.6277,  4.3967,  1.8418,  2.3444,  0.1539,  5.0234,  5.0441,  2.7278],
        [ 2.3178,  5.1077,  2.2028,  1.9775, -0.3001,  7.0581,  6.0519,  3.0458],
        [-3.7745, -3.6204, -3.9065, -4.0480, -2.9870, -3.3878, -3.6348, -2.3873]])...
	Related Layers:
		- parent layers: linear_82_324
		- child layers: matmul_49_327
		- shares parents with no other layers
		- shares children with layers: linear_83_325
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.3.sa.heads.0:1
	Time elapsed:  5.984E-05s
	Output of modules: none
	Lookup keys: -345, 353, transpose_25, transpose_25:1, transpose_25_326, transpose_25_326:1
--------------------------------------------
Layer matmul_49_327, operation 330/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[  10.7452,   87.5341,    3.5246,   -5.2211,  -42.5676,  139.2386,
          150.5781,   49.1528],
        [  36.4925,   68.6206,   46.0557,   41.2227,   15.4652,   78.2344,
           82.5404,   45.0951],
        [  35.2078,  111.2903,   27.9577,   17.7667,  -24.0473,  158.6889,
          168.0661,   65.7724],
        [  58.0175,  146.6058,   50.1341,   38.7258,  -12.0580,  191.3001,
          201.0738,   85.9102],
        [  33.4568,  127.5872,   22.7782,   11.1672,  -39.2152,  187.8977,
          199.3425,   74.2873],
        [ -80.5838, -159.6440,  -68.0496,  -59.8297,   -3.8471, -190.0545,
         -204.0114, -102.5432],
        [   8.0974,   26.3370,   14.3312,   10.5309,   -2.2752,   38.7994,
           38.0451,   15.9173],
        [  45.2539,  120.7851,   46.2681,   36.5695,   -8.5057,  149.8507,
          168.0151,   77.5452]])...
	Related Layers:
		- parent layers: linear_83_325, transpose_25_326
		- child layers: mul_25_328
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.0:1
	Time elapsed:  8.845E-05s
	Output of modules: none
	Lookup keys: -344, 354, matmul_49, matmul_49:1, matmul_49_327, matmul_49_327:1
--------------------------------------------
Layer mul_25_328, operation 331/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[  0.5483,   4.4670,   0.1799,  -0.2664,  -2.1723,   7.1055,   7.6842,
           2.5083],
        [  1.8622,   3.5018,   2.3503,   2.1036,   0.7892,   3.9924,   4.2121,
           2.3012],
        [  1.7967,   5.6793,   1.4267,   0.9067,  -1.2272,   8.0981,   8.5766,
           3.3564],
        [  2.9607,   7.4814,   2.5584,   1.9762,  -0.6153,   9.7622,  10.2610,
           4.3841],
        [  1.7073,   6.5109,   1.1624,   0.5699,  -2.0012,   9.5886,  10.1727,
           3.7910],
        [ -4.1123,  -8.1468,  -3.4726,  -3.0532,  -0.1963,  -9.6987, -10.4109,
          -5.2329],
        [  0.4132,   1.3440,   0.7313,   0.5374,  -0.1161,   1.9800,   1.9415,
           0.8123],
        [  2.3094,   6.1638,   2.3611,   1.8662,  -0.4341,   7.6470,   8.5740,
           3.9572]])...
	Related Layers:
		- parent layers: matmul_49_327
		- child layers: maskedfill_25_331
		- shares parents with no other layers
		- shares children with layers: eq_25_330
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.3.sa.heads.0:1
	Time elapsed:  6.914E-05s
	Output of modules: none
	Lookup keys: -343, 355, mul_25, mul_25:1, mul_25_328, mul_25_328:1
--------------------------------------------
Layer getitem_25_329, operation 332/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_25
		- child layers: eq_25_330
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.583E-05s
	Output of modules: none
	Lookup keys: -341, 357, getitem_25, getitem_25:1, getitem_25_329, getitem_25_329:1
--------------------------------------------
Layer eq_25_330, operation 333/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_25_329
		- child layers: maskedfill_25_331
		- shares parents with no other layers
		- shares children with layers: mul_25_328
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  7.820E-05s
	Output of modules: none
	Lookup keys: -340, 358, eq_25, eq_25:1, eq_25_330, eq_25_330:1
--------------------------------------------
Layer maskedfill_25_331, operation 334/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ 0.5483,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [ 1.8622,  3.5018,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [ 1.7967,  5.6793,  1.4267,    -inf,    -inf,    -inf,    -inf,    -inf],
        [ 2.9607,  7.4814,  2.5584,  1.9762,    -inf,    -inf,    -inf,    -inf],
        [ 1.7073,  6.5109,  1.1624,  0.5699, -2.0012,    -inf,    -inf,    -inf],
        [-4.1123, -8.1468, -3.4726, -3.0532, -0.1963, -9.6987,    -inf,    -inf],
        [ 0.4132,  1.3440,  0.7313,  0.5374, -0.1161,  1.9800,  1.9415,    -inf],
        [ 2.3094,  6.1638,  2.3611,  1.8662, -0.4341,  7.6470,  8.5740,  3.9572]])...
	Related Layers:
		- parent layers: mul_25_328, eq_25_330
		- child layers: softmax_25_332
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.3.sa.heads.0:1
	Time elapsed:  8.225E-05s
	Output of modules: none
	Lookup keys: -339, 359, maskedfill_25, maskedfill_25:1, maskedfill_25_331, maskedfill_25_331:1
--------------------------------------------
Layer softmax_25_332, operation 335/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.6253e-01, 8.3747e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.9905e-02, 9.6635e-01, 1.3749e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.0644e-02, 9.7826e-01, 7.1187e-03, 3.9771e-03, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [8.0729e-03, 9.8446e-01, 4.6813e-03, 2.5885e-03, 1.9790e-04, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.7858e-02, 3.1598e-04, 3.3855e-02, 5.1498e-02, 8.9641e-01, 6.6940e-05,
         0.0000e+00, 0.0000e+00],
        [6.2369e-02, 1.5820e-01, 8.5729e-02, 7.0616e-02, 3.6736e-02, 2.9882e-01,
         2.8753e-01, 0.0000e+00],
        [1.2677e-03, 5.9838e-02, 1.3350e-03, 8.1385e-04, 8.1577e-05, 2.6372e-01,
         6.6636e-01, 6.5867e-03]])...
	Related Layers:
		- parent layers: maskedfill_25_331
		- child layers: dropout_31_333
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.3.sa.heads.0:1
	Time elapsed:  6.294E-05s
	Output of modules: none
	Lookup keys: -338, 360, softmax_25, softmax_25:1, softmax_25_332, softmax_25_332:1
--------------------------------------------
Layer dropout_31_333, operation 336/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.6253e-01, 8.3747e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.9905e-02, 9.6635e-01, 1.3749e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.0644e-02, 9.7826e-01, 7.1187e-03, 3.9771e-03, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [8.0729e-03, 9.8446e-01, 4.6813e-03, 2.5885e-03, 1.9790e-04, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.7858e-02, 3.1598e-04, 3.3855e-02, 5.1498e-02, 8.9641e-01, 6.6940e-05,
         0.0000e+00, 0.0000e+00],
        [6.2369e-02, 1.5820e-01, 8.5729e-02, 7.0616e-02, 3.6736e-02, 2.9882e-01,
         2.8753e-01, 0.0000e+00],
        [1.2677e-03, 5.9838e-02, 1.3350e-03, 8.1385e-04, 8.1577e-05, 2.6372e-01,
         6.6636e-01, 6.5867e-03]])...
	Related Layers:
		- parent layers: softmax_25_332
		- child layers: matmul_50_335
		- shares parents with no other layers
		- shares children with layers: linear_84_334
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.3.sa.heads.0.dropout:1
	Time elapsed:  5.436E-05s
	Output of modules: blocks.3.sa.heads.0.dropout
	Output of bottom-level module: blocks.3.sa.heads.0.dropout:1
	Lookup keys: -337, 361, blocks.3.sa.heads.0.dropout, blocks.3.sa.heads.0.dropout:1, dropout_31, dropout_31:1, dropout_31_333, dropout_31_333:1
--------------------------------------------
Layer linear_84_334, operation 337/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.4185,  1.2749,  0.0019, -0.9902, -1.6180, -0.8272,  0.4441, -0.2893],
        [ 0.3636,  0.7210,  0.1892, -0.5324, -1.2550, -0.3995,  0.2782,  0.3413],
        [-0.5408,  0.4547, -0.1556, -0.9752, -1.2294, -0.1925,  0.0933, -0.0053],
        [-0.5178,  0.4999, -0.3480, -0.8318, -1.0885, -0.6796,  0.0689,  0.2656],
        [-0.6541,  0.4928, -0.1472, -0.9796, -1.6594, -0.3623,  0.4013,  0.2844],
        [ 0.5461,  0.7177,  0.3196, -1.3213, -0.3598, -0.4203, -0.3766, -0.3685],
        [ 1.0576,  0.4896,  0.6842, -0.3363, -0.7272, -0.6762, -0.2755, -0.1758],
        [-0.0980,  0.1606,  0.4127, -0.0511, -0.8750, -1.0304,  0.0627, -0.7114]])...
	Related Layers:
		- parent layers: layernorm_7_323
		- child layers: matmul_50_335
		- shares parents with layers: linear_82_324, linear_83_325, linear_85_336, linear_86_337, linear_87_346, linear_88_348, linear_89_349, linear_90_358, linear_91_360, linear_92_361, linear_93_370, linear_94_372, linear_95_373, linear_96_382, linear_97_384, linear_98_385, linear_99_394, linear_100_396, linear_101_397, linear_102_406, linear_103_408, linear_104_409, linear_105_418
		- shares children with layers: dropout_31_333
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.0.value:1
	Time elapsed:  1.228E-04s
	Output of modules: blocks.3.sa.heads.0.value
	Output of bottom-level module: blocks.3.sa.heads.0.value:1
	Lookup keys: -336, 362, blocks.3.sa.heads.0.value, blocks.3.sa.heads.0.value:1, linear_84, linear_84:1, linear_84_334, linear_84_334:1
--------------------------------------------
Layer matmul_50_335, operation 338/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.4185,  1.2749,  0.0019, -0.9902, -1.6180, -0.8272,  0.4441, -0.2893],
        [ 0.2365,  0.8111,  0.1587, -0.6068, -1.3140, -0.4690,  0.3051,  0.2388],
        [ 0.3356,  0.7284,  0.1807, -0.5476, -1.2619, -0.4052,  0.2789,  0.3240],
        [ 0.3453,  0.7242,  0.1826, -0.5416, -1.2580, -0.4037,  0.2778,  0.3318],
        [ 0.3506,  0.7237,  0.1846, -0.5390, -1.2575, -0.4027,  0.2781,  0.3344],
        [-0.6386,  0.5060, -0.1550, -0.9719, -1.6145, -0.3812,  0.3744,  0.2634],
        [ 0.3917,  0.6412,  0.2790, -0.8158, -0.8593, -0.5126, -0.0924, -0.0960],
        [ 0.8681,  0.5624,  0.5537, -0.6081, -0.6651, -0.5941, -0.2651, -0.1987]])...
	Related Layers:
		- parent layers: dropout_31_333, linear_84_334
		- child layers: cat_4_420
		- shares parents with no other layers
		- shares children with layers: matmul_52_347, matmul_54_359, matmul_56_371, matmul_58_383, matmul_60_395, matmul_62_407, matmul_64_419
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.0:1
	Time elapsed:  8.392E-05s
	Output of modules: blocks.3.sa.heads.0
	Lookup keys: -335, 363, blocks.3.sa.heads.0, blocks.3.sa.heads.0:1, matmul_50, matmul_50:1, matmul_50_335, matmul_50_335:1
--------------------------------------------
Layer linear_85_336, operation 339/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.7099e+00,  2.1500e+00, -1.7279e-03, -2.3774e+00,  2.6445e+00,
          3.3072e+00, -8.2173e-02,  1.8497e+00],
        [-1.4627e+00, -1.4245e+00, -8.8747e-01,  2.2062e-01, -1.5573e+00,
          1.3709e+00,  5.1222e-01, -2.6793e+00],
        [ 9.3614e-01,  1.2757e+00, -8.7018e-01, -1.2753e+00,  2.5000e+00,
          3.3469e+00,  2.1814e-01,  1.0206e+00],
        [ 1.1495e+00,  1.6223e+00, -8.7997e-01, -1.5957e+00,  2.6412e+00,
          3.0285e+00, -3.8728e-01,  8.4783e-01],
        [ 2.5251e+00,  3.4317e+00,  5.7140e-01, -3.2047e+00,  4.6615e+00,
          4.2519e+00,  1.5928e-01,  3.5067e+00],
        [-1.5754e+00, -4.7553e+00, -5.0992e+00,  2.5800e+00, -4.6272e+00,
         -1.7767e+00,  1.5102e-01, -3.5682e+00],
        [-2.5016e+00, -4.5859e+00, -4.1521e+00,  3.2911e+00, -5.4391e+00,
         -1.2862e+00,  4.9135e-01, -5.6634e+00],
        [ 1.3168e+00, -3.1277e-01, -1.5690e+00, -1.0543e-01,  3.9770e-01,
          1.9555e+00, -3.9659e-01, -8.7046e-01]])...
	Related Layers:
		- parent layers: layernorm_7_323
		- child layers: transpose_26_338
		- shares parents with layers: linear_82_324, linear_83_325, linear_84_334, linear_86_337, linear_87_346, linear_88_348, linear_89_349, linear_90_358, linear_91_360, linear_92_361, linear_93_370, linear_94_372, linear_95_373, linear_96_382, linear_97_384, linear_98_385, linear_99_394, linear_100_396, linear_101_397, linear_102_406, linear_103_408, linear_104_409, linear_105_418
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.1.key:1
	Time elapsed:  1.163E-04s
	Output of modules: blocks.3.sa.heads.1.key
	Output of bottom-level module: blocks.3.sa.heads.1.key:1
	Lookup keys: -334, 364, blocks.3.sa.heads.1.key, blocks.3.sa.heads.1.key:1, linear_85, linear_85:1, linear_85_336, linear_85_336:1
--------------------------------------------
Layer linear_86_337, operation 340/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-2.0176, -1.1491,  1.0967,  1.3793, -2.3568, -1.8403,  0.7163, -1.9154],
        [-3.9126, -3.3025, -1.1659,  3.9251, -4.2002, -2.6098,  0.6576, -3.1875],
        [-2.8675, -1.6979,  0.2986,  2.1023, -3.1343, -2.3281,  0.5593, -2.6039],
        [-3.0116, -1.5916,  0.8089,  1.9856, -3.1193, -2.1571,  0.4534, -2.5184],
        [-2.6983, -1.3426,  1.2037,  1.7599, -2.1379, -1.4958,  0.0859, -2.0678],
        [-0.3218, -1.4683, -2.6186,  0.9548, -1.5834, -3.5395,  0.5750, -1.1918],
        [-2.7300, -3.1905, -3.1187,  2.9617, -3.7413, -3.7903,  0.8673, -3.3505],
        [-3.6989, -3.6282, -2.6681,  4.1125, -4.8614, -3.9215,  0.7983, -3.4155]])...
	Related Layers:
		- parent layers: layernorm_7_323
		- child layers: matmul_51_339
		- shares parents with layers: linear_82_324, linear_83_325, linear_84_334, linear_85_336, linear_87_346, linear_88_348, linear_89_349, linear_90_358, linear_91_360, linear_92_361, linear_93_370, linear_94_372, linear_95_373, linear_96_382, linear_97_384, linear_98_385, linear_99_394, linear_100_396, linear_101_397, linear_102_406, linear_103_408, linear_104_409, linear_105_418
		- shares children with layers: transpose_26_338
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.1.query:1
	Time elapsed:  1.137E-04s
	Output of modules: blocks.3.sa.heads.1.query
	Output of bottom-level module: blocks.3.sa.heads.1.query:1
	Lookup keys: -333, 365, blocks.3.sa.heads.1.query, blocks.3.sa.heads.1.query:1, linear_86, linear_86:1, linear_86_337, linear_86_337:1
--------------------------------------------
Layer transpose_26_338, operation 341/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.7099e+00, -1.4627e+00,  9.3614e-01,  1.1495e+00,  2.5251e+00,
         -1.5754e+00, -2.5016e+00,  1.3168e+00],
        [ 2.1500e+00, -1.4245e+00,  1.2757e+00,  1.6223e+00,  3.4317e+00,
         -4.7553e+00, -4.5859e+00, -3.1277e-01],
        [-1.7279e-03, -8.8747e-01, -8.7018e-01, -8.7997e-01,  5.7140e-01,
         -5.0992e+00, -4.1521e+00, -1.5690e+00],
        [-2.3774e+00,  2.2062e-01, -1.2753e+00, -1.5957e+00, -3.2047e+00,
          2.5800e+00,  3.2911e+00, -1.0543e-01],
        [ 2.6445e+00, -1.5573e+00,  2.5000e+00,  2.6412e+00,  4.6615e+00,
         -4.6272e+00, -5.4391e+00,  3.9770e-01],
        [ 3.3072e+00,  1.3709e+00,  3.3469e+00,  3.0285e+00,  4.2519e+00,
         -1.7767e+00, -1.2862e+00,  1.9555e+00],
        [-8.2173e-02,  5.1222e-01,  2.1814e-01, -3.8728e-01,  1.5928e-01,
          1.5102e-01,  4.9135e-01, -3.9659e-01],
        [ 1.8497e+00, -2.6793e+00,  1.0206e+00,  8.4783e-01,  3.5067e+00,
         -3.5682e+00, -5.6634e+00, -8.7046e-01]])...
	Related Layers:
		- parent layers: linear_85_336
		- child layers: matmul_51_339
		- shares parents with no other layers
		- shares children with layers: linear_86_337
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.3.sa.heads.1:1
	Time elapsed:  5.913E-05s
	Output of modules: none
	Lookup keys: -332, 366, transpose_26, transpose_26:1, transpose_26_338, transpose_26_338:1
--------------------------------------------
Layer matmul_51_339, operation 342/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -88.5216,   13.3756,  -69.6314,  -76.3018, -121.5663,   59.6421,
           91.0917,  -37.9366],
        [-173.8184,   82.5278, -118.0527, -139.0226, -258.0057,  251.4099,
          318.8022,  -34.6131],
        [-143.8100,   29.0033, -111.1167, -123.1428, -199.3183,  122.0126,
          171.1229,  -57.1505],
        [-141.9146,   32.7739, -107.9438, -120.2062, -197.4790,  124.4105,
          175.1266,  -55.6169],
        [-117.0630,   13.2836,  -91.9674, -100.2369, -157.5054,   70.3127,
          110.1336,  -54.5613],
        [-111.3447,  -15.0522,  -99.7982, -108.3436, -142.3019,  104.1242,
          113.7159,  -48.8266],
        [-186.8714,   61.8970, -135.8524, -154.6198, -269.3614,  262.0947,
          317.6660,  -45.4278],
        [-220.5741,   55.2765, -164.4133, -184.5868, -309.3460,  259.3199,
          322.6925,  -67.7850]])...
	Related Layers:
		- parent layers: linear_86_337, transpose_26_338
		- child layers: mul_26_340
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.1:1
	Time elapsed:  8.893E-05s
	Output of modules: none
	Lookup keys: -331, 367, matmul_51, matmul_51:1, matmul_51_339, matmul_51_339:1
--------------------------------------------
Layer mul_26_340, operation 343/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -4.5174,   0.6826,  -3.5534,  -3.8938,  -6.2037,   3.0436,   4.6485,
          -1.9359],
        [ -8.8701,   4.2115,  -6.0244,  -7.0945, -13.1663,  12.8297,  16.2688,
          -1.7663],
        [ -7.3388,   1.4801,  -5.6704,  -6.2841, -10.1714,   6.2264,   8.7326,
          -2.9164],
        [ -7.2421,   1.6725,  -5.5085,  -6.1342, -10.0776,   6.3488,   8.9369,
          -2.8382],
        [ -5.9738,   0.6779,  -4.6932,  -5.1152,  -8.0377,   3.5881,   5.6202,
          -2.7843],
        [ -5.6820,  -0.7681,  -5.0928,  -5.5289,  -7.2618,   5.3136,   5.8030,
          -2.4917],
        [ -9.5362,   3.1587,  -6.9327,  -7.8904, -13.7458,  13.3750,  16.2108,
          -2.3182],
        [-11.2561,   2.8208,  -8.3902,  -9.4197, -15.7862,  13.2334,  16.4673,
          -3.4591]])...
	Related Layers:
		- parent layers: matmul_51_339
		- child layers: maskedfill_26_343
		- shares parents with no other layers
		- shares children with layers: eq_26_342
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.3.sa.heads.1:1
	Time elapsed:  6.890E-05s
	Output of modules: none
	Lookup keys: -330, 368, mul_26, mul_26:1, mul_26_340, mul_26_340:1
--------------------------------------------
Layer getitem_26_341, operation 344/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_26
		- child layers: eq_26_342
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.655E-05s
	Output of modules: none
	Lookup keys: -328, 370, getitem_26, getitem_26:1, getitem_26_341, getitem_26_341:1
--------------------------------------------
Layer eq_26_342, operation 345/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_26_341
		- child layers: maskedfill_26_343
		- shares parents with no other layers
		- shares children with layers: mul_26_340
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.416E-05s
	Output of modules: none
	Lookup keys: -327, 371, eq_26, eq_26:1, eq_26_342, eq_26_342:1
--------------------------------------------
Layer maskedfill_26_343, operation 346/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -4.5174,     -inf,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -8.8701,   4.2115,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -7.3388,   1.4801,  -5.6704,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -7.2421,   1.6725,  -5.5085,  -6.1342,     -inf,     -inf,     -inf,
             -inf],
        [ -5.9738,   0.6779,  -4.6932,  -5.1152,  -8.0377,     -inf,     -inf,
             -inf],
        [ -5.6820,  -0.7681,  -5.0928,  -5.5289,  -7.2618,   5.3136,     -inf,
             -inf],
        [ -9.5362,   3.1587,  -6.9327,  -7.8904, -13.7458,  13.3750,  16.2108,
             -inf],
        [-11.2561,   2.8208,  -8.3902,  -9.4197, -15.7862,  13.2334,  16.4673,
          -3.4591]])...
	Related Layers:
		- parent layers: mul_26_340, eq_26_342
		- child layers: softmax_26_344
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.3.sa.heads.1:1
	Time elapsed:  8.869E-05s
	Output of modules: none
	Lookup keys: -326, 372, maskedfill_26, maskedfill_26:1, maskedfill_26_343, maskedfill_26_343:1
--------------------------------------------
Layer softmax_26_344, operation 347/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.0832e-06, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.4778e-04, 9.9907e-01, 7.8377e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.3425e-04, 9.9870e-01, 7.5994e-04, 4.0645e-04, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.2801e-03, 9.9093e-01, 4.6070e-03, 3.0210e-03, 1.6253e-04, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.6736e-05, 2.2789e-03, 3.0168e-05, 1.9506e-05, 3.4479e-06, 9.9765e-01,
         0.0000e+00, 0.0000e+00],
        [6.2148e-12, 2.0266e-06, 8.3973e-11, 3.2226e-11, 9.2309e-14, 5.5417e-02,
         9.4458e-01, 0.0000e+00],
        [8.7715e-13, 1.1392e-06, 1.5408e-11, 5.5035e-12, 9.4550e-15, 3.7907e-02,
         9.6209e-01, 2.1343e-09]])...
	Related Layers:
		- parent layers: maskedfill_26_343
		- child layers: dropout_32_345
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.3.sa.heads.1:1
	Time elapsed:  6.962E-05s
	Output of modules: none
	Lookup keys: -325, 373, softmax_26, softmax_26:1, softmax_26_344, softmax_26_344:1
--------------------------------------------
Layer dropout_32_345, operation 348/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.0832e-06, 1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.4778e-04, 9.9907e-01, 7.8377e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.3425e-04, 9.9870e-01, 7.5994e-04, 4.0645e-04, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.2801e-03, 9.9093e-01, 4.6070e-03, 3.0210e-03, 1.6253e-04, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.6736e-05, 2.2789e-03, 3.0168e-05, 1.9506e-05, 3.4479e-06, 9.9765e-01,
         0.0000e+00, 0.0000e+00],
        [6.2148e-12, 2.0266e-06, 8.3973e-11, 3.2226e-11, 9.2309e-14, 5.5417e-02,
         9.4458e-01, 0.0000e+00],
        [8.7715e-13, 1.1392e-06, 1.5408e-11, 5.5035e-12, 9.4550e-15, 3.7907e-02,
         9.6209e-01, 2.1343e-09]])...
	Related Layers:
		- parent layers: softmax_26_344
		- child layers: matmul_52_347
		- shares parents with no other layers
		- shares children with layers: linear_87_346
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.3.sa.heads.1.dropout:1
	Time elapsed:  5.579E-05s
	Output of modules: blocks.3.sa.heads.1.dropout
	Output of bottom-level module: blocks.3.sa.heads.1.dropout:1
	Lookup keys: -324, 374, blocks.3.sa.heads.1.dropout, blocks.3.sa.heads.1.dropout:1, dropout_32, dropout_32:1, dropout_32_345, dropout_32_345:1
--------------------------------------------
Layer linear_87_346, operation 349/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-1.3111,  0.4004,  0.7933,  0.6222,  0.9387, -1.8227, -0.3114, -0.4339],
        [-0.8565,  0.5387,  1.0008,  0.2415,  0.8248, -1.2352, -0.4462, -0.5725],
        [-0.5475, -0.1141,  0.7239,  0.9448,  1.1318, -0.5608, -0.3632, -0.6979],
        [-0.5123, -0.1985,  0.9544,  1.2340,  0.8115, -0.8526, -0.0460, -0.8488],
        [-1.1533,  0.6527,  0.4030,  0.7902,  1.1895, -0.9566,  0.7114, -1.2540],
        [-0.8335, -0.3497,  0.1289, -0.2137, -0.4835, -1.0200, -0.8262,  0.6059],
        [ 0.1453, -0.4318,  1.1097, -0.1269,  1.4149, -0.9387, -1.1484, -0.6700],
        [-0.7880, -0.1401,  1.0695,  0.3952,  1.5208, -1.2969,  0.0676, -1.4709]])...
	Related Layers:
		- parent layers: layernorm_7_323
		- child layers: matmul_52_347
		- shares parents with layers: linear_82_324, linear_83_325, linear_84_334, linear_85_336, linear_86_337, linear_88_348, linear_89_349, linear_90_358, linear_91_360, linear_92_361, linear_93_370, linear_94_372, linear_95_373, linear_96_382, linear_97_384, linear_98_385, linear_99_394, linear_100_396, linear_101_397, linear_102_406, linear_103_408, linear_104_409, linear_105_418
		- shares children with layers: dropout_32_345
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.1.value:1
	Time elapsed:  1.321E-04s
	Output of modules: blocks.3.sa.heads.1.value
	Output of bottom-level module: blocks.3.sa.heads.1.value:1
	Lookup keys: -323, 375, blocks.3.sa.heads.1.value, blocks.3.sa.heads.1.value:1, linear_87, linear_87:1, linear_87_346, linear_87_346:1
--------------------------------------------
Layer matmul_52_347, operation 350/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-1.3111,  0.4004,  0.7933,  0.6222,  0.9387, -1.8227, -0.3114, -0.4339],
        [-0.8565,  0.5387,  1.0008,  0.2415,  0.8248, -1.2352, -0.4462, -0.5725],
        [-0.8563,  0.5382,  1.0005,  0.2421,  0.8251, -1.2347, -0.4461, -0.5726],
        [-0.8562,  0.5379,  1.0005,  0.2425,  0.8251, -1.2346, -0.4459, -0.5727],
        [-0.8547,  0.5333,  0.9990,  0.2483,  0.8264, -1.2316, -0.4442, -0.5739],
        [-0.8336, -0.3476,  0.1310, -0.2126, -0.4805, -1.0205, -0.8253,  0.6031],
        [ 0.0911, -0.4273,  1.0554, -0.1317,  1.3097, -0.9432, -1.1306, -0.5993],
        [ 0.1082, -0.4287,  1.0725, -0.1302,  1.3430, -0.9418, -1.1362, -0.6216]])...
	Related Layers:
		- parent layers: dropout_32_345, linear_87_346
		- child layers: cat_4_420
		- shares parents with no other layers
		- shares children with layers: matmul_50_335, matmul_54_359, matmul_56_371, matmul_58_383, matmul_60_395, matmul_62_407, matmul_64_419
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.1:1
	Time elapsed:  8.702E-05s
	Output of modules: blocks.3.sa.heads.1
	Lookup keys: -322, 376, blocks.3.sa.heads.1, blocks.3.sa.heads.1:1, matmul_52, matmul_52:1, matmul_52_347, matmul_52_347:1
--------------------------------------------
Layer linear_88_348, operation 351/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.7935,  1.2823,  1.9784,  5.0154,  2.2924, -3.7210,  0.8293,  4.5933],
        [ 0.1183,  1.5988,  2.3545,  2.8179,  0.2226, -2.5744,  0.9044,  4.7738],
        [-1.0374,  0.5152,  1.3171,  3.3591,  2.2402, -2.5072,  1.4536,  3.6526],
        [-1.4242,  0.7214,  0.7172,  3.2813,  1.9819, -2.5449,  1.7150,  3.5020],
        [-1.0540,  0.3333,  1.9574,  3.3959,  2.7909, -2.2485,  0.0569,  2.4576],
        [ 1.0503,  1.6976,  0.3511,  1.8536, -1.5357, -1.4569,  3.1542,  4.9979],
        [ 1.7144,  1.0679,  1.0707,  1.1774, -1.8624, -0.8259,  2.5078,  4.7700],
        [ 0.9721, -0.2798,  1.8275,  2.0413,  0.9756, -2.5167,  1.6741,  4.4216]])...
	Related Layers:
		- parent layers: layernorm_7_323
		- child layers: transpose_27_350
		- shares parents with layers: linear_82_324, linear_83_325, linear_84_334, linear_85_336, linear_86_337, linear_87_346, linear_89_349, linear_90_358, linear_91_360, linear_92_361, linear_93_370, linear_94_372, linear_95_373, linear_96_382, linear_97_384, linear_98_385, linear_99_394, linear_100_396, linear_101_397, linear_102_406, linear_103_408, linear_104_409, linear_105_418
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.2.key:1
	Time elapsed:  1.135E-04s
	Output of modules: blocks.3.sa.heads.2.key
	Output of bottom-level module: blocks.3.sa.heads.2.key:1
	Lookup keys: -321, 377, blocks.3.sa.heads.2.key, blocks.3.sa.heads.2.key:1, linear_88, linear_88:1, linear_88_348, linear_88_348:1
--------------------------------------------
Layer linear_89_349, operation 352/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.8727, -0.6422, -0.3675, -0.4981,  0.7348,  0.8059,  1.0467, -0.8114],
        [ 0.7788,  0.5056, -0.6815, -0.6776, -1.2411,  1.2037, -0.9445,  1.1908],
        [-0.6347, -0.7132, -1.2154, -1.2881,  0.3719,  1.2563,  1.3687, -0.2367],
        [-0.9429, -0.7034, -1.0449, -1.0879,  0.4213,  0.9949,  0.9882, -0.3307],
        [ 0.0342, -0.3172, -1.0413, -1.0230,  0.3359,  1.5689,  2.0533, -0.4660],
        [-0.0349, -0.0272, -0.6324, -0.5080,  0.4753,  1.0151, -0.3883, -1.6586],
        [-0.4871,  0.2043, -0.1971, -1.6307, -0.0286,  0.6173, -0.5462, -0.6559],
        [ 0.7825,  0.3296, -0.2552,  0.0551, -0.3805,  0.0139,  1.1060,  0.9957]])...
	Related Layers:
		- parent layers: layernorm_7_323
		- child layers: matmul_53_351
		- shares parents with layers: linear_82_324, linear_83_325, linear_84_334, linear_85_336, linear_86_337, linear_87_346, linear_88_348, linear_90_358, linear_91_360, linear_92_361, linear_93_370, linear_94_372, linear_95_373, linear_96_382, linear_97_384, linear_98_385, linear_99_394, linear_100_396, linear_101_397, linear_102_406, linear_103_408, linear_104_409, linear_105_418
		- shares children with layers: transpose_27_350
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.2.query:1
	Time elapsed:  1.109E-04s
	Output of modules: blocks.3.sa.heads.2.query
	Output of bottom-level module: blocks.3.sa.heads.2.query:1
	Lookup keys: -320, 378, blocks.3.sa.heads.2.query, blocks.3.sa.heads.2.query:1, linear_89, linear_89:1, linear_89_349, linear_89_349:1
--------------------------------------------
Layer transpose_27_350, operation 353/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.7935,  0.1183, -1.0374, -1.4242, -1.0540,  1.0503,  1.7144,  0.9721],
        [ 1.2823,  1.5988,  0.5152,  0.7214,  0.3333,  1.6976,  1.0679, -0.2798],
        [ 1.9784,  2.3545,  1.3171,  0.7172,  1.9574,  0.3511,  1.0707,  1.8275],
        [ 5.0154,  2.8179,  3.3591,  3.2813,  3.3959,  1.8536,  1.1774,  2.0413],
        [ 2.2924,  0.2226,  2.2402,  1.9819,  2.7909, -1.5357, -1.8624,  0.9756],
        [-3.7210, -2.5744, -2.5072, -2.5449, -2.2485, -1.4569, -0.8259, -2.5167],
        [ 0.8293,  0.9044,  1.4536,  1.7150,  0.0569,  3.1542,  2.5078,  1.6741],
        [ 4.5933,  4.7738,  3.6526,  3.5020,  2.4576,  4.9979,  4.7700,  4.4216]])...
	Related Layers:
		- parent layers: linear_88_348
		- child layers: matmul_53_351
		- shares parents with no other layers
		- shares children with layers: linear_89_349
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.3.sa.heads.2:1
	Time elapsed:  6.127E-05s
	Output of modules: none
	Lookup keys: -319, 379, transpose_27, transpose_27:1, transpose_27_350, transpose_27_350:1
--------------------------------------------
Layer matmul_53_351, operation 354/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -52.5624,  -56.7185,  -37.7672,  -31.0503,  -23.3703,  -44.9769,
          -46.7441,  -41.5124],
        [ -57.3752,  -28.6910,  -50.6482,  -48.4708,  -44.1470,   -1.9836,
            7.7416,  -24.7800],
        [ -92.1514,  -85.3735,  -68.3280,  -60.3170,  -48.8852,  -63.0119,
          -59.8887,  -64.4309],
        [ -88.1223,  -82.1036,  -66.5782,  -58.0505,  -47.1486,  -60.7265,
          -57.7448,  -61.0093],
        [ -36.4746,  -30.2295,  -23.5842,  -18.1238,  -24.5461,   -2.8093,
           -7.4049,  -17.4201],
        [-103.4196,  -94.8624,  -82.5457,  -76.3330,  -53.5330,  -85.8838,
          -78.9212,  -83.8528],
        [ -88.9099,  -76.1453,  -74.5632,  -68.0494,  -48.4626,  -62.6548,
          -55.7885,  -66.1008],
        [  25.4413,   31.6310,   21.8211,   25.5868,    3.7458,   65.7425,
           55.5103,   37.8311]])...
	Related Layers:
		- parent layers: linear_89_349, transpose_27_350
		- child layers: mul_27_352
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.2:1
	Time elapsed:  8.869E-05s
	Output of modules: none
	Lookup keys: -318, 380, matmul_53, matmul_53:1, matmul_53_351, matmul_53_351:1
--------------------------------------------
Layer mul_27_352, operation 355/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-2.6823, -2.8944, -1.9273, -1.5845, -1.1926, -2.2952, -2.3854, -2.1184],
        [-2.9279, -1.4641, -2.5846, -2.4735, -2.2529, -0.1012,  0.3951, -1.2646],
        [-4.7026, -4.3567, -3.4869, -3.0780, -2.4947, -3.2156, -3.0562, -3.2880],
        [-4.4970, -4.1898, -3.3976, -2.9624, -2.4060, -3.0989, -2.9468, -3.1134],
        [-1.8613, -1.5426, -1.2035, -0.9249, -1.2526, -0.1434, -0.3779, -0.8890],
        [-5.2776, -4.8409, -4.2124, -3.8954, -2.7318, -4.3827, -4.0274, -4.2791],
        [-4.5372, -3.8858, -3.8050, -3.4726, -2.4731, -3.1973, -2.8469, -3.3732],
        [ 1.2983,  1.6142,  1.1136,  1.3057,  0.1912,  3.3549,  2.8327,  1.9306]])...
	Related Layers:
		- parent layers: matmul_53_351
		- child layers: maskedfill_27_355
		- shares parents with no other layers
		- shares children with layers: eq_27_354
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.3.sa.heads.2:1
	Time elapsed:  7.224E-05s
	Output of modules: none
	Lookup keys: -317, 381, mul_27, mul_27:1, mul_27_352, mul_27_352:1
--------------------------------------------
Layer getitem_27_353, operation 356/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_27
		- child layers: eq_27_354
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.441E-05s
	Output of modules: none
	Lookup keys: -315, 383, getitem_27, getitem_27:1, getitem_27_353, getitem_27_353:1
--------------------------------------------
Layer eq_27_354, operation 357/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_27_353
		- child layers: maskedfill_27_355
		- shares parents with no other layers
		- shares children with layers: mul_27_352
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.154E-05s
	Output of modules: none
	Lookup keys: -314, 384, eq_27, eq_27:1, eq_27_354, eq_27_354:1
--------------------------------------------
Layer maskedfill_27_355, operation 358/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-2.6823,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-2.9279, -1.4641,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-4.7026, -4.3567, -3.4869,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-4.4970, -4.1898, -3.3976, -2.9624,    -inf,    -inf,    -inf,    -inf],
        [-1.8613, -1.5426, -1.2035, -0.9249, -1.2526,    -inf,    -inf,    -inf],
        [-5.2776, -4.8409, -4.2124, -3.8954, -2.7318, -4.3827,    -inf,    -inf],
        [-4.5372, -3.8858, -3.8050, -3.4726, -2.4731, -3.1973, -2.8469,    -inf],
        [ 1.2983,  1.6142,  1.1136,  1.3057,  0.1912,  3.3549,  2.8327,  1.9306]])...
	Related Layers:
		- parent layers: mul_27_352, eq_27_354
		- child layers: softmax_27_356
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.3.sa.heads.2:1
	Time elapsed:  8.225E-05s
	Output of modules: none
	Lookup keys: -313, 385, maskedfill_27, maskedfill_27:1, maskedfill_27_355, maskedfill_27_355:1
--------------------------------------------
Layer softmax_27_356, operation 359/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1879, 0.8121, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1728, 0.2443, 0.5829, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1000, 0.1359, 0.3002, 0.4639, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1150, 0.1582, 0.2220, 0.2934, 0.2114, 0.0000, 0.0000, 0.0000],
        [0.0406, 0.0628, 0.1178, 0.1617, 0.5177, 0.0993, 0.0000, 0.0000],
        [0.0400, 0.0767, 0.0831, 0.1159, 0.3149, 0.1526, 0.2167, 0.0000],
        [0.0530, 0.0726, 0.0440, 0.0534, 0.0175, 0.4141, 0.2457, 0.0997]])...
	Related Layers:
		- parent layers: maskedfill_27_355
		- child layers: dropout_33_357
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.3.sa.heads.2:1
	Time elapsed:  7.010E-05s
	Output of modules: none
	Lookup keys: -312, 386, softmax_27, softmax_27:1, softmax_27_356, softmax_27_356:1
--------------------------------------------
Layer dropout_33_357, operation 360/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1879, 0.8121, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1728, 0.2443, 0.5829, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1000, 0.1359, 0.3002, 0.4639, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1150, 0.1582, 0.2220, 0.2934, 0.2114, 0.0000, 0.0000, 0.0000],
        [0.0406, 0.0628, 0.1178, 0.1617, 0.5177, 0.0993, 0.0000, 0.0000],
        [0.0400, 0.0767, 0.0831, 0.1159, 0.3149, 0.1526, 0.2167, 0.0000],
        [0.0530, 0.0726, 0.0440, 0.0534, 0.0175, 0.4141, 0.2457, 0.0997]])...
	Related Layers:
		- parent layers: softmax_27_356
		- child layers: matmul_54_359
		- shares parents with no other layers
		- shares children with layers: linear_90_358
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.3.sa.heads.2.dropout:1
	Time elapsed:  5.531E-05s
	Output of modules: blocks.3.sa.heads.2.dropout
	Output of bottom-level module: blocks.3.sa.heads.2.dropout:1
	Lookup keys: -311, 387, blocks.3.sa.heads.2.dropout, blocks.3.sa.heads.2.dropout:1, dropout_33, dropout_33:1, dropout_33_357, dropout_33_357:1
--------------------------------------------
Layer linear_90_358, operation 361/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.2871,  0.0508, -0.0970,  0.2220, -0.8790, -0.0617, -0.5894,  0.2331],
        [ 1.2538, -0.2275, -0.5043,  0.9069, -0.2218, -0.2737, -0.1148,  0.0112],
        [ 0.5148, -0.1712,  0.2773,  0.4828, -0.5177, -0.1445, -0.2643,  0.0492],
        [ 0.5884, -0.2624,  0.2080,  0.7750, -0.5134,  0.0494, -0.6570, -0.2857],
        [ 0.4723, -0.4811, -0.1135,  1.0437, -0.5458,  0.1835, -0.7464,  0.0485],
        [-0.2555, -0.0653, -0.6092, -0.9702,  1.7379,  1.3664, -0.8968,  0.3859],
        [ 0.6996,  0.1322,  0.6383,  1.3389,  1.6779,  0.7418, -0.1808,  0.5259],
        [ 0.6976,  0.5722,  0.2588,  0.6838,  0.4913, -0.5778,  0.2967,  0.6240]])...
	Related Layers:
		- parent layers: layernorm_7_323
		- child layers: matmul_54_359
		- shares parents with layers: linear_82_324, linear_83_325, linear_84_334, linear_85_336, linear_86_337, linear_87_346, linear_88_348, linear_89_349, linear_91_360, linear_92_361, linear_93_370, linear_94_372, linear_95_373, linear_96_382, linear_97_384, linear_98_385, linear_99_394, linear_100_396, linear_101_397, linear_102_406, linear_103_408, linear_104_409, linear_105_418
		- shares children with layers: dropout_33_357
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.2.value:1
	Time elapsed:  1.185E-04s
	Output of modules: blocks.3.sa.heads.2.value
	Output of bottom-level module: blocks.3.sa.heads.2.value:1
	Lookup keys: -310, 388, blocks.3.sa.heads.2.value, blocks.3.sa.heads.2.value:1, linear_90, linear_90:1, linear_90_358, linear_90_358:1
--------------------------------------------
Layer matmul_54_359, operation 362/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.2871,  0.0508, -0.0970,  0.2220, -0.8790, -0.0617, -0.5894,  0.2331],
        [ 1.0722, -0.1752, -0.4277,  0.7782, -0.3453, -0.2339, -0.2040,  0.0529],
        [ 0.6560, -0.1466,  0.0217,  0.5413, -0.5079, -0.1618, -0.2840,  0.0717],
        [ 0.6267, -0.1990,  0.1015,  0.6499, -0.5116, -0.0639, -0.4587, -0.0929],
        [ 0.6181, -0.2468,  0.0077,  0.7242, -0.5171, -0.0292, -0.4952, -0.0341],
        [ 0.4654, -0.3304, -0.0886,  0.6922, -0.3036,  0.2020, -0.6440,  0.0332],
        [ 0.4800, -0.1929,  0.0142,  0.6791,  0.3023,  0.3974, -0.5416,  0.1693],
        [ 0.3042,  0.0187, -0.0901,  0.1538,  1.0585,  0.6669, -0.4856,  0.3522]])...
	Related Layers:
		- parent layers: dropout_33_357, linear_90_358
		- child layers: cat_4_420
		- shares parents with no other layers
		- shares children with layers: matmul_50_335, matmul_52_347, matmul_56_371, matmul_58_383, matmul_60_395, matmul_62_407, matmul_64_419
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.2:1
	Time elapsed:  8.416E-05s
	Output of modules: blocks.3.sa.heads.2
	Lookup keys: -309, 389, blocks.3.sa.heads.2, blocks.3.sa.heads.2:1, matmul_54, matmul_54:1, matmul_54_359, matmul_54_359:1
--------------------------------------------
Layer linear_91_360, operation 363/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-3.3494, -2.0294,  3.8118,  3.5974,  1.2446, -1.7380, -1.8259,  1.1431],
        [-2.5476, -2.0558,  1.9302,  1.4029,  0.7425, -1.9796,  0.7581,  0.3666],
        [-3.6071, -1.5752,  3.5577,  2.4983,  1.5189, -1.4261, -1.4947,  0.4230],
        [-3.6311, -0.7977,  3.4200,  2.3024,  1.4169, -1.4268, -1.7290,  0.9389],
        [-3.8665, -1.2422,  3.6112,  2.4035,  1.6805, -0.9323, -2.1363,  1.5937],
        [-0.6389, -0.6791,  0.0218, -0.2829, -0.0261,  0.5766,  0.5951, -0.5531],
        [-1.4533, -1.1743,  0.0680,  1.5153, -0.1207, -0.4809,  0.7544, -1.9850],
        [-3.5550, -1.7014,  1.3527,  2.3424,  0.2587, -1.3453, -0.7678,  0.9977]])...
	Related Layers:
		- parent layers: layernorm_7_323
		- child layers: transpose_28_362
		- shares parents with layers: linear_82_324, linear_83_325, linear_84_334, linear_85_336, linear_86_337, linear_87_346, linear_88_348, linear_89_349, linear_90_358, linear_92_361, linear_93_370, linear_94_372, linear_95_373, linear_96_382, linear_97_384, linear_98_385, linear_99_394, linear_100_396, linear_101_397, linear_102_406, linear_103_408, linear_104_409, linear_105_418
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.3.key:1
	Time elapsed:  1.149E-04s
	Output of modules: blocks.3.sa.heads.3.key
	Output of bottom-level module: blocks.3.sa.heads.3.key:1
	Lookup keys: -308, 390, blocks.3.sa.heads.3.key, blocks.3.sa.heads.3.key:1, linear_91, linear_91:1, linear_91_360, linear_91_360:1
--------------------------------------------
Layer linear_92_361, operation 364/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.5310,  1.1482, -0.2298, -0.8918, -2.1564, -0.8185, -0.9689,  2.0631],
        [ 0.4538,  0.7719,  0.3790, -1.4213, -0.5544, -0.7598, -0.7491,  1.3209],
        [ 0.1773,  0.6956, -0.1440, -0.7954, -1.4246, -0.6663, -0.5735,  1.2402],
        [-0.1310,  0.7072,  0.0782, -0.4366, -1.6222, -1.1343, -0.7271,  1.1653],
        [ 0.1630,  1.4425, -0.2894, -0.6786, -1.3346, -0.3918, -0.5496,  1.9597],
        [ 1.1910,  0.9718, -2.0653, -1.0756, -0.8058,  1.3991,  0.2790,  0.0671],
        [ 1.0654,  0.6683, -0.9790, -2.0968, -0.0892,  0.3096, -0.0862, -0.3465],
        [ 0.2563,  0.8906, -0.7919, -1.1908, -0.2319, -0.8788,  0.0145,  0.5165]])...
	Related Layers:
		- parent layers: layernorm_7_323
		- child layers: matmul_55_363
		- shares parents with layers: linear_82_324, linear_83_325, linear_84_334, linear_85_336, linear_86_337, linear_87_346, linear_88_348, linear_89_349, linear_90_358, linear_91_360, linear_93_370, linear_94_372, linear_95_373, linear_96_382, linear_97_384, linear_98_385, linear_99_394, linear_100_396, linear_101_397, linear_102_406, linear_103_408, linear_104_409, linear_105_418
		- shares children with layers: transpose_28_362
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.3.query:1
	Time elapsed:  1.116E-04s
	Output of modules: blocks.3.sa.heads.3.query
	Output of bottom-level module: blocks.3.sa.heads.3.query:1
	Lookup keys: -307, 391, blocks.3.sa.heads.3.query, blocks.3.sa.heads.3.query:1, linear_92, linear_92:1, linear_92_361, linear_92_361:1
--------------------------------------------
Layer transpose_28_362, operation 365/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-3.3494, -2.5476, -3.6071, -3.6311, -3.8665, -0.6389, -1.4533, -3.5550],
        [-2.0294, -2.0558, -1.5752, -0.7977, -1.2422, -0.6791, -1.1743, -1.7014],
        [ 3.8118,  1.9302,  3.5577,  3.4200,  3.6112,  0.0218,  0.0680,  1.3527],
        [ 3.5974,  1.4029,  2.4983,  2.3024,  2.4035, -0.2829,  1.5153,  2.3424],
        [ 1.2446,  0.7425,  1.5189,  1.4169,  1.6805, -0.0261, -0.1207,  0.2587],
        [-1.7380, -1.9796, -1.4261, -1.4268, -0.9323,  0.5766, -0.4809, -1.3453],
        [-1.8259,  0.7581, -1.4947, -1.7290, -2.1363,  0.5951,  0.7544, -0.7678],
        [ 1.1431,  0.3666,  0.4230,  0.9389,  1.5937, -0.5531, -1.9850,  0.9977]])...
	Related Layers:
		- parent layers: linear_91_360
		- child layers: matmul_55_363
		- shares parents with no other layers
		- shares children with layers: linear_92_361
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.3.sa.heads.3:1
	Time elapsed:  6.056E-05s
	Output of modules: none
	Lookup keys: -306, 392, transpose_28, transpose_28:1, transpose_28_362, transpose_28_362:1
--------------------------------------------
Layer matmul_55_363, operation 366/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[  18.7412,   12.5495,   13.1573,   18.4636,   18.2515,   -8.6598,
          -10.3475,   14.4310],
        [  -7.9293,   -5.1858,  -10.2847,   -7.0442,   -5.8247,   -3.5974,
          -13.8842,   -7.7422],
        [  23.7394,   22.3701,   22.3090,   27.1926,   24.0607,   -8.0196,
            4.0081,   21.8777],
        [  35.5528,   28.4084,   31.9954,   36.3902,   33.4324,   -8.3332,
            7.2880,   28.4945],
        [   7.6023,   -1.3973,    1.5733,    5.7130,    5.4994,   -4.9433,
          -14.2070,    0.1281],
        [-103.2167,  -44.7199,  -86.7303,  -86.1640,  -94.6072,    0.6486,
          -14.3591,  -65.1084],
        [ -75.9682,  -40.0983,  -69.4095,  -69.3365,  -71.7952,    4.6659,
          -16.0335,  -55.6977],
        [ -22.5839,   -8.6675,  -20.3733,  -17.9349,  -19.5050,    0.7922,
           -5.9343,  -15.6827]])...
	Related Layers:
		- parent layers: linear_92_361, transpose_28_362
		- child layers: mul_28_364
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.3:1
	Time elapsed:  8.678E-05s
	Output of modules: none
	Lookup keys: -305, 393, matmul_55, matmul_55:1, matmul_55_363, matmul_55_363:1
--------------------------------------------
Layer mul_28_364, operation 367/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ 0.9564,  0.6404,  0.6714,  0.9422,  0.9314, -0.4419, -0.5280,  0.7364],
        [-0.4046, -0.2646, -0.5248, -0.3595, -0.2972, -0.1836, -0.7085, -0.3951],
        [ 1.2114,  1.1416,  1.1385,  1.3877,  1.2278, -0.4093,  0.2045,  1.1164],
        [ 1.8143,  1.4497,  1.6328,  1.8570,  1.7061, -0.4253,  0.3719,  1.4541],
        [ 0.3880, -0.0713,  0.0803,  0.2915,  0.2806, -0.2523, -0.7250,  0.0065],
        [-5.2673, -2.2821, -4.4259, -4.3970, -4.8279,  0.0331, -0.7328, -3.3225],
        [-3.8767, -2.0463, -3.5420, -3.5383, -3.6638,  0.2381, -0.8182, -2.8423],
        [-1.1525, -0.4423, -1.0397, -0.9152, -0.9954,  0.0404, -0.3028, -0.8003]])...
	Related Layers:
		- parent layers: matmul_55_363
		- child layers: maskedfill_28_367
		- shares parents with no other layers
		- shares children with layers: eq_28_366
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.3.sa.heads.3:1
	Time elapsed:  6.819E-05s
	Output of modules: none
	Lookup keys: -304, 394, mul_28, mul_28:1, mul_28_364, mul_28_364:1
--------------------------------------------
Layer getitem_28_365, operation 368/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_28
		- child layers: eq_28_366
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.584E-05s
	Output of modules: none
	Lookup keys: -302, 396, getitem_28, getitem_28:1, getitem_28_365, getitem_28_365:1
--------------------------------------------
Layer eq_28_366, operation 369/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_28_365
		- child layers: maskedfill_28_367
		- shares parents with no other layers
		- shares children with layers: mul_28_364
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  7.868E-05s
	Output of modules: none
	Lookup keys: -301, 397, eq_28, eq_28:1, eq_28_366, eq_28_366:1
--------------------------------------------
Layer maskedfill_28_367, operation 370/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ 0.9564,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-0.4046, -0.2646,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [ 1.2114,  1.1416,  1.1385,    -inf,    -inf,    -inf,    -inf,    -inf],
        [ 1.8143,  1.4497,  1.6328,  1.8570,    -inf,    -inf,    -inf,    -inf],
        [ 0.3880, -0.0713,  0.0803,  0.2915,  0.2806,    -inf,    -inf,    -inf],
        [-5.2673, -2.2821, -4.4259, -4.3970, -4.8279,  0.0331,    -inf,    -inf],
        [-3.8767, -2.0463, -3.5420, -3.5383, -3.6638,  0.2381, -0.8182,    -inf],
        [-1.1525, -0.4423, -1.0397, -0.9152, -0.9954,  0.0404, -0.3028, -0.8003]])...
	Related Layers:
		- parent layers: mul_28_364, eq_28_366
		- child layers: softmax_28_368
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.3.sa.heads.3:1
	Time elapsed:  8.345E-05s
	Output of modules: none
	Lookup keys: -300, 398, maskedfill_28, maskedfill_28:1, maskedfill_28_367, maskedfill_28_367:1
--------------------------------------------
Layer softmax_28_368, operation 371/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.4651, 0.5349, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3494, 0.3258, 0.3248, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2799, 0.1944, 0.2335, 0.2922, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2396, 0.1514, 0.1762, 0.2176, 0.2152, 0.0000, 0.0000, 0.0000],
        [0.0044, 0.0870, 0.0102, 0.0105, 0.0068, 0.8811, 0.0000, 0.0000],
        [0.0107, 0.0665, 0.0149, 0.0150, 0.0132, 0.6528, 0.2270, 0.0000],
        [0.0733, 0.1490, 0.0820, 0.0929, 0.0857, 0.2415, 0.1714, 0.1042]])...
	Related Layers:
		- parent layers: maskedfill_28_367
		- child layers: dropout_34_369
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.3.sa.heads.3:1
	Time elapsed:  7.057E-05s
	Output of modules: none
	Lookup keys: -299, 399, softmax_28, softmax_28:1, softmax_28_368, softmax_28_368:1
--------------------------------------------
Layer dropout_34_369, operation 372/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.4651, 0.5349, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3494, 0.3258, 0.3248, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2799, 0.1944, 0.2335, 0.2922, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2396, 0.1514, 0.1762, 0.2176, 0.2152, 0.0000, 0.0000, 0.0000],
        [0.0044, 0.0870, 0.0102, 0.0105, 0.0068, 0.8811, 0.0000, 0.0000],
        [0.0107, 0.0665, 0.0149, 0.0150, 0.0132, 0.6528, 0.2270, 0.0000],
        [0.0733, 0.1490, 0.0820, 0.0929, 0.0857, 0.2415, 0.1714, 0.1042]])...
	Related Layers:
		- parent layers: softmax_28_368
		- child layers: matmul_56_371
		- shares parents with no other layers
		- shares children with layers: linear_93_370
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.3.sa.heads.3.dropout:1
	Time elapsed:  5.412E-05s
	Output of modules: blocks.3.sa.heads.3.dropout
	Output of bottom-level module: blocks.3.sa.heads.3.dropout:1
	Lookup keys: -298, 400, blocks.3.sa.heads.3.dropout, blocks.3.sa.heads.3.dropout:1, dropout_34, dropout_34:1, dropout_34_369, dropout_34_369:1
--------------------------------------------
Layer linear_93_370, operation 373/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 2.8400e-01, -1.2101e+00, -5.6221e-01, -1.2758e+00,  9.6397e-01,
          7.0999e-01,  3.6273e-01,  3.1205e-01],
        [ 9.5399e-01, -7.1643e-01, -6.1632e-01, -1.5447e+00,  6.7141e-01,
          5.7261e-01,  3.9729e-01, -9.5676e-01],
        [ 1.9139e-01, -1.2065e+00, -6.2472e-01, -1.2278e+00,  7.4089e-01,
          5.6570e-01,  2.1325e-01,  1.5660e-01],
        [ 2.5861e-01, -1.3393e+00, -5.8442e-01, -1.6827e+00,  7.8567e-01,
          4.8495e-01,  1.4436e-01,  1.1935e-01],
        [ 1.7210e-01, -1.6319e+00, -9.0388e-01, -1.9824e+00,  6.8324e-01,
          4.0376e-01, -2.9146e-01,  1.0165e-01],
        [ 7.1796e-01,  1.3385e+00, -2.7129e-01,  1.7655e-03, -8.7107e-01,
         -1.6555e+00,  1.1429e+00, -1.6091e+00],
        [ 6.6065e-01,  7.3063e-01, -2.5285e-01, -1.0765e+00, -5.0105e-01,
          5.5989e-01,  1.2678e+00, -3.9618e-01],
        [ 8.7838e-01,  1.5638e-01, -2.5265e-01, -9.9417e-01,  6.5316e-01,
          8.8638e-01,  9.1890e-01, -1.0929e+00]])...
	Related Layers:
		- parent layers: layernorm_7_323
		- child layers: matmul_56_371
		- shares parents with layers: linear_82_324, linear_83_325, linear_84_334, linear_85_336, linear_86_337, linear_87_346, linear_88_348, linear_89_349, linear_90_358, linear_91_360, linear_92_361, linear_94_372, linear_95_373, linear_96_382, linear_97_384, linear_98_385, linear_99_394, linear_100_396, linear_101_397, linear_102_406, linear_103_408, linear_104_409, linear_105_418
		- shares children with layers: dropout_34_369
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.3.value:1
	Time elapsed:  1.183E-04s
	Output of modules: blocks.3.sa.heads.3.value
	Output of bottom-level module: blocks.3.sa.heads.3.value:1
	Lookup keys: -297, 401, blocks.3.sa.heads.3.value, blocks.3.sa.heads.3.value:1, linear_93, linear_93:1, linear_93_370, linear_93_370:1
--------------------------------------------
Layer matmul_56_371, operation 374/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.2840, -1.2101, -0.5622, -1.2758,  0.9640,  0.7100,  0.3627,  0.3121],
        [ 0.6424, -0.9460, -0.5912, -1.4197,  0.8075,  0.6365,  0.3812, -0.3667],
        [ 0.4722, -1.0481, -0.6001, -1.3478,  0.7962,  0.6184,  0.3254, -0.1518],
        [ 0.3852, -1.1510, -0.5938, -1.4358,  0.8029,  0.5838,  0.2707, -0.0272],
        [ 0.3395, -1.2536, -0.6598, -1.5487,  0.7812,  0.5489,  0.1533,  0.0054],
        [ 0.7227,  1.0742, -0.3138, -0.1822, -0.6844, -1.3921,  1.0449, -1.4961],
        [ 0.6941,  0.9196, -0.3114, -0.4291, -0.5957, -0.8870,  1.0657, -1.1952],
        [ 0.5956, -0.0939, -0.4512, -1.0383,  0.1348,  0.0518,  0.6807, -0.6575]])...
	Related Layers:
		- parent layers: dropout_34_369, linear_93_370
		- child layers: cat_4_420
		- shares parents with no other layers
		- shares children with layers: matmul_50_335, matmul_52_347, matmul_54_359, matmul_58_383, matmul_60_395, matmul_62_407, matmul_64_419
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.3:1
	Time elapsed:  8.368E-05s
	Output of modules: blocks.3.sa.heads.3
	Lookup keys: -296, 402, blocks.3.sa.heads.3, blocks.3.sa.heads.3:1, matmul_56, matmul_56:1, matmul_56_371, matmul_56_371:1
--------------------------------------------
Layer linear_94_372, operation 375/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-4.4501,  3.2831,  3.2714, -4.7493, -0.3566, -0.9308, -1.7574, -0.7987],
        [-3.1784,  2.7844,  2.8796, -3.7213,  0.8705, -1.0895, -1.2011,  0.3139],
        [-4.1632,  2.1848,  3.4684, -3.7514, -0.4284, -0.9762, -1.5382, -0.2190],
        [-4.3240,  2.0451,  3.5168, -3.9203, -0.8153, -0.9245, -1.4601, -0.2309],
        [-3.6670,  1.8195,  2.5614, -3.6996,  0.0268, -0.0292, -1.0746, -0.3076],
        [-3.0219,  0.9342,  3.3419, -2.9528, -2.7631, -3.0911, -1.8741, -2.3498],
        [-1.3666,  0.5142,  2.6798, -2.4526, -2.0862, -3.2553, -1.2304, -1.0286],
        [-2.5187,  1.3257,  2.3899, -2.5662,  0.7204, -1.4738, -1.8539, -0.8013]])...
	Related Layers:
		- parent layers: layernorm_7_323
		- child layers: transpose_29_374
		- shares parents with layers: linear_82_324, linear_83_325, linear_84_334, linear_85_336, linear_86_337, linear_87_346, linear_88_348, linear_89_349, linear_90_358, linear_91_360, linear_92_361, linear_93_370, linear_95_373, linear_96_382, linear_97_384, linear_98_385, linear_99_394, linear_100_396, linear_101_397, linear_102_406, linear_103_408, linear_104_409, linear_105_418
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.4.key:1
	Time elapsed:  1.137E-04s
	Output of modules: blocks.3.sa.heads.4.key
	Output of bottom-level module: blocks.3.sa.heads.4.key:1
	Lookup keys: -295, 403, blocks.3.sa.heads.4.key, blocks.3.sa.heads.4.key:1, linear_94, linear_94:1, linear_94_372, linear_94_372:1
--------------------------------------------
Layer linear_95_373, operation 376/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.1789, -0.8174,  0.3143,  0.0705, -1.6992, -0.2344, -0.2007,  0.1766],
        [ 0.4450,  0.2314,  1.0548,  0.3905, -1.2897, -0.5204, -0.2413, -0.1785],
        [ 0.4398, -0.6260,  0.5548,  0.6414, -1.9972, -0.6293, -0.8739,  0.1303],
        [ 0.1808, -1.0889,  0.7108,  0.3363, -1.7667, -0.8705, -0.9248, -0.6503],
        [ 0.8771, -1.5221, -0.5158,  1.3985, -1.4532, -0.3158, -0.6957, -0.5993],
        [-1.5762,  0.8024,  0.6894, -0.8997, -0.7061, -1.5984, -0.3058, -0.2414],
        [-0.7101,  0.1439, -0.7352,  0.2613, -0.6181, -0.8962, -0.2454,  0.2302],
        [ 0.0805, -0.1887,  1.0448, -0.7525, -2.1001, -1.7176, -1.7609, -0.3030]])...
	Related Layers:
		- parent layers: layernorm_7_323
		- child layers: matmul_57_375
		- shares parents with layers: linear_82_324, linear_83_325, linear_84_334, linear_85_336, linear_86_337, linear_87_346, linear_88_348, linear_89_349, linear_90_358, linear_91_360, linear_92_361, linear_93_370, linear_94_372, linear_96_382, linear_97_384, linear_98_385, linear_99_394, linear_100_396, linear_101_397, linear_102_406, linear_103_408, linear_104_409, linear_105_418
		- shares children with layers: transpose_29_374
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.4.query:1
	Time elapsed:  1.135E-04s
	Output of modules: blocks.3.sa.heads.4.query
	Output of bottom-level module: blocks.3.sa.heads.4.query:1
	Lookup keys: -294, 404, blocks.3.sa.heads.4.query, blocks.3.sa.heads.4.query:1, linear_95, linear_95:1, linear_95_373, linear_95_373:1
--------------------------------------------
Layer transpose_29_374, operation 377/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-4.4501, -3.1784, -4.1632, -4.3240, -3.6670, -3.0219, -1.3666, -2.5187],
        [ 3.2831,  2.7844,  2.1848,  2.0451,  1.8195,  0.9342,  0.5142,  1.3257],
        [ 3.2714,  2.8796,  3.4684,  3.5168,  2.5614,  3.3419,  2.6798,  2.3899],
        [-4.7493, -3.7213, -3.7514, -3.9203, -3.6996, -2.9528, -2.4526, -2.5662],
        [-0.3566,  0.8705, -0.4284, -0.8153,  0.0268, -2.7631, -2.0862,  0.7204],
        [-0.9308, -1.0895, -0.9762, -0.9245, -0.0292, -3.0911, -3.2553, -1.4738],
        [-1.7574, -1.2011, -1.5382, -1.4601, -1.0746, -1.8741, -1.2304, -1.8539],
        [-0.7987,  0.3139, -0.2190, -0.2309, -0.3076, -2.3498, -1.0286, -0.8013]])...
	Related Layers:
		- parent layers: linear_94_372
		- child layers: matmul_57_375
		- shares parents with no other layers
		- shares children with layers: linear_95_373
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.3.sa.heads.4:1
	Time elapsed:  5.841E-05s
	Output of modules: none
	Lookup keys: -293, 405, transpose_29, transpose_29:1, transpose_29_374, transpose_29_374:1
--------------------------------------------
Layer matmul_57_375, operation 378/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-4.6828e+01, -5.6525e+01, -3.7003e+01, -3.7755e+01, -3.8748e+01,
          7.8312e-01,  1.2344e+00, -3.3285e+01],
        [-6.4981e+00,  2.0898e+00, -6.2525e+00, -7.0986e+00, -1.6660e+01,
          2.6409e+01,  3.2598e+01, -7.8839e+00],
        [-6.4023e+01, -5.4623e+01, -4.9436e+01, -5.1599e+01, -5.7510e+01,
         -4.6784e-01,  1.4036e+01, -3.3964e+01],
        [-7.1775e+01, -6.2973e+01, -5.6301e+01, -5.7919e+01, -6.3803e+01,
         -1.3403e+00,  1.1610e+01, -3.7894e+01],
        [-1.1068e+02, -1.0692e+02, -8.7467e+01, -9.0437e+01, -8.6917e+01,
         -3.1147e+01, -2.1162e+01, -6.5915e+01],
        [ 7.7121e+01,  6.2151e+01,  6.4077e+01,  6.3678e+01,  5.4278e+01,
          6.3102e+01,  5.2069e+01,  5.1841e+01],
        [ 1.1627e+01,  1.0943e+01,  9.9435e+00,  8.7304e+00,  3.1133e+00,
          2.4551e+01,  2.6004e+01,  1.0717e+01],
        [-1.7174e+01, -1.9377e+01, -9.8805e+00, -1.1614e+01, -2.1080e+01,
          4.8700e+01,  4.8799e+01,  1.0206e-01]])...
	Related Layers:
		- parent layers: linear_95_373, transpose_29_374
		- child layers: mul_29_376
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.4:1
	Time elapsed:  8.821E-05s
	Output of modules: none
	Lookup keys: -292, 406, matmul_57, matmul_57:1, matmul_57_375, matmul_57_375:1
--------------------------------------------
Layer mul_29_376, operation 379/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-2.3897e+00, -2.8846e+00, -1.8883e+00, -1.9267e+00, -1.9773e+00,
          3.9963e-02,  6.2992e-02, -1.6986e+00],
        [-3.3160e-01,  1.0665e-01, -3.1907e-01, -3.6225e-01, -8.5020e-01,
          1.3477e+00,  1.6635e+00, -4.0232e-01],
        [-3.2672e+00, -2.7874e+00, -2.5228e+00, -2.6332e+00, -2.9348e+00,
         -2.3874e-02,  7.1629e-01, -1.7332e+00],
        [-3.6628e+00, -3.2136e+00, -2.8731e+00, -2.9557e+00, -3.2560e+00,
         -6.8397e-02,  5.9246e-01, -1.9338e+00],
        [-5.6482e+00, -5.4564e+00, -4.4635e+00, -4.6151e+00, -4.4355e+00,
         -1.5894e+00, -1.0799e+00, -3.3637e+00],
        [ 3.9356e+00,  3.1716e+00,  3.2699e+00,  3.2496e+00,  2.7699e+00,
          3.2202e+00,  2.6572e+00,  2.6455e+00],
        [ 5.9333e-01,  5.5843e-01,  5.0743e-01,  4.4552e-01,  1.5888e-01,
          1.2529e+00,  1.3270e+00,  5.4688e-01],
        [-8.7639e-01, -9.8881e-01, -5.0421e-01, -5.9269e-01, -1.0757e+00,
          2.4852e+00,  2.4903e+00,  5.2082e-03]])...
	Related Layers:
		- parent layers: matmul_57_375
		- child layers: maskedfill_29_379
		- shares parents with no other layers
		- shares children with layers: eq_29_378
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.3.sa.heads.4:1
	Time elapsed:  7.081E-05s
	Output of modules: none
	Lookup keys: -291, 407, mul_29, mul_29:1, mul_29_376, mul_29_376:1
--------------------------------------------
Layer getitem_29_377, operation 380/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_29
		- child layers: eq_29_378
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.203E-05s
	Output of modules: none
	Lookup keys: -289, 409, getitem_29, getitem_29:1, getitem_29_377, getitem_29_377:1
--------------------------------------------
Layer eq_29_378, operation 381/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_29_377
		- child layers: maskedfill_29_379
		- shares parents with no other layers
		- shares children with layers: mul_29_376
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.225E-05s
	Output of modules: none
	Lookup keys: -288, 410, eq_29, eq_29:1, eq_29_378, eq_29_378:1
--------------------------------------------
Layer maskedfill_29_379, operation 382/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-2.3897e+00,        -inf,        -inf,        -inf,        -inf,
                -inf,        -inf,        -inf],
        [-3.3160e-01,  1.0665e-01,        -inf,        -inf,        -inf,
                -inf,        -inf,        -inf],
        [-3.2672e+00, -2.7874e+00, -2.5228e+00,        -inf,        -inf,
                -inf,        -inf,        -inf],
        [-3.6628e+00, -3.2136e+00, -2.8731e+00, -2.9557e+00,        -inf,
                -inf,        -inf,        -inf],
        [-5.6482e+00, -5.4564e+00, -4.4635e+00, -4.6151e+00, -4.4355e+00,
                -inf,        -inf,        -inf],
        [ 3.9356e+00,  3.1716e+00,  3.2699e+00,  3.2496e+00,  2.7699e+00,
          3.2202e+00,        -inf,        -inf],
        [ 5.9333e-01,  5.5843e-01,  5.0743e-01,  4.4552e-01,  1.5888e-01,
          1.2529e+00,  1.3270e+00,        -inf],
        [-8.7639e-01, -9.8881e-01, -5.0421e-01, -5.9269e-01, -1.0757e+00,
          2.4852e+00,  2.4903e+00,  5.2082e-03]])...
	Related Layers:
		- parent layers: mul_29_376, eq_29_378
		- child layers: softmax_29_380
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.3.sa.heads.4:1
	Time elapsed:  8.726E-05s
	Output of modules: none
	Lookup keys: -287, 411, maskedfill_29, maskedfill_29:1, maskedfill_29_379, maskedfill_29_379:1
--------------------------------------------
Layer softmax_29_380, operation 383/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3922, 0.6078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2118, 0.3422, 0.4459, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1471, 0.2305, 0.3240, 0.2983, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0858, 0.1040, 0.2806, 0.2411, 0.2886, 0.0000, 0.0000, 0.0000],
        [0.3045, 0.1418, 0.1565, 0.1533, 0.0949, 0.1489, 0.0000, 0.0000],
        [0.1189, 0.1148, 0.1091, 0.1026, 0.0770, 0.2299, 0.2476, 0.0000],
        [0.0152, 0.0136, 0.0221, 0.0202, 0.0125, 0.4387, 0.4410, 0.0367]])...
	Related Layers:
		- parent layers: maskedfill_29_379
		- child layers: dropout_35_381
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.3.sa.heads.4:1
	Time elapsed:  7.057E-05s
	Output of modules: none
	Lookup keys: -286, 412, softmax_29, softmax_29:1, softmax_29_380, softmax_29_380:1
--------------------------------------------
Layer dropout_35_381, operation 384/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3922, 0.6078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2118, 0.3422, 0.4459, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1471, 0.2305, 0.3240, 0.2983, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0858, 0.1040, 0.2806, 0.2411, 0.2886, 0.0000, 0.0000, 0.0000],
        [0.3045, 0.1418, 0.1565, 0.1533, 0.0949, 0.1489, 0.0000, 0.0000],
        [0.1189, 0.1148, 0.1091, 0.1026, 0.0770, 0.2299, 0.2476, 0.0000],
        [0.0152, 0.0136, 0.0221, 0.0202, 0.0125, 0.4387, 0.4410, 0.0367]])...
	Related Layers:
		- parent layers: softmax_29_380
		- child layers: matmul_58_383
		- shares parents with no other layers
		- shares children with layers: linear_96_382
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.3.sa.heads.4.dropout:1
	Time elapsed:  5.579E-05s
	Output of modules: blocks.3.sa.heads.4.dropout
	Output of bottom-level module: blocks.3.sa.heads.4.dropout:1
	Lookup keys: -285, 413, blocks.3.sa.heads.4.dropout, blocks.3.sa.heads.4.dropout:1, dropout_35, dropout_35:1, dropout_35_381, dropout_35_381:1
--------------------------------------------
Layer linear_96_382, operation 385/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.8551,  0.4563,  0.4156,  1.4575, -0.2465,  0.3860,  0.4991, -0.4181],
        [-0.1689, -0.7009, -0.5603,  1.5487, -0.1741,  0.2579,  0.1332, -0.3619],
        [-0.2053,  0.1784, -0.9793,  0.9106, -0.2054, -0.0210,  0.4626, -0.2279],
        [ 0.1014,  0.1220, -0.7299,  0.8239, -0.2611,  0.0502,  0.4249, -0.8394],
        [-1.2856,  0.8941, -0.3169,  0.6145, -0.2160,  0.0722,  0.7511, -0.4488],
        [ 0.3487, -0.5002,  0.3628,  0.5956,  0.0538, -0.3021, -0.4841,  0.1650],
        [-0.3523, -0.4265,  0.7492,  0.8513,  0.0193,  0.5061, -0.6378,  0.2303],
        [-0.1882,  0.8795,  0.6089,  0.3661, -0.0508, -0.3320,  0.7659,  0.2440]])...
	Related Layers:
		- parent layers: layernorm_7_323
		- child layers: matmul_58_383
		- shares parents with layers: linear_82_324, linear_83_325, linear_84_334, linear_85_336, linear_86_337, linear_87_346, linear_88_348, linear_89_349, linear_90_358, linear_91_360, linear_92_361, linear_93_370, linear_94_372, linear_95_373, linear_97_384, linear_98_385, linear_99_394, linear_100_396, linear_101_397, linear_102_406, linear_103_408, linear_104_409, linear_105_418
		- shares children with layers: dropout_35_381
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.4.value:1
	Time elapsed:  1.273E-04s
	Output of modules: blocks.3.sa.heads.4.value
	Output of bottom-level module: blocks.3.sa.heads.4.value:1
	Lookup keys: -284, 414, blocks.3.sa.heads.4.value, blocks.3.sa.heads.4.value:1, linear_96, linear_96:1, linear_96_382, linear_96_382:1
--------------------------------------------
Layer matmul_58_383, operation 386/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-8.5512e-01,  4.5634e-01,  4.1556e-01,  1.4575e+00, -2.4645e-01,
          3.8595e-01,  4.9911e-01, -4.1815e-01],
        [-4.3798e-01, -2.4710e-01, -1.7763e-01,  1.5130e+00, -2.0247e-01,
          3.0815e-01,  2.7669e-01, -3.8395e-01],
        [-3.3047e-01, -6.3653e-02, -5.4046e-01,  1.2448e+00, -2.0338e-01,
          1.6066e-01,  3.5762e-01, -3.1408e-01],
        [-2.0100e-01, -2.3287e-04, -6.0315e-01,  1.1123e+00, -2.2084e-01,
          1.2439e-01,  3.8079e-01, -4.6923e-01],
        [-4.9506e-01,  3.0376e-01, -5.6480e-01,  9.1752e-01, -2.2216e-01,
          8.6958e-02,  5.0564e-01, -4.6934e-01],
        [-3.7103e-01,  9.6542e-02, -1.9420e-01,  1.0793e+00, -1.8441e-01,
          1.2038e-01,  3.0763e-01, -3.6107e-01],
        [-2.3915e-01, -1.4603e-01,  4.7897e-02,  9.3009e-01, -9.7967e-02,
          1.3978e-01, -4.2725e-02, -1.4181e-01],
        [-4.3130e-02, -3.6025e-01,  4.7030e-01,  7.3779e-01,  1.1629e-02,
          8.9252e-02, -4.2792e-01,  1.4407e-01]])...
	Related Layers:
		- parent layers: dropout_35_381, linear_96_382
		- child layers: cat_4_420
		- shares parents with no other layers
		- shares children with layers: matmul_50_335, matmul_52_347, matmul_54_359, matmul_56_371, matmul_60_395, matmul_62_407, matmul_64_419
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.4:1
	Time elapsed:  8.368E-05s
	Output of modules: blocks.3.sa.heads.4
	Lookup keys: -283, 415, blocks.3.sa.heads.4, blocks.3.sa.heads.4:1, matmul_58, matmul_58:1, matmul_58_383, matmul_58_383:1
--------------------------------------------
Layer linear_97_384, operation 387/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 2.0863,  2.9227, -3.4699,  0.5425, -1.6634,  3.1332, -0.6897, -2.9773],
        [ 2.1013,  2.8277, -3.3662,  1.1498, -1.5871,  2.0197, -0.3445, -3.2034],
        [ 1.6874,  2.9550, -2.2533,  1.4811, -0.7905,  3.4437, -0.4225, -2.9259],
        [ 1.5155,  2.8882, -2.2154,  2.1116, -0.7689,  3.3857, -0.9542, -2.9398],
        [ 1.2824,  2.0021, -2.3125, -0.0436, -1.7533,  1.8547,  0.1626, -2.5965],
        [ 2.3989,  4.2539, -3.1938,  3.4578, -0.3676,  3.9294, -1.9757, -1.8666],
        [ 1.8904,  3.0088, -2.8260,  1.4942, -1.0015,  2.4217, -2.0045, -1.1551],
        [ 2.0115,  3.4998, -2.4757,  0.6869, -2.4338,  2.0524, -0.3073, -1.9377]])...
	Related Layers:
		- parent layers: layernorm_7_323
		- child layers: transpose_30_386
		- shares parents with layers: linear_82_324, linear_83_325, linear_84_334, linear_85_336, linear_86_337, linear_87_346, linear_88_348, linear_89_349, linear_90_358, linear_91_360, linear_92_361, linear_93_370, linear_94_372, linear_95_373, linear_96_382, linear_98_385, linear_99_394, linear_100_396, linear_101_397, linear_102_406, linear_103_408, linear_104_409, linear_105_418
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.5.key:1
	Time elapsed:  1.132E-04s
	Output of modules: blocks.3.sa.heads.5.key
	Output of bottom-level module: blocks.3.sa.heads.5.key:1
	Lookup keys: -282, 416, blocks.3.sa.heads.5.key, blocks.3.sa.heads.5.key:1, linear_97, linear_97:1, linear_97_384, linear_97_384:1
--------------------------------------------
Layer linear_98_385, operation 388/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 2.6703e-02, -9.4705e-01, -2.9673e-02,  5.2817e-01,  8.3750e-01,
         -5.7942e-01, -2.0221e-03, -1.3258e-01],
        [-8.8526e-01,  9.1638e-01, -1.7552e+00,  2.2191e+00, -5.9352e-01,
          1.0939e+00, -3.5923e-01,  5.3973e-01],
        [-3.8521e-01,  6.0319e-01, -7.7156e-01,  1.2200e+00, -1.7610e+00,
         -4.4063e-01, -9.7789e-01,  5.7047e-01],
        [-6.9175e-01,  9.9475e-01, -5.0778e-01,  1.1143e+00, -1.9574e+00,
         -7.8430e-01, -5.6180e-01,  3.6908e-01],
        [-8.1018e-02, -5.7837e-01,  4.3057e-01,  8.3267e-01,  3.6797e-01,
         -8.0904e-01,  5.1479e-01,  2.2036e-01],
        [-2.1499e+00, -1.1245e-01,  1.1063e-01,  1.9178e+00,  5.3867e-01,
          1.8472e+00, -2.5880e+00, -2.9863e-01],
        [-1.4408e+00,  7.0899e-02,  5.3864e-01,  1.1112e+00,  1.0691e+00,
          9.4307e-01, -2.7013e+00,  2.8615e-01],
        [-2.2237e+00, -3.8251e-01, -3.4708e-01, -4.9767e-01, -6.1171e-02,
         -1.2628e+00, -1.0949e+00,  1.7756e+00]])...
	Related Layers:
		- parent layers: layernorm_7_323
		- child layers: matmul_59_387
		- shares parents with layers: linear_82_324, linear_83_325, linear_84_334, linear_85_336, linear_86_337, linear_87_346, linear_88_348, linear_89_349, linear_90_358, linear_91_360, linear_92_361, linear_93_370, linear_94_372, linear_95_373, linear_96_382, linear_97_384, linear_99_394, linear_100_396, linear_101_397, linear_102_406, linear_103_408, linear_104_409, linear_105_418
		- shares children with layers: transpose_30_386
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.5.query:1
	Time elapsed:  1.109E-04s
	Output of modules: blocks.3.sa.heads.5.query
	Output of bottom-level module: blocks.3.sa.heads.5.query:1
	Lookup keys: -281, 417, blocks.3.sa.heads.5.query, blocks.3.sa.heads.5.query:1, linear_98, linear_98:1, linear_98_385, linear_98_385:1
--------------------------------------------
Layer transpose_30_386, operation 389/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 2.0863,  2.1013,  1.6874,  1.5155,  1.2824,  2.3989,  1.8904,  2.0115],
        [ 2.9227,  2.8277,  2.9550,  2.8882,  2.0021,  4.2539,  3.0088,  3.4998],
        [-3.4699, -3.3662, -2.2533, -2.2154, -2.3125, -3.1938, -2.8260, -2.4757],
        [ 0.5425,  1.1498,  1.4811,  2.1116, -0.0436,  3.4578,  1.4942,  0.6869],
        [-1.6634, -1.5871, -0.7905, -0.7689, -1.7533, -0.3676, -1.0015, -2.4338],
        [ 3.1332,  2.0197,  3.4437,  3.3857,  1.8547,  3.9294,  2.4217,  2.0524],
        [-0.6897, -0.3445, -0.4225, -0.9542,  0.1626, -1.9757, -2.0045, -0.3073],
        [-2.9773, -3.2034, -2.9259, -2.9398, -2.5965, -1.8666, -1.1551, -1.9377]])...
	Related Layers:
		- parent layers: linear_97_384
		- child layers: matmul_59_387
		- shares parents with no other layers
		- shares children with layers: linear_98_385
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.3.sa.heads.5:1
	Time elapsed:  6.270E-05s
	Output of modules: none
	Lookup keys: -280, 418, transpose_30, transpose_30:1, transpose_30_386, transpose_30_386:1
--------------------------------------------
Layer matmul_59_387, operation 390/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -48.2079,  -40.2487,  -47.4405,  -46.1300,  -42.5375,  -38.9763,
          -26.2154,  -41.8758],
        [  76.8407,   79.1199,   70.5845,   71.0002,   38.4460,  139.4771,
          106.3842,   83.6367],
        [  -5.9170,   13.1231,   -0.5729,    1.5580,   -9.9152,   19.2065,
           28.6564,   29.8986],
        [  -6.2645,   15.7688,    0.1468,    2.1769,   -7.5136,   16.0498,
           27.4639,   33.3181],
        [ -68.1877,  -55.4912,  -64.1450,  -62.9752,  -58.4281,  -53.1469,
          -36.4960,  -52.6958],
        [ -13.2842,   -2.3814,   18.6311,   28.5922,  -12.7106,   19.5219,
           24.1745,   21.5226],
        [ -57.9712,  -46.9364,  -30.1145,  -19.1306,  -42.0202,  -51.9102,
          -26.9968,  -28.5090],
        [-108.0910,  -98.4291, -107.6610, -101.9057,  -76.3138, -129.5774,
          -87.7773,  -92.0862]])...
	Related Layers:
		- parent layers: linear_98_385, transpose_30_386
		- child layers: mul_30_388
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.5:1
	Time elapsed:  8.821E-05s
	Output of modules: none
	Lookup keys: -279, 419, matmul_59, matmul_59:1, matmul_59_387, matmul_59_387:1
--------------------------------------------
Layer mul_30_388, operation 391/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-2.4601, -2.0539, -2.4209, -2.3541, -2.1707, -1.9890, -1.3378, -2.1370],
        [ 3.9213,  4.0376,  3.6020,  3.6232,  1.9619,  7.1177,  5.4289,  4.2681],
        [-0.3020,  0.6697, -0.0292,  0.0795, -0.5060,  0.9801,  1.4624,  1.5258],
        [-0.3197,  0.8047,  0.0075,  0.1111, -0.3834,  0.8190,  1.4015,  1.7003],
        [-3.4797, -2.8318, -3.2734, -3.2137, -2.9816, -2.7121, -1.8624, -2.6891],
        [-0.6779, -0.1215,  0.9508,  1.4591, -0.6486,  0.9962,  1.2336,  1.0983],
        [-2.9583, -2.3952, -1.5368, -0.9763, -2.1443, -2.6490, -1.3777, -1.4548],
        [-5.5160, -5.0229, -5.4941, -5.2004, -3.8944, -6.6125, -4.4794, -4.6993]])...
	Related Layers:
		- parent layers: matmul_59_387
		- child layers: maskedfill_30_391
		- shares parents with no other layers
		- shares children with layers: eq_30_390
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.3.sa.heads.5:1
	Time elapsed:  7.010E-05s
	Output of modules: none
	Lookup keys: -278, 420, mul_30, mul_30:1, mul_30_388, mul_30_388:1
--------------------------------------------
Layer getitem_30_389, operation 392/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_30
		- child layers: eq_30_390
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.346E-05s
	Output of modules: none
	Lookup keys: -276, 422, getitem_30, getitem_30:1, getitem_30_389, getitem_30_389:1
--------------------------------------------
Layer eq_30_390, operation 393/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_30_389
		- child layers: maskedfill_30_391
		- shares parents with no other layers
		- shares children with layers: mul_30_388
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.273E-05s
	Output of modules: none
	Lookup keys: -275, 423, eq_30, eq_30:1, eq_30_390, eq_30_390:1
--------------------------------------------
Layer maskedfill_30_391, operation 394/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-2.4601,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [ 3.9213,  4.0376,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-0.3020,  0.6697, -0.0292,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-0.3197,  0.8047,  0.0075,  0.1111,    -inf,    -inf,    -inf,    -inf],
        [-3.4797, -2.8318, -3.2734, -3.2137, -2.9816,    -inf,    -inf,    -inf],
        [-0.6779, -0.1215,  0.9508,  1.4591, -0.6486,  0.9962,    -inf,    -inf],
        [-2.9583, -2.3952, -1.5368, -0.9763, -2.1443, -2.6490, -1.3777,    -inf],
        [-5.5160, -5.0229, -5.4941, -5.2004, -3.8944, -6.6125, -4.4794, -4.6993]])...
	Related Layers:
		- parent layers: mul_30_388, eq_30_390
		- child layers: softmax_30_392
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.3.sa.heads.5:1
	Time elapsed:  8.297E-05s
	Output of modules: none
	Lookup keys: -274, 424, maskedfill_30, maskedfill_30:1, maskedfill_30_391, maskedfill_30_391:1
--------------------------------------------
Layer softmax_30_392, operation 395/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.4710, 0.5290, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2018, 0.5332, 0.2650, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1428, 0.4395, 0.1980, 0.2197, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1410, 0.2696, 0.1733, 0.1840, 0.2321, 0.0000, 0.0000, 0.0000],
        [0.0441, 0.0769, 0.2247, 0.3736, 0.0454, 0.2352, 0.0000, 0.0000],
        [0.0442, 0.0776, 0.1831, 0.3206, 0.0997, 0.0602, 0.2146, 0.0000],
        [0.0645, 0.1056, 0.0659, 0.0884, 0.3264, 0.0215, 0.1818, 0.1459]])...
	Related Layers:
		- parent layers: maskedfill_30_391
		- child layers: dropout_36_393
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.3.sa.heads.5:1
	Time elapsed:  6.914E-05s
	Output of modules: none
	Lookup keys: -273, 425, softmax_30, softmax_30:1, softmax_30_392, softmax_30_392:1
--------------------------------------------
Layer dropout_36_393, operation 396/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.4710, 0.5290, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2018, 0.5332, 0.2650, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1428, 0.4395, 0.1980, 0.2197, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1410, 0.2696, 0.1733, 0.1840, 0.2321, 0.0000, 0.0000, 0.0000],
        [0.0441, 0.0769, 0.2247, 0.3736, 0.0454, 0.2352, 0.0000, 0.0000],
        [0.0442, 0.0776, 0.1831, 0.3206, 0.0997, 0.0602, 0.2146, 0.0000],
        [0.0645, 0.1056, 0.0659, 0.0884, 0.3264, 0.0215, 0.1818, 0.1459]])...
	Related Layers:
		- parent layers: softmax_30_392
		- child layers: matmul_60_395
		- shares parents with no other layers
		- shares children with layers: linear_99_394
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.3.sa.heads.5.dropout:1
	Time elapsed:  5.651E-05s
	Output of modules: blocks.3.sa.heads.5.dropout
	Output of bottom-level module: blocks.3.sa.heads.5.dropout:1
	Lookup keys: -272, 426, blocks.3.sa.heads.5.dropout, blocks.3.sa.heads.5.dropout:1, dropout_36, dropout_36:1, dropout_36_393, dropout_36_393:1
--------------------------------------------
Layer linear_99_394, operation 397/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.0407,  0.1344,  0.0608,  0.0828,  0.1136, -0.3001, -0.0876,  0.1587],
        [ 0.5494, -0.0498,  0.3652,  0.1352,  1.3112, -0.3511, -1.0529,  0.0821],
        [ 0.5563, -0.4827, -0.5173, -0.0363,  0.4193,  0.3053, -0.8716, -0.0275],
        [ 0.2962, -0.4238, -0.1859, -0.3972, -0.1495,  0.1769, -0.8528, -0.2175],
        [-0.0874, -0.0088,  0.3114,  0.0361, -0.1076, -0.7011, -0.2052, -0.0726],
        [ 0.9524,  0.8248,  0.2295, -0.2264,  1.9620, -0.6744, -1.4549,  0.3020],
        [ 0.6289,  0.0417,  0.4557, -1.0651,  1.9008,  0.4640, -1.8228, -0.3855],
        [ 0.7408, -0.2418,  0.2378,  0.5568,  0.4876, -0.0439, -0.2575,  0.2072]])...
	Related Layers:
		- parent layers: layernorm_7_323
		- child layers: matmul_60_395
		- shares parents with layers: linear_82_324, linear_83_325, linear_84_334, linear_85_336, linear_86_337, linear_87_346, linear_88_348, linear_89_349, linear_90_358, linear_91_360, linear_92_361, linear_93_370, linear_94_372, linear_95_373, linear_96_382, linear_97_384, linear_98_385, linear_100_396, linear_101_397, linear_102_406, linear_103_408, linear_104_409, linear_105_418
		- shares children with layers: dropout_36_393
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.5.value:1
	Time elapsed:  1.168E-04s
	Output of modules: blocks.3.sa.heads.5.value
	Output of bottom-level module: blocks.3.sa.heads.5.value:1
	Lookup keys: -271, 427, blocks.3.sa.heads.5.value, blocks.3.sa.heads.5.value:1, linear_99, linear_99:1, linear_99_394, linear_99_394:1
--------------------------------------------
Layer matmul_60_395, operation 398/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.0407,  0.1344,  0.0608,  0.0828,  0.1136, -0.3001, -0.0876,  0.1587],
        [ 0.3098,  0.0370,  0.2219,  0.1105,  0.7472, -0.3271, -0.5983,  0.1182],
        [ 0.4486, -0.1273,  0.0699,  0.0792,  0.8331, -0.1668, -0.8100,  0.0685],
        [ 0.4225, -0.1914,  0.0259, -0.0232,  0.6427, -0.0978, -0.8352,  0.0055],
        [ 0.2845, -0.1581,  0.0554, -0.0229,  0.3897, -0.2142, -0.6518, -0.0171],
        [ 0.4998, -0.0711, -0.0868, -0.1941,  0.6008, -0.0960, -0.9509, -0.0064],
        [ 0.4248, -0.1644,  0.0194, -0.3585,  0.6509,  0.0612, -1.0178, -0.1332],
        [ 0.3379, -0.0787,  0.2161, -0.1234,  0.5841, -0.1861, -0.7169, -0.0592]])...
	Related Layers:
		- parent layers: dropout_36_393, linear_99_394
		- child layers: cat_4_420
		- shares parents with no other layers
		- shares children with layers: matmul_50_335, matmul_52_347, matmul_54_359, matmul_56_371, matmul_58_383, matmul_62_407, matmul_64_419
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.5:1
	Time elapsed:  8.225E-05s
	Output of modules: blocks.3.sa.heads.5
	Lookup keys: -270, 428, blocks.3.sa.heads.5, blocks.3.sa.heads.5:1, matmul_60, matmul_60:1, matmul_60_395, matmul_60_395:1
--------------------------------------------
Layer linear_100_396, operation 399/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.4363,  0.1968,  2.5996, -0.3884,  1.7173,  1.5369, -1.4855,  2.5537],
        [-1.7196, -1.7681,  3.4168, -1.2958,  1.9830,  3.4536, -3.0866,  2.6771],
        [ 0.7170,  0.5623,  1.7780, -0.9995,  2.1555,  1.1987, -1.9626,  2.0639],
        [ 0.9848,  0.5997,  2.3446, -1.0632,  1.8383,  1.1746, -1.2660,  1.2889],
        [ 0.2396,  1.2120,  1.6978, -0.4914,  1.1001,  0.4330, -0.9186,  1.3091],
        [-0.8185, -2.3434,  3.7868, -2.6220,  2.4123,  4.3856, -3.2695,  3.1790],
        [ 0.1118, -1.8514,  3.3638, -1.7001,  2.1773,  3.3387, -3.4149,  2.2252],
        [-0.0755,  0.1822,  1.7917, -0.5975,  1.4565,  0.6146, -1.6880,  0.9329]])...
	Related Layers:
		- parent layers: layernorm_7_323
		- child layers: transpose_31_398
		- shares parents with layers: linear_82_324, linear_83_325, linear_84_334, linear_85_336, linear_86_337, linear_87_346, linear_88_348, linear_89_349, linear_90_358, linear_91_360, linear_92_361, linear_93_370, linear_94_372, linear_95_373, linear_96_382, linear_97_384, linear_98_385, linear_99_394, linear_101_397, linear_102_406, linear_103_408, linear_104_409, linear_105_418
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.6.key:1
	Time elapsed:  1.137E-04s
	Output of modules: blocks.3.sa.heads.6.key
	Output of bottom-level module: blocks.3.sa.heads.6.key:1
	Lookup keys: -269, 429, blocks.3.sa.heads.6.key, blocks.3.sa.heads.6.key:1, linear_100, linear_100:1, linear_100_396, linear_100_396:1
--------------------------------------------
Layer linear_101_397, operation 400/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.9988, -0.0153, -0.6511, -0.7726, -0.1967, -0.7936,  0.3982, -0.3938],
        [ 0.7728,  0.7476, -0.2798, -1.7927,  0.4127, -0.8433,  0.6395, -0.6615],
        [ 0.7778, -0.3992, -0.3201, -1.1072,  0.2577, -1.1979,  0.1431, -0.7395],
        [ 1.1155, -0.3571,  0.2962, -1.3220,  0.4006, -0.9783, -0.0588, -0.1847],
        [ 2.1613,  0.4540, -0.1256, -1.8660,  0.1394, -0.7742, -0.3216, -0.0053],
        [ 0.3963,  0.1705, -2.2761,  0.8387, -2.1498, -0.8284,  2.1290, -2.1198],
        [-0.6845, -0.8366, -1.3919,  0.5517, -0.8505, -0.1937,  1.1013, -1.0847],
        [ 0.4534, -0.8808,  0.1971, -1.5791,  0.6696,  0.0671, -0.9743,  0.5177]])...
	Related Layers:
		- parent layers: layernorm_7_323
		- child layers: matmul_61_399
		- shares parents with layers: linear_82_324, linear_83_325, linear_84_334, linear_85_336, linear_86_337, linear_87_346, linear_88_348, linear_89_349, linear_90_358, linear_91_360, linear_92_361, linear_93_370, linear_94_372, linear_95_373, linear_96_382, linear_97_384, linear_98_385, linear_99_394, linear_100_396, linear_102_406, linear_103_408, linear_104_409, linear_105_418
		- shares children with layers: transpose_31_398
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.6.query:1
	Time elapsed:  1.135E-04s
	Output of modules: blocks.3.sa.heads.6.query
	Output of bottom-level module: blocks.3.sa.heads.6.query:1
	Lookup keys: -268, 430, blocks.3.sa.heads.6.query, blocks.3.sa.heads.6.query:1, linear_101, linear_101:1, linear_101_397, linear_101_397:1
--------------------------------------------
Layer transpose_31_398, operation 401/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.4363, -1.7196,  0.7170,  0.9848,  0.2396, -0.8185,  0.1118, -0.0755],
        [ 0.1968, -1.7681,  0.5623,  0.5997,  1.2120, -2.3434, -1.8514,  0.1822],
        [ 2.5996,  3.4168,  1.7780,  2.3446,  1.6978,  3.7868,  3.3638,  1.7917],
        [-0.3884, -1.2958, -0.9995, -1.0632, -0.4914, -2.6220, -1.7001, -0.5975],
        [ 1.7173,  1.9830,  2.1555,  1.8383,  1.1001,  2.4123,  2.1773,  1.4565],
        [ 1.5369,  3.4536,  1.1987,  1.1746,  0.4330,  4.3856,  3.3387,  0.6146],
        [-1.4855, -3.0866, -1.9626, -1.2660, -0.9186, -3.2695, -3.4149, -1.6880],
        [ 2.5537,  2.6771,  2.0639,  1.2889,  1.3091,  3.1790,  2.2252,  0.9329]])...
	Related Layers:
		- parent layers: linear_100_396
		- child layers: matmul_61_399
		- shares parents with no other layers
		- shares children with layers: linear_101_397
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.3.sa.heads.6:1
	Time elapsed:  5.770E-05s
	Output of modules: none
	Lookup keys: -267, 431, transpose_31, transpose_31:1, transpose_31_398, transpose_31_398:1
--------------------------------------------
Layer matmul_61_399, operation 402/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -41.3696,  -47.1610,  -24.5722,  -23.5788,  -22.8130,  -42.4792,
          -35.9260,  -16.4989],
        [ -25.8934,  -39.0511,  -17.5876,  -15.5562,  -11.3906,  -34.3878,
          -28.2679,  -10.3843],
        [ -51.4845,  -44.9558,  -30.8901,  -29.8998,  -31.9565,  -36.9123,
          -28.5117,  -18.3205],
        [ -33.6277,  -20.3960,  -16.6320,  -15.7453,  -22.7623,  -10.9823,
           -8.5241,   -9.0868],
        [ -33.4154,  -20.8308,   -0.4527,    0.7100,  -16.6422,  -12.4077,
          -12.9274,   -4.7366],
        [-109.9442, -159.7919, -114.4255, -112.5715,  -71.1735, -156.9237,
         -115.1600,  -72.1373],
        [ -48.6373,  -72.3678,  -63.1045,  -61.3946,  -36.4891,  -68.2452,
          -54.0767,  -39.5873],
        [   1.4317,   27.0785,    8.5630,    8.2549,   -4.8774,   41.7187,
           36.2440,    7.9154]])...
	Related Layers:
		- parent layers: linear_101_397, transpose_31_398
		- child layers: mul_31_400
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.6:1
	Time elapsed:  8.726E-05s
	Output of modules: none
	Lookup keys: -266, 432, matmul_61, matmul_61:1, matmul_61_399, matmul_61_399:1
--------------------------------------------
Layer mul_31_400, operation 403/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-2.1111, -2.4067, -1.2539, -1.2032, -1.1642, -2.1678, -1.8333, -0.8420],
        [-1.3214, -1.9928, -0.8975, -0.7938, -0.5813, -1.7548, -1.4425, -0.5299],
        [-2.6273, -2.2941, -1.5764, -1.5258, -1.6308, -1.8837, -1.4550, -0.9349],
        [-1.7161, -1.0408, -0.8487, -0.8035, -1.1616, -0.5604, -0.4350, -0.4637],
        [-1.7052, -1.0630, -0.0231,  0.0362, -0.8493, -0.6332, -0.6597, -0.2417],
        [-5.6106, -8.1543, -5.8393, -5.7446, -3.6321, -8.0080, -5.8767, -3.6812],
        [-2.4820, -3.6930, -3.2203, -3.1330, -1.8621, -3.4826, -2.7596, -2.0202],
        [ 0.0731,  1.3818,  0.4370,  0.4213, -0.2489,  2.1289,  1.8496,  0.4039]])...
	Related Layers:
		- parent layers: matmul_61_399
		- child layers: maskedfill_31_403
		- shares parents with no other layers
		- shares children with layers: eq_31_402
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.3.sa.heads.6:1
	Time elapsed:  6.843E-05s
	Output of modules: none
	Lookup keys: -265, 433, mul_31, mul_31:1, mul_31_400, mul_31_400:1
--------------------------------------------
Layer getitem_31_401, operation 404/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_31
		- child layers: eq_31_402
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  1.004E-04s
	Output of modules: none
	Lookup keys: -263, 435, getitem_31, getitem_31:1, getitem_31_401, getitem_31_401:1
--------------------------------------------
Layer eq_31_402, operation 405/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_31_401
		- child layers: maskedfill_31_403
		- shares parents with no other layers
		- shares children with layers: mul_31_400
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  7.677E-05s
	Output of modules: none
	Lookup keys: -262, 436, eq_31, eq_31:1, eq_31_402, eq_31_402:1
--------------------------------------------
Layer maskedfill_31_403, operation 406/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-2.1111,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-1.3214, -1.9928,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-2.6273, -2.2941, -1.5764,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-1.7161, -1.0408, -0.8487, -0.8035,    -inf,    -inf,    -inf,    -inf],
        [-1.7052, -1.0630, -0.0231,  0.0362, -0.8493,    -inf,    -inf,    -inf],
        [-5.6106, -8.1543, -5.8393, -5.7446, -3.6321, -8.0080,    -inf,    -inf],
        [-2.4820, -3.6930, -3.2203, -3.1330, -1.8621, -3.4826, -2.7596,    -inf],
        [ 0.0731,  1.3818,  0.4370,  0.4213, -0.2489,  2.1289,  1.8496,  0.4039]])...
	Related Layers:
		- parent layers: mul_31_400, eq_31_402
		- child layers: softmax_31_404
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.3.sa.heads.6:1
	Time elapsed:  8.345E-05s
	Output of modules: none
	Lookup keys: -261, 437, maskedfill_31, maskedfill_31:1, maskedfill_31_403, maskedfill_31_403:1
--------------------------------------------
Layer softmax_31_404, operation 407/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.6618, 0.3382, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1903, 0.2655, 0.5442, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1276, 0.2507, 0.3038, 0.3179, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0612, 0.1163, 0.3291, 0.3492, 0.1441, 0.0000, 0.0000, 0.0000],
        [0.0993, 0.0078, 0.0790, 0.0868, 0.7181, 0.0090, 0.0000, 0.0000],
        [0.1893, 0.0564, 0.0905, 0.0987, 0.3520, 0.0696, 0.1434, 0.0000],
        [0.0427, 0.1582, 0.0615, 0.0605, 0.0310, 0.3340, 0.2526, 0.0595]])...
	Related Layers:
		- parent layers: maskedfill_31_403
		- child layers: dropout_37_405
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.3.sa.heads.6:1
	Time elapsed:  7.224E-05s
	Output of modules: none
	Lookup keys: -260, 438, softmax_31, softmax_31:1, softmax_31_404, softmax_31_404:1
--------------------------------------------
Layer dropout_37_405, operation 408/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.6618, 0.3382, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1903, 0.2655, 0.5442, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1276, 0.2507, 0.3038, 0.3179, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0612, 0.1163, 0.3291, 0.3492, 0.1441, 0.0000, 0.0000, 0.0000],
        [0.0993, 0.0078, 0.0790, 0.0868, 0.7181, 0.0090, 0.0000, 0.0000],
        [0.1893, 0.0564, 0.0905, 0.0987, 0.3520, 0.0696, 0.1434, 0.0000],
        [0.0427, 0.1582, 0.0615, 0.0605, 0.0310, 0.3340, 0.2526, 0.0595]])...
	Related Layers:
		- parent layers: softmax_31_404
		- child layers: matmul_62_407
		- shares parents with no other layers
		- shares children with layers: linear_102_406
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.3.sa.heads.6.dropout:1
	Time elapsed:  5.460E-05s
	Output of modules: blocks.3.sa.heads.6.dropout
	Output of bottom-level module: blocks.3.sa.heads.6.dropout:1
	Lookup keys: -259, 439, blocks.3.sa.heads.6.dropout, blocks.3.sa.heads.6.dropout:1, dropout_37, dropout_37:1, dropout_37_405, dropout_37_405:1
--------------------------------------------
Layer linear_102_406, operation 409/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 4.4063e-01, -3.5769e-01,  3.4446e-02, -4.2564e-01,  5.1779e-01,
          7.7969e-02,  1.8480e+00, -3.0087e-01],
        [ 2.2164e-01, -5.8492e-01, -3.6097e-01, -1.0508e+00,  3.2942e-01,
         -1.1605e-01,  1.5701e+00,  4.7669e-02],
        [-2.9614e-01, -8.5913e-01, -2.2355e-01, -1.1789e-01, -1.1413e-03,
         -6.8514e-02,  9.7211e-01, -5.7358e-01],
        [-2.4162e-01, -8.2380e-01, -9.5260e-02,  1.1217e-01,  4.2616e-02,
         -5.1797e-01,  5.4511e-01, -1.3287e-01],
        [ 1.4174e-01, -3.1539e-01, -2.2699e-01, -3.1089e-01,  1.7582e-01,
         -4.4657e-01,  2.0439e+00, -8.2427e-01],
        [ 8.4950e-01,  1.7848e-01, -1.9770e-01, -5.6447e-01,  6.6053e-01,
          4.5236e-01, -1.0602e-01, -2.6619e-01],
        [-1.6927e-01, -5.3412e-01, -1.6948e-01, -2.9608e-01,  2.0174e-01,
          8.3187e-01,  7.9815e-01, -8.2481e-01],
        [-4.4479e-01, -5.7458e-01, -1.4559e-01, -3.7332e-01, -1.0282e-01,
         -7.2993e-01,  1.2669e+00, -1.1152e+00]])...
	Related Layers:
		- parent layers: layernorm_7_323
		- child layers: matmul_62_407
		- shares parents with layers: linear_82_324, linear_83_325, linear_84_334, linear_85_336, linear_86_337, linear_87_346, linear_88_348, linear_89_349, linear_90_358, linear_91_360, linear_92_361, linear_93_370, linear_94_372, linear_95_373, linear_96_382, linear_97_384, linear_98_385, linear_99_394, linear_100_396, linear_101_397, linear_103_408, linear_104_409, linear_105_418
		- shares children with layers: dropout_37_405
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.6.value:1
	Time elapsed:  1.209E-04s
	Output of modules: blocks.3.sa.heads.6.value
	Output of bottom-level module: blocks.3.sa.heads.6.value:1
	Lookup keys: -258, 440, blocks.3.sa.heads.6.value, blocks.3.sa.heads.6.value:1, linear_102, linear_102:1, linear_102_406, linear_102_406:1
--------------------------------------------
Layer matmul_62_407, operation 410/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.4406, -0.3577,  0.0344, -0.4256,  0.5178,  0.0780,  1.8480, -0.3009],
        [ 0.3666, -0.4345, -0.0993, -0.6371,  0.4541,  0.0124,  1.7540, -0.1830],
        [-0.0185, -0.6909, -0.2109, -0.4241,  0.1854, -0.0533,  1.2975, -0.3568],
        [-0.0550, -0.7152, -0.1843, -0.3179,  0.1619, -0.2046,  1.0981, -0.2429],
        [-0.1087, -0.7059, -0.1794, -0.1927,  0.1099, -0.2765,  1.1006, -0.3668],
        [ 0.1106, -0.4043, -0.1901, -0.2784,  0.1898, -0.3601,  1.7865, -0.6806],
        [ 0.1300, -0.4350, -0.1614, -0.3306,  0.2575, -0.0555,  1.4067, -0.5463],
        [ 0.2399, -0.3298, -0.1997, -0.4800,  0.3476,  0.2533,  0.7250, -0.4377]])...
	Related Layers:
		- parent layers: dropout_37_405, linear_102_406
		- child layers: cat_4_420
		- shares parents with no other layers
		- shares children with layers: matmul_50_335, matmul_52_347, matmul_54_359, matmul_56_371, matmul_58_383, matmul_60_395, matmul_64_419
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.6:1
	Time elapsed:  8.368E-05s
	Output of modules: blocks.3.sa.heads.6
	Lookup keys: -257, 441, blocks.3.sa.heads.6, blocks.3.sa.heads.6:1, matmul_62, matmul_62:1, matmul_62_407, matmul_62_407:1
--------------------------------------------
Layer linear_103_408, operation 411/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.8119,  3.1235,  1.1489, -3.1029,  4.2953,  3.1555, -0.9776, -2.0301],
        [ 2.2054,  2.9059,  2.0709, -3.7029,  4.1356,  3.2751,  0.7535, -4.1138],
        [ 1.7491,  3.2240,  2.0845, -2.4004,  3.8506,  2.7878, -0.9434, -1.8591],
        [ 1.5720,  2.9939,  2.0325, -1.8900,  3.5755,  2.9876, -1.0785, -1.6057],
        [ 1.7100,  1.6194,  0.6482, -1.8586,  3.3456,  2.6971, -1.2923, -0.5568],
        [ 2.6567,  4.5329,  2.4619, -2.5486,  4.0947,  2.5615, -1.5970, -4.0820],
        [ 2.3027,  2.9847,  0.7740, -2.9641,  3.7274,  2.9151, -0.6341, -3.8969],
        [ 1.2765,  2.9745,  1.0523, -2.1593,  4.0699,  3.1010, -1.0659, -2.8429]])...
	Related Layers:
		- parent layers: layernorm_7_323
		- child layers: transpose_32_410
		- shares parents with layers: linear_82_324, linear_83_325, linear_84_334, linear_85_336, linear_86_337, linear_87_346, linear_88_348, linear_89_349, linear_90_358, linear_91_360, linear_92_361, linear_93_370, linear_94_372, linear_95_373, linear_96_382, linear_97_384, linear_98_385, linear_99_394, linear_100_396, linear_101_397, linear_102_406, linear_104_409, linear_105_418
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.7.key:1
	Time elapsed:  1.137E-04s
	Output of modules: blocks.3.sa.heads.7.key
	Output of bottom-level module: blocks.3.sa.heads.7.key:1
	Lookup keys: -256, 442, blocks.3.sa.heads.7.key, blocks.3.sa.heads.7.key:1, linear_103, linear_103:1, linear_103_408, linear_103_408:1
--------------------------------------------
Layer linear_104_409, operation 412/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.1629,  0.1994,  0.5498, -0.5980, -0.1704, -0.1187, -0.7715, -0.4854],
        [ 2.1144, -0.0109,  0.3646,  0.0714, -0.2074,  1.2146, -0.2439, -0.2331],
        [-0.0657, -0.3114,  0.6529,  0.3021, -0.2792,  0.1504, -1.7677,  0.6981],
        [-0.1399,  0.4115,  0.8446,  0.3707,  0.1308,  0.2278, -1.7278,  0.4401],
        [ 0.6692,  0.0776,  0.8540, -0.1861, -0.4267, -0.0434,  0.3133, -0.7246],
        [ 0.3454, -0.7007,  0.0275,  0.9132, -0.8946,  0.8803, -2.0620,  2.3730],
        [ 0.8073, -1.1978,  0.3772,  1.1287, -1.7888,  0.5716, -2.0578,  2.1325],
        [ 0.6552, -0.6779, -0.2951, -0.0503, -2.1076, -0.6811,  1.1835,  0.6837]])...
	Related Layers:
		- parent layers: layernorm_7_323
		- child layers: matmul_63_411
		- shares parents with layers: linear_82_324, linear_83_325, linear_84_334, linear_85_336, linear_86_337, linear_87_346, linear_88_348, linear_89_349, linear_90_358, linear_91_360, linear_92_361, linear_93_370, linear_94_372, linear_95_373, linear_96_382, linear_97_384, linear_98_385, linear_99_394, linear_100_396, linear_101_397, linear_102_406, linear_103_408, linear_105_418
		- shares children with layers: transpose_32_410
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.7.query:1
	Time elapsed:  1.130E-04s
	Output of modules: blocks.3.sa.heads.7.query
	Output of bottom-level module: blocks.3.sa.heads.7.query:1
	Lookup keys: -255, 443, blocks.3.sa.heads.7.query, blocks.3.sa.heads.7.query:1, linear_104, linear_104:1, linear_104_409, linear_104_409:1
--------------------------------------------
Layer transpose_32_410, operation 413/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.8119,  2.2054,  1.7491,  1.5720,  1.7100,  2.6567,  2.3027,  1.2765],
        [ 3.1235,  2.9059,  3.2240,  2.9939,  1.6194,  4.5329,  2.9847,  2.9745],
        [ 1.1489,  2.0709,  2.0845,  2.0325,  0.6482,  2.4619,  0.7740,  1.0523],
        [-3.1029, -3.7029, -2.4004, -1.8900, -1.8586, -2.5486, -2.9641, -2.1593],
        [ 4.2953,  4.1356,  3.8506,  3.5755,  3.3456,  4.0947,  3.7274,  4.0699],
        [ 3.1555,  3.2751,  2.7878,  2.9876,  2.6971,  2.5615,  2.9151,  3.1010],
        [-0.9776,  0.7535, -0.9434, -1.0785, -1.2923, -1.5970, -0.6341, -1.0659],
        [-2.0301, -4.1138, -1.8591, -1.6057, -0.5568, -4.0820, -3.8969, -2.8429]])...
	Related Layers:
		- parent layers: linear_103_408
		- child layers: matmul_63_411
		- shares parents with no other layers
		- shares children with layers: linear_104_409
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.3.sa.heads.7:1
	Time elapsed:  6.127E-05s
	Output of modules: none
	Lookup keys: -254, 444, transpose_32, transpose_32:1, transpose_32_410, transpose_32_410:1
--------------------------------------------
Layer matmul_63_411, operation 414/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[  38.4490,   51.7544,   30.0099,   27.4974,   21.0063,   56.0903,
           49.5298,   39.9980],
        [  39.7079,   39.6925,   34.0080,   33.3099,   28.5115,   54.7724,
           41.0881,   38.3719],
        [   2.3644,    0.4885,    2.7156,    4.9950,    1.1115,    5.6377,
            2.7478,    4.7098],
        [  16.6867,    9.3976,   15.1012,   17.3501,    9.7461,   26.6622,
           14.1626,   16.4676],
        [  16.5016,   35.3580,    9.6646,    7.1685,    2.0058,   36.3710,
           33.3756,   20.8839],
        [ -72.7062,  -97.5313,  -44.3798,  -33.6478,  -32.4025,  -96.9874,
          -86.7176,  -67.1590],
        [ -73.7392, -120.2573,  -45.4321,  -33.2633,  -35.1791,  -91.8254,
          -97.5799,  -76.3721],
        [ -83.4490,  -51.2056,  -67.5631,  -64.3217,  -54.2780, -101.5925,
          -64.9585,  -61.3474]])...
	Related Layers:
		- parent layers: linear_104_409, transpose_32_410
		- child layers: mul_32_412
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.7:1
	Time elapsed:  9.179E-05s
	Output of modules: none
	Lookup keys: -253, 445, matmul_63, matmul_63:1, matmul_63_411, matmul_63_411:1
--------------------------------------------
Layer mul_32_412, operation 415/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ 1.9621,  2.6411,  1.5314,  1.4032,  1.0720,  2.8623,  2.5276,  2.0411],
        [ 2.0263,  2.0256,  1.7355,  1.6998,  1.4550,  2.7951,  2.0968,  1.9582],
        [ 0.1207,  0.0249,  0.1386,  0.2549,  0.0567,  0.2877,  0.1402,  0.2403],
        [ 0.8515,  0.4796,  0.7706,  0.8854,  0.4974,  1.3606,  0.7227,  0.8404],
        [ 0.8421,  1.8044,  0.4932,  0.3658,  0.1024,  1.8561,  1.7032,  1.0657],
        [-3.7103, -4.9771, -2.2647, -1.7171, -1.6535, -4.9494, -4.4253, -3.4272],
        [-3.7630, -6.1369, -2.3184, -1.6975, -1.7952, -4.6859, -4.9796, -3.8973],
        [-4.2585, -2.6131, -3.4478, -3.2824, -2.7699, -5.1844, -3.3149, -3.1306]])...
	Related Layers:
		- parent layers: matmul_63_411
		- child layers: maskedfill_32_415
		- shares parents with no other layers
		- shares children with layers: eq_32_414
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.3.sa.heads.7:1
	Time elapsed:  6.843E-05s
	Output of modules: none
	Lookup keys: -252, 446, mul_32, mul_32:1, mul_32_412, mul_32_412:1
--------------------------------------------
Layer getitem_32_413, operation 416/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_32
		- child layers: eq_32_414
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.418E-05s
	Output of modules: none
	Lookup keys: -250, 448, getitem_32, getitem_32:1, getitem_32_413, getitem_32_413:1
--------------------------------------------
Layer eq_32_414, operation 417/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_32_413
		- child layers: maskedfill_32_415
		- shares parents with no other layers
		- shares children with layers: mul_32_412
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.154E-05s
	Output of modules: none
	Lookup keys: -249, 449, eq_32, eq_32:1, eq_32_414, eq_32_414:1
--------------------------------------------
Layer maskedfill_32_415, operation 418/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ 1.9621,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [ 2.0263,  2.0256,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [ 0.1207,  0.0249,  0.1386,    -inf,    -inf,    -inf,    -inf,    -inf],
        [ 0.8515,  0.4796,  0.7706,  0.8854,    -inf,    -inf,    -inf,    -inf],
        [ 0.8421,  1.8044,  0.4932,  0.3658,  0.1024,    -inf,    -inf,    -inf],
        [-3.7103, -4.9771, -2.2647, -1.7171, -1.6535, -4.9494,    -inf,    -inf],
        [-3.7630, -6.1369, -2.3184, -1.6975, -1.7952, -4.6859, -4.9796,    -inf],
        [-4.2585, -2.6131, -3.4478, -3.2824, -2.7699, -5.1844, -3.3149, -3.1306]])...
	Related Layers:
		- parent layers: mul_32_412, eq_32_414
		- child layers: softmax_32_416
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.3.sa.heads.7:1
	Time elapsed:  8.273E-05s
	Output of modules: none
	Lookup keys: -248, 450, maskedfill_32, maskedfill_32:1, maskedfill_32_415, maskedfill_32_415:1
--------------------------------------------
Layer softmax_32_416, operation 419/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5002, 0.4998, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3417, 0.3105, 0.3478, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2743, 0.1891, 0.2529, 0.2837, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1845, 0.4828, 0.1301, 0.1146, 0.0880, 0.0000, 0.0000, 0.0000],
        [0.0477, 0.0134, 0.2023, 0.3499, 0.3728, 0.0138, 0.0000, 0.0000],
        [0.0475, 0.0044, 0.2012, 0.3744, 0.3396, 0.0189, 0.0141, 0.0000],
        [0.0464, 0.2403, 0.1043, 0.1230, 0.2054, 0.0184, 0.1191, 0.1432]])...
	Related Layers:
		- parent layers: maskedfill_32_415
		- child layers: dropout_38_417
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.3.sa.heads.7:1
	Time elapsed:  7.463E-05s
	Output of modules: none
	Lookup keys: -247, 451, softmax_32, softmax_32:1, softmax_32_416, softmax_32_416:1
--------------------------------------------
Layer dropout_38_417, operation 420/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5002, 0.4998, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3417, 0.3105, 0.3478, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2743, 0.1891, 0.2529, 0.2837, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1845, 0.4828, 0.1301, 0.1146, 0.0880, 0.0000, 0.0000, 0.0000],
        [0.0477, 0.0134, 0.2023, 0.3499, 0.3728, 0.0138, 0.0000, 0.0000],
        [0.0475, 0.0044, 0.2012, 0.3744, 0.3396, 0.0189, 0.0141, 0.0000],
        [0.0464, 0.2403, 0.1043, 0.1230, 0.2054, 0.0184, 0.1191, 0.1432]])...
	Related Layers:
		- parent layers: softmax_32_416
		- child layers: matmul_64_419
		- shares parents with no other layers
		- shares children with layers: linear_105_418
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.3.sa.heads.7.dropout:1
	Time elapsed:  5.269E-05s
	Output of modules: blocks.3.sa.heads.7.dropout
	Output of bottom-level module: blocks.3.sa.heads.7.dropout:1
	Lookup keys: -246, 452, blocks.3.sa.heads.7.dropout, blocks.3.sa.heads.7.dropout:1, dropout_38, dropout_38:1, dropout_38_417, dropout_38_417:1
--------------------------------------------
Layer linear_105_418, operation 421/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.3884,  1.1309, -0.2008,  0.5036, -0.2716,  0.3926,  0.8809, -1.7807],
        [-0.2911,  0.5930, -0.2979,  0.6519, -0.3802,  0.3369,  0.4726, -0.4058],
        [-0.1437,  0.3632,  0.2980,  0.2327, -0.8489, -0.3112,  1.0411, -1.0264],
        [ 0.0542,  0.9221,  0.5697,  0.5320, -0.8110, -0.6702,  1.0938, -1.4254],
        [ 0.1895,  1.3410,  1.2729, -0.2172, -0.2564, -0.6173,  0.8388, -1.6267],
        [-0.3785, -0.4930, -2.6806,  0.2405,  0.2500,  1.4095,  0.4106,  0.2026],
        [-0.5327, -0.4030, -2.1748, -0.6003,  0.0911,  1.2412, -0.0323,  0.5527],
        [ 0.1613,  0.1230, -0.0669,  0.3518, -0.8321, -0.0506,  0.3346, -0.7893]])...
	Related Layers:
		- parent layers: layernorm_7_323
		- child layers: matmul_64_419
		- shares parents with layers: linear_82_324, linear_83_325, linear_84_334, linear_85_336, linear_86_337, linear_87_346, linear_88_348, linear_89_349, linear_90_358, linear_91_360, linear_92_361, linear_93_370, linear_94_372, linear_95_373, linear_96_382, linear_97_384, linear_98_385, linear_99_394, linear_100_396, linear_101_397, linear_102_406, linear_103_408, linear_104_409
		- shares children with layers: dropout_38_417
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.7.value:1
	Time elapsed:  1.192E-04s
	Output of modules: blocks.3.sa.heads.7.value
	Output of bottom-level module: blocks.3.sa.heads.7.value:1
	Lookup keys: -245, 453, blocks.3.sa.heads.7.value, blocks.3.sa.heads.7.value:1, linear_105, linear_105:1, linear_105_418, linear_105_418:1
--------------------------------------------
Layer matmul_64_419, operation 422/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.3884,  1.1309, -0.2008,  0.5036, -0.2716,  0.3926,  0.8809, -1.7807],
        [ 0.0488,  0.8620, -0.2493,  0.5778, -0.3258,  0.3648,  0.6768, -1.0935],
        [-0.0076,  0.6968, -0.0574,  0.4554, -0.5061,  0.1305,  0.8099, -1.0914],
        [ 0.0305,  0.7758,  0.1256,  0.4712, -0.5912, -0.0975,  0.9046, -1.2291],
        [-0.0647,  0.7658,  0.0352,  0.4798, -0.4596,  0.0635,  0.7253, -0.9644],
        [ 0.0699,  0.9512,  0.6836,  0.1884, -0.5657, -0.4849,  0.9601, -1.4005],
        [ 0.0583,  0.9150,  0.6135,  0.1952, -0.5701, -0.4590,  0.9550, -1.3673],
        [-0.0686,  0.5822, -0.0361,  0.2084, -0.4486,  0.0239,  0.6214, -0.8400]])...
	Related Layers:
		- parent layers: dropout_38_417, linear_105_418
		- child layers: cat_4_420
		- shares parents with no other layers
		- shares children with layers: matmul_50_335, matmul_52_347, matmul_54_359, matmul_56_371, matmul_58_383, matmul_60_395, matmul_62_407
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.3.sa.heads.7:1
	Time elapsed:  8.225E-05s
	Output of modules: blocks.3.sa.heads.7
	Lookup keys: -244, 454, blocks.3.sa.heads.7, blocks.3.sa.heads.7:1, matmul_64, matmul_64:1, matmul_64_419, matmul_64_419:1
--------------------------------------------
Layer cat_4_420, operation 423/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-0.4185,  1.2749,  0.0019, -0.9902, -1.6180, -0.8272,  0.4441, -0.2893],
        [ 0.2365,  0.8111,  0.1587, -0.6068, -1.3140, -0.4690,  0.3051,  0.2388],
        [ 0.3356,  0.7284,  0.1807, -0.5476, -1.2619, -0.4052,  0.2789,  0.3240],
        [ 0.3453,  0.7242,  0.1826, -0.5416, -1.2580, -0.4037,  0.2778,  0.3318],
        [ 0.3506,  0.7237,  0.1846, -0.5390, -1.2575, -0.4027,  0.2781,  0.3344],
        [-0.6386,  0.5060, -0.1550, -0.9719, -1.6145, -0.3812,  0.3744,  0.2634],
        [ 0.3917,  0.6412,  0.2790, -0.8158, -0.8593, -0.5126, -0.0924, -0.0960],
        [ 0.8681,  0.5624,  0.5537, -0.6081, -0.6651, -0.5941, -0.2651, -0.1987]])...
	Related Layers:
		- parent layers: matmul_50_335, matmul_52_347, matmul_54_359, matmul_56_371, matmul_58_383, matmul_60_395, matmul_62_407, matmul_64_419
		- child layers: linear_106_421
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: cat (grad_fn: CatBackward0) 
	Computed inside module: blocks.3.sa:1
	Time elapsed:  8.869E-05s
	Output of modules: none
	Lookup keys: -243, 455, cat_4, cat_4:1, cat_4_420, cat_4_420:1
--------------------------------------------
Layer linear_106_421, operation 424/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[ 0.8682, -0.3946, -0.6300, -0.9789, -0.6366,  0.0338, -0.0564, -0.6347],
        [ 0.6618, -0.3532, -0.9928, -0.9258, -0.5399, -0.4346, -0.1804, -0.5438],
        [ 0.7421, -0.4058, -0.9401, -0.8474, -0.4685, -0.3830, -0.3183, -0.4300],
        [ 0.7851, -0.3640, -0.9843, -0.8974, -0.4186, -0.2936, -0.3465, -0.4243],
        [ 0.8252, -0.3605, -0.8634, -0.9032, -0.4369, -0.2520, -0.3161, -0.4337],
        [ 0.5895, -0.1457, -1.1479, -0.5785, -0.6305,  0.0974, -0.3501, -0.3975],
        [ 0.1128, -0.2138, -1.2593, -0.7117, -0.4394, -0.3829, -0.0844,  0.0850],
        [-0.1569, -0.5098, -1.2660, -0.9597, -0.3918, -0.5440, -0.0465,  0.4255]])...
	Related Layers:
		- parent layers: cat_4_420
		- child layers: dropout_39_422
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (384, 384), (384,); 147840 params total (577.7 KB)
	Function: linear (grad_fn: ViewBackward0) 
	Computed inside module: blocks.3.sa.proj:1
	Time elapsed:  6.683E-04s
	Output of modules: blocks.3.sa.proj
	Output of bottom-level module: blocks.3.sa.proj:1
	Lookup keys: -242, 456, blocks.3.sa.proj, blocks.3.sa.proj:1, linear_106, linear_106:1, linear_106_421, linear_106_421:1
--------------------------------------------
Layer dropout_39_422, operation 425/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[ 0.8682, -0.3946, -0.6300, -0.9789, -0.6366,  0.0338, -0.0564, -0.6347],
        [ 0.6618, -0.3532, -0.9928, -0.9258, -0.5399, -0.4346, -0.1804, -0.5438],
        [ 0.7421, -0.4058, -0.9401, -0.8474, -0.4685, -0.3830, -0.3183, -0.4300],
        [ 0.7851, -0.3640, -0.9843, -0.8974, -0.4186, -0.2936, -0.3465, -0.4243],
        [ 0.8252, -0.3605, -0.8634, -0.9032, -0.4369, -0.2520, -0.3161, -0.4337],
        [ 0.5895, -0.1457, -1.1479, -0.5785, -0.6305,  0.0974, -0.3501, -0.3975],
        [ 0.1128, -0.2138, -1.2593, -0.7117, -0.4394, -0.3829, -0.0844,  0.0850],
        [-0.1569, -0.5098, -1.2660, -0.9597, -0.3918, -0.5440, -0.0465,  0.4255]])...
	Related Layers:
		- parent layers: linear_106_421
		- child layers: add_5_423:1
		- shares parents with no other layers
		- shares children with layers: add_4_317:2
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: ViewBackward0) 
	Computed inside module: blocks.3.sa.dropout:1
	Time elapsed:  7.057E-05s
	Output of modules: blocks.3.sa.dropout, blocks.3.sa
	Output of bottom-level module: blocks.3.sa.dropout:1
	Lookup keys: -241, 457, blocks.3.sa, blocks.3.sa.dropout, blocks.3.sa.dropout:1, blocks.3.sa:1, dropout_39, dropout_39:1, dropout_39_422, dropout_39_422:1
--------------------------------------------
Layer add_5_423 (pass 1/2), operation 426/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-6.1436, -4.1281,  1.7100, -4.4168,  1.0502,  5.9600, -0.0578,  6.0753],
        [-2.8017, -5.7320,  3.8748, -2.7113,  0.8371,  4.0059,  0.3005,  4.3743],
        [-4.5861, -7.0542,  3.5632, -3.7793, -2.6184, -0.1565, -0.8000,  2.3319],
        [-4.6314, -7.1876,  6.1229, -5.1485, -1.0192, -0.4058, -0.5417,  4.2587],
        [-3.1384, -6.0218,  4.0592, -5.4661,  4.4514, -0.9076, -0.1702,  5.3969],
        [ 0.2580, -1.1166,  1.4785, -1.9533,  1.2730,  6.8555, -0.5708,  6.2271],
        [-3.6222, -0.5830,  3.3764, -3.1022,  3.4978, 10.0775,  0.2407,  0.9344],
        [-8.1000, -3.2999,  4.6157, -5.8839,  2.6439, -0.6446, -4.8195,  3.9364]])...
	Related Layers:
		- parent layers: add_4_317:2, dropout_39_422
		- child layers: layernorm_8_424, add_5_423:2
		- shares parents with layers: layernorm_7_323
		- shares children with layers: dropout_40_428
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __add__ (grad_fn: AddBackward0) 
	Computed inside module: blocks.3:1
	Time elapsed:  8.368E-05s
	Output of modules: none
	Lookup keys: -240, 458, add_5:1, add_5_423:1
--------------------------------------------
Layer layernorm_8_424, operation 427/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-1.6439, -1.1371,  0.3660, -1.2116,  0.2186,  1.5220, -0.0845,  1.5204],
        [-0.7766, -1.6035,  1.0354, -0.7647,  0.2261,  1.1233,  0.0625,  1.1996],
        [-1.3165, -2.0460,  0.9890, -1.0998, -0.7734, -0.0485, -0.2497,  0.6596],
        [-1.3049, -2.0384,  1.6532, -1.4646, -0.3116, -0.1320, -0.1848,  1.1651],
        [-0.8523, -1.6748,  1.1227, -1.5165,  1.2817, -0.2354, -0.0424,  1.5241],
        [ 0.0439, -0.2438,  0.2691, -0.4181,  0.2507,  1.4169, -0.1419,  1.2580],
        [-0.7653, -0.1165,  0.7176, -0.6658,  0.7808,  2.2348,  0.0516,  0.2098],
        [-2.4563, -1.0124,  1.3752, -1.8005,  0.8184, -0.1952, -1.4827,  1.1976]])...
	Related Layers:
		- parent layers: add_5_423:1
		- child layers: linear_107_425
		- shares parents with layers: add_5_423:2
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (384,), (384,); 768 params total (3.2 KB)
	Function: layer_norm (grad_fn: NativeLayerNormBackward0) 
	Computed inside module: blocks.3.ln2:1
	Time elapsed:  2.787E-04s
	Output of modules: blocks.3.ln2
	Output of bottom-level module: blocks.3.ln2:1
	Lookup keys: -239, 459, blocks.3.ln2, blocks.3.ln2:1, layernorm_8, layernorm_8:1, layernorm_8_424, layernorm_8_424:1
--------------------------------------------
Layer linear_107_425, operation 428/648:
	Output tensor: shape=1x11x1536(67584 bytes), dype=torch.float32, Tensor size=67728 bytes, Tensor storage size=67648
		tensor([[-0.0985,  0.9947, -0.8121, -0.4010, -0.6154, -0.2001,  0.2104, -0.4317],
        [-0.6765, -0.0222, -0.8544,  0.1754,  0.3424,  0.5808,  1.3046,  0.0971],
        [-0.6326, -0.0930, -0.5462, -0.1844, -0.4173, -0.0915,  0.4613, -1.1493],
        [-0.4889, -0.3718, -0.5032,  0.0788, -0.4130, -0.1552,  0.4970, -0.8540],
        [-0.1266,  1.1259, -0.8018, -0.7504, -0.1073, -0.6353,  0.3167, -0.6034],
        [-0.3063, -1.0749, -0.7397,  0.4482,  0.4918,  0.0793,  0.1787,  0.0681],
        [-0.1488, -0.8481, -0.7006,  0.7180, -0.7015,  0.2441,  1.3212,  0.9703],
        [ 0.2307,  0.1898, -0.3585, -0.4343, -0.3567, -0.4721,  0.6103, -0.4328]])...
	Related Layers:
		- parent layers: layernorm_8_424
		- child layers: relu_4_426
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (1536, 384), (1536,); 591360 params total (2.3 MB)
	Function: linear (grad_fn: ViewBackward0) 
	Computed inside module: blocks.3.ffwd.net.0:1
	Time elapsed:  6.938E-04s
	Output of modules: blocks.3.ffwd.net.0
	Output of bottom-level module: blocks.3.ffwd.net.0:1
	Lookup keys: -238, 460, blocks.3.ffwd.net.0, blocks.3.ffwd.net.0:1, linear_107, linear_107:1, linear_107_425, linear_107_425:1
--------------------------------------------
Layer relu_4_426, operation 429/648:
	Output tensor: shape=1x11x1536(67584 bytes), dype=torch.float32, Tensor size=67728 bytes, Tensor storage size=67648
		tensor([[0.0000, 0.9947, 0.0000, 0.0000, 0.0000, 0.0000, 0.2104, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.1754, 0.3424, 0.5808, 1.3046, 0.0971],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4613, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0788, 0.0000, 0.0000, 0.4970, 0.0000],
        [0.0000, 1.1259, 0.0000, 0.0000, 0.0000, 0.0000, 0.3167, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.4482, 0.4918, 0.0793, 0.1787, 0.0681],
        [0.0000, 0.0000, 0.0000, 0.7180, 0.0000, 0.2441, 1.3212, 0.9703],
        [0.2307, 0.1898, 0.0000, 0.0000, 0.0000, 0.0000, 0.6103, 0.0000]])...
	Related Layers:
		- parent layers: linear_107_425
		- child layers: linear_108_427
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: relu (grad_fn: ReluBackward0) 
	Computed inside module: blocks.3.ffwd.net.1:1
	Time elapsed:  9.918E-05s
	Output of modules: blocks.3.ffwd.net.1
	Output of bottom-level module: blocks.3.ffwd.net.1:1
	Lookup keys: -237, 461, blocks.3.ffwd.net.1, blocks.3.ffwd.net.1:1, relu_4, relu_4:1, relu_4_426, relu_4_426:1
--------------------------------------------
Layer linear_108_427, operation 430/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-0.9500,  0.3189,  0.9174,  0.6462,  0.4805, -0.2807,  0.8409,  0.1980],
        [-0.9532,  0.7149,  1.3308,  0.8643,  0.8031, -0.3707,  1.2311,  0.7705],
        [-0.8685, -0.0918,  0.9614,  0.5304,  0.0355, -0.4644,  0.8058,  0.3983],
        [-1.0176, -0.0327,  0.9174,  0.5889,  0.1388, -0.6181,  0.9330,  0.4115],
        [-0.8112,  0.2096,  0.8511,  0.8382,  0.4760, -0.4051,  0.8338,  0.4083],
        [-1.0182,  0.5554,  2.3487,  1.1780,  1.0157,  0.5246,  1.8716,  0.4189],
        [-1.0277,  0.9677,  1.6155,  0.9747,  0.5726,  0.2240,  1.2222,  0.1854],
        [-1.1462,  0.3067,  1.3757,  0.6815,  0.7790, -0.3812,  0.6275,  0.4159]])...
	Related Layers:
		- parent layers: relu_4_426
		- child layers: dropout_40_428
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (384, 1536), (384,); 590208 params total (2.3 MB)
	Function: linear (grad_fn: ViewBackward0) 
	Computed inside module: blocks.3.ffwd.net.2:1
	Time elapsed:  8.376E-04s
	Output of modules: blocks.3.ffwd.net.2
	Output of bottom-level module: blocks.3.ffwd.net.2:1
	Lookup keys: -236, 462, blocks.3.ffwd.net.2, blocks.3.ffwd.net.2:1, linear_108, linear_108:1, linear_108_427, linear_108_427:1
--------------------------------------------
Layer dropout_40_428, operation 431/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-0.9500,  0.3189,  0.9174,  0.6462,  0.4805, -0.2807,  0.8409,  0.1980],
        [-0.9532,  0.7149,  1.3308,  0.8643,  0.8031, -0.3707,  1.2311,  0.7705],
        [-0.8685, -0.0918,  0.9614,  0.5304,  0.0355, -0.4644,  0.8058,  0.3983],
        [-1.0176, -0.0327,  0.9174,  0.5889,  0.1388, -0.6181,  0.9330,  0.4115],
        [-0.8112,  0.2096,  0.8511,  0.8382,  0.4760, -0.4051,  0.8338,  0.4083],
        [-1.0182,  0.5554,  2.3487,  1.1780,  1.0157,  0.5246,  1.8716,  0.4189],
        [-1.0277,  0.9677,  1.6155,  0.9747,  0.5726,  0.2240,  1.2222,  0.1854],
        [-1.1462,  0.3067,  1.3757,  0.6815,  0.7790, -0.3812,  0.6275,  0.4159]])...
	Related Layers:
		- parent layers: linear_108_427
		- child layers: add_5_423:2
		- shares parents with no other layers
		- shares children with layers: add_5_423:1
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: ViewBackward0) 
	Computed inside module: blocks.3.ffwd.net.3:1
	Time elapsed:  6.390E-05s
	Output of modules: blocks.3.ffwd.net.3, blocks.3.ffwd.net, blocks.3.ffwd
	Output of bottom-level module: blocks.3.ffwd.net.3:1
	Lookup keys: -235, 463, blocks.3.ffwd, blocks.3.ffwd.net, blocks.3.ffwd.net.3, blocks.3.ffwd.net.3:1, blocks.3.ffwd.net:1, blocks.3.ffwd:1, dropout_40, dropout_40:1, dropout_40_428, dropout_40_428:1
--------------------------------------------
Layer add_5_423 (pass 2/2), operation 432/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-7.0936e+00, -3.8092e+00,  2.6274e+00, -3.7706e+00,  1.5307e+00,
          5.6793e+00,  7.8311e-01,  6.2733e+00],
        [-3.7549e+00, -5.0171e+00,  5.2056e+00, -1.8470e+00,  1.6402e+00,
          3.6352e+00,  1.5317e+00,  5.1448e+00],
        [-5.4546e+00, -7.1460e+00,  4.5246e+00, -3.2490e+00, -2.5829e+00,
         -6.2085e-01,  5.7789e-03,  2.7302e+00],
        [-5.6490e+00, -7.2203e+00,  7.0403e+00, -4.5596e+00, -8.8033e-01,
         -1.0240e+00,  3.9124e-01,  4.6702e+00],
        [-3.9496e+00, -5.8122e+00,  4.9103e+00, -4.6280e+00,  4.9274e+00,
         -1.3127e+00,  6.6352e-01,  5.8052e+00],
        [-7.6012e-01, -5.6115e-01,  3.8272e+00, -7.7531e-01,  2.2887e+00,
          7.3801e+00,  1.3008e+00,  6.6459e+00],
        [-4.6499e+00,  3.8471e-01,  4.9920e+00, -2.1275e+00,  4.0704e+00,
          1.0302e+01,  1.4629e+00,  1.1198e+00],
        [-9.2462e+00, -2.9932e+00,  5.9913e+00, -5.2024e+00,  3.4229e+00,
         -1.0258e+00, -4.1920e+00,  4.3523e+00]])...
	Related Layers:
		- parent layers: add_5_423:1, dropout_40_428
		- child layers: layernorm_9_429, add_6_529:1
		- shares parents with layers: layernorm_8_424
		- shares children with layers: dropout_49_528
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __add__ (grad_fn: AddBackward0) 
	Computed inside module: blocks.3:1
	Time elapsed:  7.343E-05s
	Output of modules: blocks.3
	Lookup keys: -234, 464, add_5:2, add_5_423:2, blocks.3, blocks.3:1
--------------------------------------------
Layer layernorm_9_429, operation 433/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-1.8210, -1.0480,  0.5794, -0.9938,  0.3057,  1.4031,  0.1312,  1.4905],
        [-0.9377, -1.3081,  1.2600, -0.4681,  0.3782,  0.9244,  0.3627,  1.2511],
        [-1.4733, -2.0186,  1.1854, -0.8816, -0.7183, -0.1737, -0.0150,  0.7090],
        [-1.4796, -1.9738,  1.7710, -1.1977, -0.2669, -0.2879,  0.0707,  1.1675],
        [-1.0097, -1.5627,  1.2668, -1.1856,  1.2695, -0.3290,  0.1782,  1.5054],
        [-0.1425, -0.1094,  0.6339, -0.1505,  0.3658,  1.3133,  0.2057,  1.1276],
        [-0.8924,  0.0954,  0.9607, -0.4055,  0.7805,  2.0771,  0.2866,  0.2124],
        [-2.5671, -0.8704,  1.6309, -1.4442,  0.9206, -0.2911, -1.1725,  1.1846]])...
	Related Layers:
		- parent layers: add_5_423:2
		- child layers: linear_109_430, linear_110_431, linear_111_440, linear_112_442, linear_113_443, linear_114_452, linear_115_454, linear_116_455, linear_117_464, linear_118_466, linear_119_467, linear_120_476, linear_121_478, linear_122_479, linear_123_488, linear_124_490, linear_125_491, linear_126_500, linear_127_502, linear_128_503, linear_129_512, linear_130_514, linear_131_515, linear_132_524
		- shares parents with layers: add_6_529:1
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (384,), (384,); 768 params total (3.2 KB)
	Function: layer_norm (grad_fn: NativeLayerNormBackward0) 
	Computed inside module: blocks.4.ln1:1
	Time elapsed:  2.346E-04s
	Output of modules: blocks.4.ln1
	Output of bottom-level module: blocks.4.ln1:1
	Lookup keys: -233, 465, blocks.4.ln1, blocks.4.ln1:1, layernorm_9, layernorm_9:1, layernorm_9_429, layernorm_9_429:1
--------------------------------------------
Layer linear_109_430, operation 434/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-1.1503,  3.4747,  1.9939, -1.2392,  2.7231, -1.2908, -0.3653, -0.7805],
        [-1.2447,  2.3766,  3.3823, -2.4166,  3.3496, -2.0800, -0.3314, -1.8090],
        [-0.8011,  2.6967,  1.8467, -1.0263,  2.8015, -1.8891,  0.2872, -1.0162],
        [-1.2449,  2.6314,  2.1148, -1.2091,  2.6760, -1.8405,  0.4720, -0.9808],
        [-1.7198,  2.7846,  2.0087, -0.7720,  2.5557, -1.7766,  0.4805, -1.1283],
        [-1.2397,  2.6446,  2.9258, -0.4245,  3.0307, -1.1425, -1.9273,  0.9895],
        [-0.6286,  1.5247,  3.1478, -1.8795,  2.6943, -1.4277, -0.9950, -0.5185],
        [-1.0440,  2.1207,  2.0802, -2.4424,  3.0311, -1.3099, -0.1151, -1.4358]])...
	Related Layers:
		- parent layers: layernorm_9_429
		- child layers: transpose_33_432
		- shares parents with layers: linear_110_431, linear_111_440, linear_112_442, linear_113_443, linear_114_452, linear_115_454, linear_116_455, linear_117_464, linear_118_466, linear_119_467, linear_120_476, linear_121_478, linear_122_479, linear_123_488, linear_124_490, linear_125_491, linear_126_500, linear_127_502, linear_128_503, linear_129_512, linear_130_514, linear_131_515, linear_132_524
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.0.key:1
	Time elapsed:  1.168E-04s
	Output of modules: blocks.4.sa.heads.0.key
	Output of bottom-level module: blocks.4.sa.heads.0.key:1
	Lookup keys: -232, 466, blocks.4.sa.heads.0.key, blocks.4.sa.heads.0.key:1, linear_109, linear_109:1, linear_109_430, linear_109_430:1
--------------------------------------------
Layer linear_110_431, operation 435/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.4152, -2.1279, -0.8845,  1.5156, -2.9987,  0.4062,  0.8568,  0.5470],
        [ 0.7811, -1.8212, -1.0195,  1.1381, -1.8886,  0.9708,  0.4213,  0.8198],
        [ 1.1324, -2.1145, -1.7463,  1.5097, -2.8410,  1.2517,  0.6765,  1.0591],
        [ 1.3062, -2.3837, -1.3862,  1.8246, -2.9495,  1.3983,  0.3675,  1.5084],
        [ 1.5002, -2.9740, -2.1534,  1.5659, -2.6257,  0.8985,  1.1449,  0.8276],
        [-0.5985, -1.5915, -0.1689, -1.2041, -0.7149,  0.5073, -0.5522, -1.2622],
        [ 0.7583, -3.2688, -1.3468,  0.1833, -2.8058,  2.2311,  1.1854, -0.2097],
        [ 1.3908, -3.7314, -1.7756,  1.0714, -2.7033,  1.1500,  1.3927, -0.6178]])...
	Related Layers:
		- parent layers: layernorm_9_429
		- child layers: matmul_65_433
		- shares parents with layers: linear_109_430, linear_111_440, linear_112_442, linear_113_443, linear_114_452, linear_115_454, linear_116_455, linear_117_464, linear_118_466, linear_119_467, linear_120_476, linear_121_478, linear_122_479, linear_123_488, linear_124_490, linear_125_491, linear_126_500, linear_127_502, linear_128_503, linear_129_512, linear_130_514, linear_131_515, linear_132_524
		- shares children with layers: transpose_33_432
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.0.query:1
	Time elapsed:  1.144E-04s
	Output of modules: blocks.4.sa.heads.0.query
	Output of bottom-level module: blocks.4.sa.heads.0.query:1
	Lookup keys: -231, 467, blocks.4.sa.heads.0.query, blocks.4.sa.heads.0.query:1, linear_110, linear_110:1, linear_110_431, linear_110_431:1
--------------------------------------------
Layer transpose_33_432, operation 436/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-1.1503, -1.2447, -0.8011, -1.2449, -1.7198, -1.2397, -0.6286, -1.0440],
        [ 3.4747,  2.3766,  2.6967,  2.6314,  2.7846,  2.6446,  1.5247,  2.1207],
        [ 1.9939,  3.3823,  1.8467,  2.1148,  2.0087,  2.9258,  3.1478,  2.0802],
        [-1.2392, -2.4166, -1.0263, -1.2091, -0.7720, -0.4245, -1.8795, -2.4424],
        [ 2.7231,  3.3496,  2.8015,  2.6760,  2.5557,  3.0307,  2.6943,  3.0311],
        [-1.2908, -2.0800, -1.8891, -1.8405, -1.7766, -1.1425, -1.4277, -1.3099],
        [-0.3653, -0.3314,  0.2872,  0.4720,  0.4805, -1.9273, -0.9950, -0.1151],
        [-0.7805, -1.8090, -1.0162, -0.9808, -1.1283,  0.9895, -0.5185, -1.4358]])...
	Related Layers:
		- parent layers: linear_109_430
		- child layers: matmul_65_433
		- shares parents with no other layers
		- shares children with layers: linear_110_431
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.4.sa.heads.0:1
	Time elapsed:  5.841E-05s
	Output of modules: none
	Lookup keys: -230, 468, transpose_33, transpose_33:1, transpose_33_432, transpose_33_432:1
--------------------------------------------
Layer matmul_65_433, operation 437/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-129.7651, -150.2790, -121.0097, -119.3804, -111.8105, -123.7878,
         -118.3641, -126.7477],
        [-131.0529, -150.2610, -119.6188, -117.1073, -113.8299, -116.9096,
         -110.0213, -127.3748],
        [-152.3655, -181.8192, -145.3955, -143.7185, -129.1095, -161.7019,
         -149.3444, -153.0523],
        [-165.9242, -197.9946, -157.9317, -156.2918, -141.8648, -165.8347,
         -157.9368, -165.7772],
        [-181.2899, -213.6690, -171.0901, -170.2056, -149.3788, -200.3639,
         -182.9312, -177.5450],
        [ -39.8370,  -37.1283,  -34.8320,  -31.4461,  -35.3752,  -42.2869,
          -24.4780,  -32.5704],
        [-155.8610, -175.0704, -142.1466, -137.6752, -131.5059, -163.7064,
         -137.3596, -147.3459],
        [-190.5282, -214.4522, -176.8804, -172.8307, -154.7130, -218.3951,
         -184.0082, -182.1143]])...
	Related Layers:
		- parent layers: linear_110_431, transpose_33_432
		- child layers: mul_33_434
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.0:1
	Time elapsed:  8.845E-05s
	Output of modules: none
	Lookup keys: -229, 469, matmul_65, matmul_65:1, matmul_65_433, matmul_65_433:1
--------------------------------------------
Layer mul_33_434, operation 438/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -6.6220,  -7.6689,  -6.1752,  -6.0921,  -5.7058,  -6.3170,  -6.0402,
          -6.4681],
        [ -6.6878,  -7.6680,  -6.1043,  -5.9761,  -5.8089,  -5.9660,  -5.6145,
          -6.5001],
        [ -7.7754,  -9.2784,  -7.4197,  -7.3341,  -6.5886,  -8.2518,  -7.6212,
          -7.8104],
        [ -8.4673, -10.1039,  -8.0594,  -7.9757,  -7.2395,  -8.4627,  -8.0597,
          -8.4598],
        [ -9.2514, -10.9037,  -8.7309,  -8.6858,  -7.6230, -10.2248,  -9.3352,
          -9.0603],
        [ -2.0329,  -1.8947,  -1.7775,  -1.6047,  -1.8052,  -2.1579,  -1.2491,
          -1.6621],
        [ -7.9537,  -8.9340,  -7.2539,  -7.0257,  -6.7109,  -8.3541,  -7.0096,
          -7.5192],
        [ -9.7229, -10.9437,  -9.0264,  -8.8197,  -7.8952, -11.1449,  -9.3901,
          -9.2935]])...
	Related Layers:
		- parent layers: matmul_65_433
		- child layers: maskedfill_33_437
		- shares parents with no other layers
		- shares children with layers: eq_33_436
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.4.sa.heads.0:1
	Time elapsed:  6.962E-05s
	Output of modules: none
	Lookup keys: -228, 470, mul_33, mul_33:1, mul_33_434, mul_33_434:1
--------------------------------------------
Layer getitem_33_435, operation 439/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_33
		- child layers: eq_33_436
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.702E-05s
	Output of modules: none
	Lookup keys: -226, 472, getitem_33, getitem_33:1, getitem_33_435, getitem_33_435:1
--------------------------------------------
Layer eq_33_436, operation 440/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_33_435
		- child layers: maskedfill_33_437
		- shares parents with no other layers
		- shares children with layers: mul_33_434
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  7.963E-05s
	Output of modules: none
	Lookup keys: -225, 473, eq_33, eq_33:1, eq_33_436, eq_33_436:1
--------------------------------------------
Layer maskedfill_33_437, operation 441/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -6.6220,     -inf,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -6.6878,  -7.6680,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -7.7754,  -9.2784,  -7.4197,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -8.4673, -10.1039,  -8.0594,  -7.9757,     -inf,     -inf,     -inf,
             -inf],
        [ -9.2514, -10.9037,  -8.7309,  -8.6858,  -7.6230,     -inf,     -inf,
             -inf],
        [ -2.0329,  -1.8947,  -1.7775,  -1.6047,  -1.8052,  -2.1579,     -inf,
             -inf],
        [ -7.9537,  -8.9340,  -7.2539,  -7.0257,  -6.7109,  -8.3541,  -7.0096,
             -inf],
        [ -9.7229, -10.9437,  -9.0264,  -8.8197,  -7.8952, -11.1449,  -9.3901,
          -9.2935]])...
	Related Layers:
		- parent layers: mul_33_434, eq_33_436
		- child layers: softmax_33_438
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.4.sa.heads.0:1
	Time elapsed:  8.345E-05s
	Output of modules: none
	Lookup keys: -224, 474, maskedfill_33, maskedfill_33:1, maskedfill_33_437, maskedfill_33_437:1
--------------------------------------------
Layer softmax_33_438, operation 442/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.7271, 0.2729, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3774, 0.0840, 0.5386, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2308, 0.0449, 0.3470, 0.3773, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1028, 0.0197, 0.1729, 0.1809, 0.5237, 0.0000, 0.0000, 0.0000],
        [0.1406, 0.1615, 0.1815, 0.2158, 0.1766, 0.1241, 0.0000, 0.0000],
        [0.0792, 0.0297, 0.1595, 0.2004, 0.2745, 0.0531, 0.2036, 0.0000],
        [0.0660, 0.0195, 0.1324, 0.1627, 0.4102, 0.0159, 0.0920, 0.1013]])...
	Related Layers:
		- parent layers: maskedfill_33_437
		- child layers: dropout_41_439
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.4.sa.heads.0:1
	Time elapsed:  6.461E-05s
	Output of modules: none
	Lookup keys: -223, 475, softmax_33, softmax_33:1, softmax_33_438, softmax_33_438:1
--------------------------------------------
Layer dropout_41_439, operation 443/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.7271, 0.2729, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3774, 0.0840, 0.5386, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2308, 0.0449, 0.3470, 0.3773, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1028, 0.0197, 0.1729, 0.1809, 0.5237, 0.0000, 0.0000, 0.0000],
        [0.1406, 0.1615, 0.1815, 0.2158, 0.1766, 0.1241, 0.0000, 0.0000],
        [0.0792, 0.0297, 0.1595, 0.2004, 0.2745, 0.0531, 0.2036, 0.0000],
        [0.0660, 0.0195, 0.1324, 0.1627, 0.4102, 0.0159, 0.0920, 0.1013]])...
	Related Layers:
		- parent layers: softmax_33_438
		- child layers: matmul_66_441
		- shares parents with no other layers
		- shares children with layers: linear_111_440
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.4.sa.heads.0.dropout:1
	Time elapsed:  5.341E-05s
	Output of modules: blocks.4.sa.heads.0.dropout
	Output of bottom-level module: blocks.4.sa.heads.0.dropout:1
	Lookup keys: -222, 476, blocks.4.sa.heads.0.dropout, blocks.4.sa.heads.0.dropout:1, dropout_41, dropout_41:1, dropout_41_439, dropout_41_439:1
--------------------------------------------
Layer linear_111_440, operation 444/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.7832, -0.0449,  0.0546, -0.2030,  0.4132,  0.4058, -1.4607, -0.8329],
        [ 0.9498, -0.3034,  0.1978, -0.0790,  0.6462, -0.6816, -1.0144, -0.7040],
        [ 0.8701, -0.0753, -0.6478,  1.0569,  0.3261,  0.7830, -1.1746, -0.5933],
        [ 1.0759, -0.2665, -0.4671,  0.8658,  0.4578,  0.6703, -0.8237, -0.1458],
        [ 0.3944, -0.1531, -0.1701,  0.9665,  0.6082,  1.0047, -1.7758, -0.7333],
        [ 0.8502,  1.5462,  0.8587, -0.3027, -0.0718, -0.4837, -0.6146, -0.5300],
        [ 1.7128,  0.6946,  0.1949, -0.3650, -0.0075, -0.4092, -0.8194, -0.1149],
        [ 0.1609, -0.6203, -0.5444,  0.9614,  0.1470,  0.2853, -1.1161, -0.7421]])...
	Related Layers:
		- parent layers: layernorm_9_429
		- child layers: matmul_66_441
		- shares parents with layers: linear_109_430, linear_110_431, linear_112_442, linear_113_443, linear_114_452, linear_115_454, linear_116_455, linear_117_464, linear_118_466, linear_119_467, linear_120_476, linear_121_478, linear_122_479, linear_123_488, linear_124_490, linear_125_491, linear_126_500, linear_127_502, linear_128_503, linear_129_512, linear_130_514, linear_131_515, linear_132_524
		- shares children with layers: dropout_41_439
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.0.value:1
	Time elapsed:  1.826E-04s
	Output of modules: blocks.4.sa.heads.0.value
	Output of bottom-level module: blocks.4.sa.heads.0.value:1
	Lookup keys: -221, 477, blocks.4.sa.heads.0.value, blocks.4.sa.heads.0.value:1, linear_111, linear_111:1, linear_111_440, linear_111_440:1
--------------------------------------------
Layer matmul_66_441, operation 445/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.7832, -0.0449,  0.0546, -0.2030,  0.4132,  0.4058, -1.4607, -0.8329],
        [ 0.8286, -0.1154,  0.0937, -0.1692,  0.4768,  0.1091, -1.3389, -0.7977],
        [ 0.8440, -0.0830, -0.3117,  0.4860,  0.3859,  0.5177, -1.2691, -0.6930],
        [ 0.9313, -0.1507, -0.3795,  0.6430,  0.4103,  0.5876, -1.1010, -0.4847],
        [ 0.6509, -0.1520, -0.2761,  0.8232,  0.5129,  0.8111, -1.4522, -0.6124],
        [ 0.8287,  0.0383, -0.1022,  0.4705,  0.4189,  0.3512, -1.1499, -0.5652],
        [ 0.9468,  0.1035, -0.1481,  0.4985,  0.3573,  0.4379, -1.1852, -0.4635],
        [ 0.7096, -0.0994, -0.2477,  0.7214,  0.4201,  0.6220, -1.3324, -0.5659]])...
	Related Layers:
		- parent layers: dropout_41_439, linear_111_440
		- child layers: cat_5_526
		- shares parents with no other layers
		- shares children with layers: matmul_68_453, matmul_70_465, matmul_72_477, matmul_74_489, matmul_76_501, matmul_78_513, matmul_80_525
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.0:1
	Time elapsed:  1.023E-04s
	Output of modules: blocks.4.sa.heads.0
	Lookup keys: -220, 478, blocks.4.sa.heads.0, blocks.4.sa.heads.0:1, matmul_66, matmul_66:1, matmul_66_441, matmul_66_441:1
--------------------------------------------
Layer linear_112_442, operation 446/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.9696, -2.5781, -3.0290,  4.8724, -0.9306, -0.1138, -2.1585,  1.7826],
        [-0.6160,  0.0485, -1.7087,  3.0020, -1.5360,  2.3486, -0.7968,  0.8588],
        [-0.2311, -2.0574, -1.9635,  4.0312, -0.7783,  1.3205, -2.0658,  1.8021],
        [-0.4928, -2.5414, -1.6145,  4.2187, -0.7066,  0.7898, -2.4450,  1.8778],
        [ 0.3989, -2.9667, -2.5298,  4.5556, -0.1062,  0.0690, -2.6874,  1.7857],
        [-1.4739,  1.0194, -0.8873,  1.6338, -1.4528,  0.1897, -0.4261,  1.5567],
        [-1.1710,  0.4054, -1.4058,  1.4415, -0.9889,  1.4868, -0.8062,  1.1206],
        [ 0.3201, -0.1738, -2.6699,  2.5802, -0.4315,  3.1442, -1.8403,  1.9926]])...
	Related Layers:
		- parent layers: layernorm_9_429
		- child layers: transpose_34_444
		- shares parents with layers: linear_109_430, linear_110_431, linear_111_440, linear_113_443, linear_114_452, linear_115_454, linear_116_455, linear_117_464, linear_118_466, linear_119_467, linear_120_476, linear_121_478, linear_122_479, linear_123_488, linear_124_490, linear_125_491, linear_126_500, linear_127_502, linear_128_503, linear_129_512, linear_130_514, linear_131_515, linear_132_524
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.1.key:1
	Time elapsed:  1.173E-04s
	Output of modules: blocks.4.sa.heads.1.key
	Output of bottom-level module: blocks.4.sa.heads.1.key:1
	Lookup keys: -219, 479, blocks.4.sa.heads.1.key, blocks.4.sa.heads.1.key:1, linear_112, linear_112:1, linear_112_442, linear_112_442:1
--------------------------------------------
Layer linear_113_443, operation 447/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.8685, -1.1068, -1.2433, -0.1643, -1.6472, -0.6224,  0.8550, -0.1172],
        [ 0.3522, -1.3866, -1.2392,  1.1703, -0.5822,  0.3559, -1.1290,  1.4126],
        [-0.3423, -1.4057, -1.2247, -0.3371, -0.4248, -0.8400,  0.7713,  0.3701],
        [-0.4540, -1.8846, -1.3395, -0.3359, -0.3279, -1.0110,  0.7080,  0.3592],
        [-2.3547, -0.6249, -0.1985, -1.1868, -1.7911, -0.6313,  1.1408,  0.1329],
        [ 2.1320, -1.4876, -1.3783,  1.0370,  0.7104, -1.2072, -1.4695, -0.4615],
        [ 0.6962, -1.2704, -1.1198,  1.6991, -0.0774, -0.5099, -0.9545,  0.6272],
        [-1.7492,  0.8245,  0.3294,  0.0760, -2.7050,  0.4797, -0.4819,  1.4479]])...
	Related Layers:
		- parent layers: layernorm_9_429
		- child layers: matmul_67_445
		- shares parents with layers: linear_109_430, linear_110_431, linear_111_440, linear_112_442, linear_114_452, linear_115_454, linear_116_455, linear_117_464, linear_118_466, linear_119_467, linear_120_476, linear_121_478, linear_122_479, linear_123_488, linear_124_490, linear_125_491, linear_126_500, linear_127_502, linear_128_503, linear_129_512, linear_130_514, linear_131_515, linear_132_524
		- shares children with layers: transpose_34_444
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.1.query:1
	Time elapsed:  1.123E-04s
	Output of modules: blocks.4.sa.heads.1.query
	Output of bottom-level module: blocks.4.sa.heads.1.query:1
	Lookup keys: -218, 480, blocks.4.sa.heads.1.query, blocks.4.sa.heads.1.query:1, linear_113, linear_113:1, linear_113_443, linear_113_443:1
--------------------------------------------
Layer transpose_34_444, operation 448/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.9696, -0.6160, -0.2311, -0.4928,  0.3989, -1.4739, -1.1710,  0.3201],
        [-2.5781,  0.0485, -2.0574, -2.5414, -2.9667,  1.0194,  0.4054, -0.1738],
        [-3.0290, -1.7087, -1.9635, -1.6145, -2.5298, -0.8873, -1.4058, -2.6699],
        [ 4.8724,  3.0020,  4.0312,  4.2187,  4.5556,  1.6338,  1.4415,  2.5802],
        [-0.9306, -1.5360, -0.7783, -0.7066, -0.1062, -1.4528, -0.9889, -0.4315],
        [-0.1138,  2.3486,  1.3205,  0.7898,  0.0690,  0.1897,  1.4868,  3.1442],
        [-2.1585, -0.7968, -2.0658, -2.4450, -2.6874, -0.4261, -0.8062, -1.8403],
        [ 1.7826,  0.8588,  1.8021,  1.8778,  1.7857,  1.5567,  1.1206,  1.9926]])...
	Related Layers:
		- parent layers: linear_112_442
		- child layers: matmul_67_445
		- shares parents with no other layers
		- shares children with layers: linear_113_443
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.4.sa.heads.1:1
	Time elapsed:  6.390E-05s
	Output of modules: none
	Lookup keys: -217, 481, transpose_34, transpose_34:1, transpose_34_444, transpose_34_444:1
--------------------------------------------
Layer matmul_67_445, operation 449/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-25.3916, -14.9153, -32.0014, -36.1329, -29.1844,  -1.0528,  -7.7126,
         -24.7188],
        [ 85.7473,  50.4813,  73.1523,  79.2140,  79.8507,  28.3157,  31.7876,
          58.3422],
        [ -6.5980, -16.6745, -15.0830, -16.1835,  -4.4612, -11.4802, -11.9405,
         -14.0958],
        [ -0.1423, -11.7357, -10.3290, -11.3210,   0.9269,  -8.6432,  -8.8903,
         -10.8329],
        [-49.0514, -20.2933, -53.0912, -58.1527, -60.5223,   4.5026,  -9.4756,
         -45.2460],
        [ 70.9336,  19.6887,  61.5894,  68.7768,  83.6454,  -5.7660,   8.0256,
          48.2433],
        [ 79.0371,  43.3243,  66.0569,  71.3395,  75.0451,  23.4578,  28.4050,
          52.4405],
        [-25.7703,  17.3708, -25.2355, -28.7834, -44.2256,  34.2171,  19.3161,
         -17.8985]])...
	Related Layers:
		- parent layers: linear_113_443, transpose_34_444
		- child layers: mul_34_446
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.1:1
	Time elapsed:  9.799E-05s
	Output of modules: none
	Lookup keys: -216, 482, matmul_67, matmul_67:1, matmul_67_445, matmul_67_445:1
--------------------------------------------
Layer mul_34_446, operation 450/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-1.2958, -0.7611, -1.6331, -1.8439, -1.4893, -0.0537, -0.3936, -1.2614],
        [ 4.3758,  2.5761,  3.7330,  4.0424,  4.0749,  1.4450,  1.6222,  2.9773],
        [-0.3367, -0.8509, -0.7697, -0.8259, -0.2277, -0.5858, -0.6093, -0.7193],
        [-0.0073, -0.5989, -0.5271, -0.5777,  0.0473, -0.4411, -0.4537, -0.5528],
        [-2.5031, -1.0356, -2.7093, -2.9676, -3.0885,  0.2298, -0.4835, -2.3090],
        [ 3.6198,  1.0047,  3.1430,  3.5098,  4.2685, -0.2942,  0.4096,  2.4619],
        [ 4.0333,  2.2109,  3.3710,  3.6405,  3.8296,  1.1971,  1.4495,  2.6761],
        [-1.3151,  0.8865, -1.2878, -1.4688, -2.2569,  1.7461,  0.9857, -0.9134]])...
	Related Layers:
		- parent layers: matmul_67_445
		- child layers: maskedfill_34_449
		- shares parents with no other layers
		- shares children with layers: eq_34_448
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.4.sa.heads.1:1
	Time elapsed:  8.035E-05s
	Output of modules: none
	Lookup keys: -215, 483, mul_34, mul_34:1, mul_34_446, mul_34_446:1
--------------------------------------------
Layer getitem_34_447, operation 451/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_34
		- child layers: eq_34_448
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.751E-05s
	Output of modules: none
	Lookup keys: -213, 485, getitem_34, getitem_34:1, getitem_34_447, getitem_34_447:1
--------------------------------------------
Layer eq_34_448, operation 452/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_34_447
		- child layers: maskedfill_34_449
		- shares parents with no other layers
		- shares children with layers: mul_34_446
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.702E-05s
	Output of modules: none
	Lookup keys: -212, 486, eq_34, eq_34:1, eq_34_448, eq_34_448:1
--------------------------------------------
Layer maskedfill_34_449, operation 453/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-1.2958,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [ 4.3758,  2.5761,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-0.3367, -0.8509, -0.7697,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-0.0073, -0.5989, -0.5271, -0.5777,    -inf,    -inf,    -inf,    -inf],
        [-2.5031, -1.0356, -2.7093, -2.9676, -3.0885,    -inf,    -inf,    -inf],
        [ 3.6198,  1.0047,  3.1430,  3.5098,  4.2685, -0.2942,    -inf,    -inf],
        [ 4.0333,  2.2109,  3.3710,  3.6405,  3.8296,  1.1971,  1.4495,    -inf],
        [-1.3151,  0.8865, -1.2878, -1.4688, -2.2569,  1.7461,  0.9857, -0.9134]])...
	Related Layers:
		- parent layers: mul_34_446, eq_34_448
		- child layers: softmax_34_450
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.4.sa.heads.1:1
	Time elapsed:  9.298E-05s
	Output of modules: none
	Lookup keys: -211, 487, maskedfill_34, maskedfill_34:1, maskedfill_34_449, maskedfill_34_449:1
--------------------------------------------
Layer softmax_34_450, operation 454/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.8581, 0.1419, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.4451, 0.2662, 0.2887, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3686, 0.2040, 0.2191, 0.2083, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1363, 0.5913, 0.1109, 0.0857, 0.0759, 0.0000, 0.0000, 0.0000],
        [0.2211, 0.0162, 0.1373, 0.1981, 0.4230, 0.0044, 0.0000, 0.0000],
        [0.3028, 0.0489, 0.1561, 0.2045, 0.2470, 0.0178, 0.0229, 0.0000],
        [0.0222, 0.2002, 0.0228, 0.0190, 0.0086, 0.4730, 0.2211, 0.0331]])...
	Related Layers:
		- parent layers: maskedfill_34_449
		- child layers: dropout_42_451
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.4.sa.heads.1:1
	Time elapsed:  7.415E-05s
	Output of modules: none
	Lookup keys: -210, 488, softmax_34, softmax_34:1, softmax_34_450, softmax_34_450:1
--------------------------------------------
Layer dropout_42_451, operation 455/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.8581, 0.1419, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.4451, 0.2662, 0.2887, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3686, 0.2040, 0.2191, 0.2083, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1363, 0.5913, 0.1109, 0.0857, 0.0759, 0.0000, 0.0000, 0.0000],
        [0.2211, 0.0162, 0.1373, 0.1981, 0.4230, 0.0044, 0.0000, 0.0000],
        [0.3028, 0.0489, 0.1561, 0.2045, 0.2470, 0.0178, 0.0229, 0.0000],
        [0.0222, 0.2002, 0.0228, 0.0190, 0.0086, 0.4730, 0.2211, 0.0331]])...
	Related Layers:
		- parent layers: softmax_34_450
		- child layers: matmul_68_453
		- shares parents with no other layers
		- shares children with layers: linear_114_452
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.4.sa.heads.1.dropout:1
	Time elapsed:  5.603E-05s
	Output of modules: blocks.4.sa.heads.1.dropout
	Output of bottom-level module: blocks.4.sa.heads.1.dropout:1
	Lookup keys: -209, 489, blocks.4.sa.heads.1.dropout, blocks.4.sa.heads.1.dropout:1, dropout_42, dropout_42:1, dropout_42_451, dropout_42_451:1
--------------------------------------------
Layer linear_114_452, operation 456/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.2869,  0.5182, -0.7858,  0.7165, -0.1309,  0.0713,  0.2613, -0.2909],
        [ 0.5214,  0.6056, -0.4027,  1.0119, -0.7662,  0.2170,  0.1474, -0.0257],
        [ 0.4393,  0.3425, -0.0131,  0.4664,  0.0767, -0.0071, -0.5531, -0.2259],
        [ 0.2215,  0.0320, -0.0886,  0.2816, -0.1049, -0.1804, -0.5414,  0.0424],
        [ 1.5990,  0.1567, -0.4482,  0.0989,  0.4592,  0.4802, -0.7239, -0.0955],
        [-0.0850,  1.2748, -1.2753,  1.4574, -0.1829, -1.1410,  0.7749,  0.6178],
        [-0.1588,  0.9984, -0.9395,  0.5648, -1.1927, -0.5273,  0.4751,  0.2523],
        [ 0.5697,  0.2949, -0.3236,  0.7278,  0.0819,  0.2519, -0.9559,  0.4264]])...
	Related Layers:
		- parent layers: layernorm_9_429
		- child layers: matmul_68_453
		- shares parents with layers: linear_109_430, linear_110_431, linear_111_440, linear_112_442, linear_113_443, linear_115_454, linear_116_455, linear_117_464, linear_118_466, linear_119_467, linear_120_476, linear_121_478, linear_122_479, linear_123_488, linear_124_490, linear_125_491, linear_126_500, linear_127_502, linear_128_503, linear_129_512, linear_130_514, linear_131_515, linear_132_524
		- shares children with layers: dropout_42_451
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.1.value:1
	Time elapsed:  1.230E-04s
	Output of modules: blocks.4.sa.heads.1.value
	Output of bottom-level module: blocks.4.sa.heads.1.value:1
	Lookup keys: -208, 490, blocks.4.sa.heads.1.value, blocks.4.sa.heads.1.value:1, linear_114, linear_114:1, linear_114_452, linear_114_452:1
--------------------------------------------
Layer matmul_68_453, operation 457/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.2869,  0.5182, -0.7858,  0.7165, -0.1309,  0.0713,  0.2613, -0.2909],
        [ 0.3202,  0.5306, -0.7314,  0.7585, -0.2210,  0.0919,  0.2452, -0.2533],
        [ 0.3933,  0.4907, -0.4608,  0.7229, -0.2401,  0.0875, -0.0041, -0.2015],
        [ 0.3545,  0.3962, -0.3931,  0.6314, -0.2095,  0.0314, -0.1076, -0.1531],
        [ 0.5365,  0.4813, -0.3882,  0.7793, -0.4365,  0.1583, -0.0399, -0.0835],
        [ 0.8520,  0.2496, -0.3948,  0.3429,  0.1419,  0.1807, -0.4258, -0.1250],
        [ 0.6161,  0.3308, -0.4327,  0.4601, -0.0037,  0.0805, -0.2649, -0.1228],
        [ 0.0823,  0.9760, -0.9255,  1.0737, -0.5001, -0.6023,  0.4461,  0.3454]])...
	Related Layers:
		- parent layers: dropout_42_451, linear_114_452
		- child layers: cat_5_526
		- shares parents with no other layers
		- shares children with layers: matmul_66_441, matmul_70_465, matmul_72_477, matmul_74_489, matmul_76_501, matmul_78_513, matmul_80_525
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.1:1
	Time elapsed:  8.655E-05s
	Output of modules: blocks.4.sa.heads.1
	Lookup keys: -207, 491, blocks.4.sa.heads.1, blocks.4.sa.heads.1:1, matmul_68, matmul_68:1, matmul_68_453, matmul_68_453:1
--------------------------------------------
Layer linear_115_454, operation 458/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-1.0664,  0.8361,  0.2346,  2.0162,  1.2089,  0.7689,  0.6333,  0.7088],
        [ 0.3375,  3.5551,  3.8646,  0.9034,  3.1839,  3.1516, -2.7131,  3.9018],
        [-1.8363,  1.2405,  1.0632,  1.8423,  1.2396,  1.6801,  0.5898, -0.2551],
        [-1.6762,  0.9811,  1.1985,  1.8548,  0.6233,  1.4622,  0.9322,  0.1963],
        [-2.0995, -0.9411, -0.0478,  2.9723, -0.1908,  0.3081,  2.1306, -0.7867],
        [ 0.8020,  5.0559,  3.5872, -1.7031,  3.2836,  4.9588, -3.2605,  3.6046],
        [ 1.4363,  4.4966,  3.9260, -0.6002,  3.0215,  3.8883, -3.0348,  3.9970],
        [ 0.3614,  2.5292,  2.3748,  2.0335,  1.5169,  2.1418, -0.7727,  1.2310]])...
	Related Layers:
		- parent layers: layernorm_9_429
		- child layers: transpose_35_456
		- shares parents with layers: linear_109_430, linear_110_431, linear_111_440, linear_112_442, linear_113_443, linear_114_452, linear_116_455, linear_117_464, linear_118_466, linear_119_467, linear_120_476, linear_121_478, linear_122_479, linear_123_488, linear_124_490, linear_125_491, linear_126_500, linear_127_502, linear_128_503, linear_129_512, linear_130_514, linear_131_515, linear_132_524
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.2.key:1
	Time elapsed:  1.135E-04s
	Output of modules: blocks.4.sa.heads.2.key
	Output of bottom-level module: blocks.4.sa.heads.2.key:1
	Lookup keys: -206, 492, blocks.4.sa.heads.2.key, blocks.4.sa.heads.2.key:1, linear_115, linear_115:1, linear_115_454, linear_115_454:1
--------------------------------------------
Layer linear_116_455, operation 459/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.0747, -0.2253,  0.7644, -2.6521,  1.4836,  1.2428, -2.3632,  0.8876],
        [-0.5559, -0.2123,  1.0072, -0.6890,  0.7089,  1.6607, -1.5918,  0.5590],
        [ 0.2283,  2.0265,  1.9088, -1.7493,  2.1203,  1.2694, -2.6597,  1.4184],
        [ 0.5606,  2.3466,  1.8412, -2.0891,  2.0672,  1.9675, -3.1022,  2.0657],
        [ 0.0976,  2.0770,  2.1855, -3.2227,  1.8422,  1.8526, -3.7514,  2.4160],
        [-0.8471, -2.9675, -1.7771,  0.9299, -0.3387, -1.1238,  2.0258, -3.6160],
        [-0.5051, -2.1293, -0.8480,  1.1063, -0.0577, -0.6492,  2.1494, -2.4102],
        [ 0.7084,  1.1444,  2.2314, -1.6544,  2.6490,  1.1059, -2.4509,  1.1873]])...
	Related Layers:
		- parent layers: layernorm_9_429
		- child layers: matmul_69_457
		- shares parents with layers: linear_109_430, linear_110_431, linear_111_440, linear_112_442, linear_113_443, linear_114_452, linear_115_454, linear_117_464, linear_118_466, linear_119_467, linear_120_476, linear_121_478, linear_122_479, linear_123_488, linear_124_490, linear_125_491, linear_126_500, linear_127_502, linear_128_503, linear_129_512, linear_130_514, linear_131_515, linear_132_524
		- shares children with layers: transpose_35_456
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.2.query:1
	Time elapsed:  1.113E-04s
	Output of modules: blocks.4.sa.heads.2.query
	Output of bottom-level module: blocks.4.sa.heads.2.query:1
	Lookup keys: -205, 493, blocks.4.sa.heads.2.query, blocks.4.sa.heads.2.query:1, linear_116, linear_116:1, linear_116_455, linear_116_455:1
--------------------------------------------
Layer transpose_35_456, operation 460/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-1.0664,  0.3375, -1.8363, -1.6762, -2.0995,  0.8020,  1.4363,  0.3614],
        [ 0.8361,  3.5551,  1.2405,  0.9811, -0.9411,  5.0559,  4.4966,  2.5292],
        [ 0.2346,  3.8646,  1.0632,  1.1985, -0.0478,  3.5872,  3.9260,  2.3748],
        [ 2.0162,  0.9034,  1.8423,  1.8548,  2.9723, -1.7031, -0.6002,  2.0335],
        [ 1.2089,  3.1839,  1.2396,  0.6233, -0.1908,  3.2836,  3.0215,  1.5169],
        [ 0.7689,  3.1516,  1.6801,  1.4622,  0.3081,  4.9588,  3.8883,  2.1418],
        [ 0.6333, -2.7131,  0.5898,  0.9322,  2.1306, -3.2605, -3.0348, -0.7727],
        [ 0.7088,  3.9018, -0.2551,  0.1963, -0.7867,  3.6046,  3.9970,  1.2310]])...
	Related Layers:
		- parent layers: linear_115_454
		- child layers: matmul_69_457
		- shares parents with no other layers
		- shares children with layers: linear_116_455
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.4.sa.heads.2:1
	Time elapsed:  6.938E-05s
	Output of modules: none
	Lookup keys: -204, 494, transpose_35, transpose_35:1, transpose_35_456, transpose_35_456:1
--------------------------------------------
Layer matmul_69_457, operation 461/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[   7.5873,  124.5441,   14.1753,    9.3630,  -39.4271,  156.9084,
          144.1176,   52.8944],
        [  33.6088,  105.4784,   30.4237,   28.4972,   -1.7790,  123.4936,
          112.1648,   58.4648],
        [  29.7359,  199.7562,   34.3382,   27.9120,  -48.5572,  241.5836,
          225.4777,  101.7262],
        [  36.7649,  228.2823,   40.2943,   33.8039,  -52.9703,  275.8491,
          256.0171,  116.9900],
        [   6.6087,  229.5019,   21.3184,   13.2000,  -87.0194,  286.9723,
          270.9705,  102.3365],
        [ -14.1819, -195.8910,  -20.2134,  -20.0231,   81.1513, -242.3794,
         -242.8580, -116.0942],
        [  12.8509, -110.8120,    2.1483,    4.3203,   66.8342, -142.5669,
         -139.6874,  -50.8839],
        [  23.3591,  161.1808,   27.5346,   22.4095,  -36.4848,  187.4506,
          178.7109,   77.9762]])...
	Related Layers:
		- parent layers: linear_116_455, transpose_35_456
		- child layers: mul_35_458
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.2:1
	Time elapsed:  8.917E-05s
	Output of modules: none
	Lookup keys: -203, 495, matmul_69, matmul_69:1, matmul_69_457, matmul_69_457:1
--------------------------------------------
Layer mul_35_458, operation 462/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[  0.3872,   6.3556,   0.7234,   0.4778,  -2.0120,   8.0072,   7.3545,
           2.6993],
        [  1.7151,   5.3827,   1.5526,   1.4542,  -0.0908,   6.3020,   5.7239,
           2.9835],
        [  1.5175,  10.1938,   1.7523,   1.4244,  -2.4779,  12.3283,  11.5064,
           5.1912],
        [  1.8762,  11.6495,   2.0563,   1.7250,  -2.7031,  14.0769,  13.0648,
           5.9701],
        [  0.3372,  11.7117,   1.0879,   0.6736,  -4.4407,  14.6445,  13.8279,
           5.2223],
        [ -0.7237,  -9.9965,  -1.0315,  -1.0218,   4.1412, -12.3689, -12.3933,
          -5.9244],
        [  0.6558,  -5.6549,   0.1096,   0.2205,   3.4106,  -7.2753,  -7.1284,
          -2.5967],
        [  1.1920,   8.2252,   1.4051,   1.1436,  -1.8619,   9.5658,   9.1198,
           3.9792]])...
	Related Layers:
		- parent layers: matmul_69_457
		- child layers: maskedfill_35_461
		- shares parents with no other layers
		- shares children with layers: eq_35_460
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.4.sa.heads.2:1
	Time elapsed:  7.319E-05s
	Output of modules: none
	Lookup keys: -202, 496, mul_35, mul_35:1, mul_35_458, mul_35_458:1
--------------------------------------------
Layer getitem_35_459, operation 463/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_35
		- child layers: eq_35_460
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.012E-05s
	Output of modules: none
	Lookup keys: -200, 498, getitem_35, getitem_35:1, getitem_35_459, getitem_35_459:1
--------------------------------------------
Layer eq_35_460, operation 464/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_35_459
		- child layers: maskedfill_35_461
		- shares parents with no other layers
		- shares children with layers: mul_35_458
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.155E-05s
	Output of modules: none
	Lookup keys: -199, 499, eq_35, eq_35:1, eq_35_460, eq_35_460:1
--------------------------------------------
Layer maskedfill_35_461, operation 465/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[  0.3872,     -inf,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [  1.7151,   5.3827,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [  1.5175,  10.1938,   1.7523,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [  1.8762,  11.6495,   2.0563,   1.7250,     -inf,     -inf,     -inf,
             -inf],
        [  0.3372,  11.7117,   1.0879,   0.6736,  -4.4407,     -inf,     -inf,
             -inf],
        [ -0.7237,  -9.9965,  -1.0315,  -1.0218,   4.1412, -12.3689,     -inf,
             -inf],
        [  0.6558,  -5.6549,   0.1096,   0.2205,   3.4106,  -7.2753,  -7.1284,
             -inf],
        [  1.1920,   8.2252,   1.4051,   1.1436,  -1.8619,   9.5658,   9.1198,
           3.9792]])...
	Related Layers:
		- parent layers: mul_35_458, eq_35_460
		- child layers: softmax_35_462
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.4.sa.heads.2:1
	Time elapsed:  1.090E-04s
	Output of modules: none
	Lookup keys: -198, 500, maskedfill_35, maskedfill_35:1, maskedfill_35_461, maskedfill_35_461:1
--------------------------------------------
Layer softmax_35_462, operation 466/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.4902e-02, 9.7510e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.7051e-04, 9.9961e-01, 2.1565e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [5.6941e-05, 9.9983e-01, 6.8178e-05, 4.8955e-05, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.1484e-05, 9.9995e-01, 2.4328e-05, 1.6076e-05, 9.6622e-08, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [7.5676e-03, 7.1094e-07, 5.5627e-03, 5.6170e-03, 9.8125e-01, 6.6303e-08,
         0.0000e+00, 0.0000e+00],
        [5.5719e-02, 1.0123e-04, 3.2271e-02, 3.6053e-02, 8.7581e-01, 2.0024e-05,
         2.3194e-05, 0.0000e+00],
        [1.2109e-04, 1.3727e-01, 1.4985e-04, 1.1536e-04, 5.7125e-06, 5.2455e-01,
         3.3581e-01, 1.9659e-03]])...
	Related Layers:
		- parent layers: maskedfill_35_461
		- child layers: dropout_43_463
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.4.sa.heads.2:1
	Time elapsed:  8.941E-05s
	Output of modules: none
	Lookup keys: -197, 501, softmax_35, softmax_35:1, softmax_35_462, softmax_35_462:1
--------------------------------------------
Layer dropout_43_463, operation 467/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.4902e-02, 9.7510e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.7051e-04, 9.9961e-01, 2.1565e-04, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [5.6941e-05, 9.9983e-01, 6.8178e-05, 4.8955e-05, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.1484e-05, 9.9995e-01, 2.4328e-05, 1.6076e-05, 9.6622e-08, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [7.5676e-03, 7.1094e-07, 5.5627e-03, 5.6170e-03, 9.8125e-01, 6.6303e-08,
         0.0000e+00, 0.0000e+00],
        [5.5719e-02, 1.0123e-04, 3.2271e-02, 3.6053e-02, 8.7581e-01, 2.0024e-05,
         2.3194e-05, 0.0000e+00],
        [1.2109e-04, 1.3727e-01, 1.4985e-04, 1.1536e-04, 5.7125e-06, 5.2455e-01,
         3.3581e-01, 1.9659e-03]])...
	Related Layers:
		- parent layers: softmax_35_462
		- child layers: matmul_70_465
		- shares parents with no other layers
		- shares children with layers: linear_117_464
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.4.sa.heads.2.dropout:1
	Time elapsed:  5.460E-05s
	Output of modules: blocks.4.sa.heads.2.dropout
	Output of bottom-level module: blocks.4.sa.heads.2.dropout:1
	Lookup keys: -196, 502, blocks.4.sa.heads.2.dropout, blocks.4.sa.heads.2.dropout:1, dropout_43, dropout_43:1, dropout_43_463, dropout_43_463:1
--------------------------------------------
Layer linear_117_464, operation 468/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 3.1444e-01, -1.7989e-01, -4.7326e-02, -9.6059e-02,  6.6653e-01,
          3.9424e-02, -1.0636e-01,  4.5970e-01],
        [ 3.1907e-01,  7.5874e-01,  5.5642e-02, -3.9509e-01,  8.9147e-01,
         -5.7840e-04,  3.1731e-01, -1.2664e-01],
        [ 3.7240e-01,  1.9987e-01, -4.8345e-01, -6.3540e-02,  5.8362e-02,
         -7.7821e-01,  4.2499e-01,  6.7296e-02],
        [ 5.5103e-01,  1.2809e-01, -1.6297e-01,  1.3018e-01,  3.0794e-02,
         -6.6984e-01,  5.7845e-01,  4.7396e-02],
        [ 5.4117e-01, -3.6986e-01, -1.1714e-01, -3.3321e-01,  2.3107e-01,
         -9.7882e-02,  1.1955e+00,  3.9674e-01],
        [-6.1319e-01,  4.2514e-01, -5.2891e-01,  1.3147e-01,  1.0190e+00,
         -1.0305e-01, -2.3084e+00, -8.2275e-02],
        [-5.6377e-01,  9.3898e-01,  3.6653e-01,  5.2162e-01,  4.6562e-01,
          2.7634e-01, -1.1158e+00, -3.1925e-01],
        [ 1.0660e+00,  5.6965e-01, -2.3842e-01, -6.0103e-01,  5.0371e-01,
         -2.4282e-01,  2.0002e-01,  3.8320e-01]])...
	Related Layers:
		- parent layers: layernorm_9_429
		- child layers: matmul_70_465
		- shares parents with layers: linear_109_430, linear_110_431, linear_111_440, linear_112_442, linear_113_443, linear_114_452, linear_115_454, linear_116_455, linear_118_466, linear_119_467, linear_120_476, linear_121_478, linear_122_479, linear_123_488, linear_124_490, linear_125_491, linear_126_500, linear_127_502, linear_128_503, linear_129_512, linear_130_514, linear_131_515, linear_132_524
		- shares children with layers: dropout_43_463
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.2.value:1
	Time elapsed:  1.202E-04s
	Output of modules: blocks.4.sa.heads.2.value
	Output of bottom-level module: blocks.4.sa.heads.2.value:1
	Lookup keys: -195, 503, blocks.4.sa.heads.2.value, blocks.4.sa.heads.2.value:1, linear_117, linear_117:1, linear_117_464, linear_117_464:1
--------------------------------------------
Layer matmul_70_465, operation 469/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 3.1444e-01, -1.7989e-01, -4.7326e-02, -9.6059e-02,  6.6653e-01,
          3.9424e-02, -1.0636e-01,  4.5970e-01],
        [ 3.1895e-01,  7.3537e-01,  5.3078e-02, -3.8764e-01,  8.8587e-01,
          4.1775e-04,  3.0676e-01, -1.1204e-01],
        [ 3.1908e-01,  7.5846e-01,  5.5509e-02, -3.9497e-01,  8.9126e-01,
         -7.3928e-04,  3.1726e-01, -1.2650e-01],
        [ 3.1908e-01,  7.5862e-01,  5.5589e-02, -3.9503e-01,  8.9136e-01,
         -6.6191e-04,  3.1731e-01, -1.2658e-01],
        [ 3.1907e-01,  7.5871e-01,  5.5625e-02, -3.9507e-01,  8.9144e-01,
         -6.0763e-04,  3.1731e-01, -1.2662e-01],
        [ 5.3857e-01, -3.6245e-01, -1.1891e-01, -3.2731e-01,  2.3228e-01,
         -1.0384e-01,  1.1779e+00,  3.9342e-01],
        [ 5.2338e-01, -3.2277e-01, -1.2670e-01, -2.9456e-01,  2.4263e-01,
         -1.3279e-01,  1.0757e+00,  3.7694e-01],
        [-4.6492e-01,  6.4363e-01, -1.4728e-01,  1.8870e-01,  8.1432e-01,
          3.7995e-02, -1.5415e+00, -1.6692e-01]])...
	Related Layers:
		- parent layers: dropout_43_463, linear_117_464
		- child layers: cat_5_526
		- shares parents with no other layers
		- shares children with layers: matmul_66_441, matmul_68_453, matmul_72_477, matmul_74_489, matmul_76_501, matmul_78_513, matmul_80_525
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.2:1
	Time elapsed:  8.416E-05s
	Output of modules: blocks.4.sa.heads.2
	Lookup keys: -194, 504, blocks.4.sa.heads.2, blocks.4.sa.heads.2:1, matmul_70, matmul_70:1, matmul_70_465, matmul_70_465:1
--------------------------------------------
Layer linear_118_466, operation 470/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-4.0314, -2.8554, -1.3177, -3.9660, -3.5317,  0.4008,  3.4337,  3.0226],
        [-1.7044, -2.0195, -1.8027, -2.1754, -2.6730,  0.2223,  2.8186,  2.3398],
        [-2.6649, -0.9013, -1.5059, -2.1547, -1.8269,  0.7079,  2.5046,  2.7137],
        [-2.4735, -1.1222, -0.9336, -2.1455, -2.0870,  0.6313,  2.9230,  2.4179],
        [-3.2178, -1.0059, -0.5699, -2.4259, -2.0390,  0.3516,  1.8806,  2.5921],
        [ 0.3209, -2.2479, -0.8449, -0.2066, -3.7592, -0.1975,  3.0950,  0.3904],
        [-0.0407, -1.1018,  0.5648, -0.1761, -3.1349, -0.8791,  1.9227,  1.3280],
        [-2.2018, -0.5864, -0.5674, -2.3243, -2.2328,  0.9257,  1.4770,  2.6395]])...
	Related Layers:
		- parent layers: layernorm_9_429
		- child layers: transpose_36_468
		- shares parents with layers: linear_109_430, linear_110_431, linear_111_440, linear_112_442, linear_113_443, linear_114_452, linear_115_454, linear_116_455, linear_117_464, linear_119_467, linear_120_476, linear_121_478, linear_122_479, linear_123_488, linear_124_490, linear_125_491, linear_126_500, linear_127_502, linear_128_503, linear_129_512, linear_130_514, linear_131_515, linear_132_524
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.3.key:1
	Time elapsed:  1.140E-04s
	Output of modules: blocks.4.sa.heads.3.key
	Output of bottom-level module: blocks.4.sa.heads.3.key:1
	Lookup keys: -193, 505, blocks.4.sa.heads.3.key, blocks.4.sa.heads.3.key:1, linear_118, linear_118:1, linear_118_466, linear_118_466:1
--------------------------------------------
Layer linear_119_467, operation 471/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.3163,  1.2185,  0.5163,  0.5393,  0.8689,  0.8848, -2.3166, -0.8146],
        [ 0.1857,  1.9765,  2.7416,  0.3147, -0.9550, -0.8923, -0.1086, -1.4109],
        [ 0.2100,  2.7540,  1.3478,  0.9281,  0.6121, -0.1922, -2.5224, -1.4737],
        [-0.2374,  2.2153,  1.5762,  0.8964,  1.1047,  0.0300, -2.8542, -1.8010],
        [ 0.3270,  1.1858,  0.7327,  1.2621,  1.9414,  0.6745, -2.4487, -1.9818],
        [-0.2541,  0.1483, -0.1495, -1.0842, -0.1109, -0.2388,  0.4485,  1.2904],
        [-0.0929,  1.8037,  1.4093, -1.1949, -1.0831, -1.2483,  1.1790,  0.6225],
        [ 0.0486,  2.0901,  2.8348,  0.7410, -1.0221, -1.6549, -0.4448, -0.0952]])...
	Related Layers:
		- parent layers: layernorm_9_429
		- child layers: matmul_71_469
		- shares parents with layers: linear_109_430, linear_110_431, linear_111_440, linear_112_442, linear_113_443, linear_114_452, linear_115_454, linear_116_455, linear_117_464, linear_118_466, linear_120_476, linear_121_478, linear_122_479, linear_123_488, linear_124_490, linear_125_491, linear_126_500, linear_127_502, linear_128_503, linear_129_512, linear_130_514, linear_131_515, linear_132_524
		- shares children with layers: transpose_36_468
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.3.query:1
	Time elapsed:  1.121E-04s
	Output of modules: blocks.4.sa.heads.3.query
	Output of bottom-level module: blocks.4.sa.heads.3.query:1
	Lookup keys: -192, 506, blocks.4.sa.heads.3.query, blocks.4.sa.heads.3.query:1, linear_119, linear_119:1, linear_119_467, linear_119_467:1
--------------------------------------------
Layer transpose_36_468, operation 472/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-4.0314, -1.7044, -2.6649, -2.4735, -3.2178,  0.3209, -0.0407, -2.2018],
        [-2.8554, -2.0195, -0.9013, -1.1222, -1.0059, -2.2479, -1.1018, -0.5864],
        [-1.3177, -1.8027, -1.5059, -0.9336, -0.5699, -0.8449,  0.5648, -0.5674],
        [-3.9660, -2.1754, -2.1547, -2.1455, -2.4259, -0.2066, -0.1761, -2.3243],
        [-3.5317, -2.6730, -1.8269, -2.0870, -2.0390, -3.7592, -3.1349, -2.2328],
        [ 0.4008,  0.2223,  0.7079,  0.6313,  0.3516, -0.1975, -0.8791,  0.9257],
        [ 3.4337,  2.8186,  2.5046,  2.9230,  1.8806,  3.0950,  1.9227,  1.4770],
        [ 3.0226,  2.3398,  2.7137,  2.4179,  2.5921,  0.3904,  1.3280,  2.6395]])...
	Related Layers:
		- parent layers: linear_118_466
		- child layers: matmul_71_469
		- shares parents with no other layers
		- shares children with layers: linear_119_467
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.4.sa.heads.3:1
	Time elapsed:  5.841E-05s
	Output of modules: none
	Lookup keys: -191, 507, transpose_36, transpose_36:1, transpose_36_468, transpose_36_468:1
--------------------------------------------
Layer matmul_71_469, operation 473/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-178.4136, -151.6403, -124.4854, -126.0510, -100.6219, -148.6517,
         -120.2476, -115.2713],
        [ -28.0477,  -28.8406,  -26.4199,  -20.7280,   -7.8529,  -28.3770,
           -1.8176,  -15.6078],
        [-170.0708, -139.7580, -122.7810, -120.1074,  -91.5339, -150.6071,
         -107.3302, -111.3912],
        [-179.1063, -149.8071, -129.9855, -127.8297,  -97.7624, -157.7551,
         -115.2016, -117.9227],
        [-190.2129, -150.9582, -128.2785, -131.9109, -113.7430, -142.3442,
         -109.5077, -112.8912],
        [ -14.7020,  -23.0646,  -18.1367,  -11.9601,    3.4996,  -56.5212,
          -49.2990,  -32.1138],
        [  30.7378,   16.3001,   13.8814,   23.1362,   35.8513,  -15.0342,
            7.1759,   10.0415],
        [ -46.9507,  -33.9082,  -40.6174,  -35.5975,  -20.2468,  -47.5837,
            6.0361,  -19.4909]])...
	Related Layers:
		- parent layers: linear_119_467, transpose_36_468
		- child layers: mul_36_470
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.3:1
	Time elapsed:  8.774E-05s
	Output of modules: none
	Lookup keys: -190, 508, matmul_71, matmul_71:1, matmul_71_469, matmul_71_469:1
--------------------------------------------
Layer mul_36_470, operation 474/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-9.1046, -7.7384, -6.3526, -6.4325, -5.1348, -7.5858, -6.1364, -5.8824],
        [-1.4313, -1.4718, -1.3482, -1.0578, -0.4007, -1.4481, -0.0928, -0.7965],
        [-8.6789, -7.1320, -6.2656, -6.1292, -4.6711, -7.6856, -5.4772, -5.6844],
        [-9.1400, -7.6448, -6.6333, -6.5233, -4.9889, -8.0504, -5.8789, -6.0177],
        [-9.7068, -7.7036, -6.5462, -6.7316, -5.8044, -7.2640, -5.5883, -5.7610],
        [-0.7503, -1.1770, -0.9255, -0.6103,  0.1786, -2.8843, -2.5158, -1.6388],
        [ 1.5686,  0.8318,  0.7084,  1.1807,  1.8295, -0.7672,  0.3662,  0.5124],
        [-2.3959, -1.7304, -2.0727, -1.8166, -1.0332, -2.4282,  0.3080, -0.9946]])...
	Related Layers:
		- parent layers: matmul_71_469
		- child layers: maskedfill_36_473
		- shares parents with no other layers
		- shares children with layers: eq_36_472
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.4.sa.heads.3:1
	Time elapsed:  6.890E-05s
	Output of modules: none
	Lookup keys: -189, 509, mul_36, mul_36:1, mul_36_470, mul_36_470:1
--------------------------------------------
Layer getitem_36_471, operation 475/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_36
		- child layers: eq_36_472
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.274E-05s
	Output of modules: none
	Lookup keys: -187, 511, getitem_36, getitem_36:1, getitem_36_471, getitem_36_471:1
--------------------------------------------
Layer eq_36_472, operation 476/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_36_471
		- child layers: maskedfill_36_473
		- shares parents with no other layers
		- shares children with layers: mul_36_470
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  7.796E-05s
	Output of modules: none
	Lookup keys: -186, 512, eq_36, eq_36:1, eq_36_472, eq_36_472:1
--------------------------------------------
Layer maskedfill_36_473, operation 477/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-9.1046,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-1.4313, -1.4718,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-8.6789, -7.1320, -6.2656,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-9.1400, -7.6448, -6.6333, -6.5233,    -inf,    -inf,    -inf,    -inf],
        [-9.7068, -7.7036, -6.5462, -6.7316, -5.8044,    -inf,    -inf,    -inf],
        [-0.7503, -1.1770, -0.9255, -0.6103,  0.1786, -2.8843,    -inf,    -inf],
        [ 1.5686,  0.8318,  0.7084,  1.1807,  1.8295, -0.7672,  0.3662,    -inf],
        [-2.3959, -1.7304, -2.0727, -1.8166, -1.0332, -2.4282,  0.3080, -0.9946]])...
	Related Layers:
		- parent layers: mul_36_470, eq_36_472
		- child layers: softmax_36_474
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.4.sa.heads.3:1
	Time elapsed:  8.321E-05s
	Output of modules: none
	Lookup keys: -185, 513, maskedfill_36, maskedfill_36:1, maskedfill_36_473, maskedfill_36_473:1
--------------------------------------------
Layer softmax_36_474, operation 478/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5101, 0.4899, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0593, 0.2785, 0.6622, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0318, 0.1420, 0.3904, 0.4358, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0099, 0.0733, 0.2333, 0.1938, 0.4898, 0.0000, 0.0000, 0.0000],
        [0.1589, 0.1037, 0.1334, 0.1828, 0.4024, 0.0188, 0.0000, 0.0000],
        [0.2339, 0.1120, 0.0990, 0.1587, 0.3036, 0.0226, 0.0703, 0.0000],
        [0.0333, 0.0649, 0.0461, 0.0595, 0.1303, 0.0323, 0.4982, 0.1354]])...
	Related Layers:
		- parent layers: maskedfill_36_473
		- child layers: dropout_44_475
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.4.sa.heads.3:1
	Time elapsed:  6.914E-05s
	Output of modules: none
	Lookup keys: -184, 514, softmax_36, softmax_36:1, softmax_36_474, softmax_36_474:1
--------------------------------------------
Layer dropout_44_475, operation 479/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5101, 0.4899, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0593, 0.2785, 0.6622, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0318, 0.1420, 0.3904, 0.4358, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0099, 0.0733, 0.2333, 0.1938, 0.4898, 0.0000, 0.0000, 0.0000],
        [0.1589, 0.1037, 0.1334, 0.1828, 0.4024, 0.0188, 0.0000, 0.0000],
        [0.2339, 0.1120, 0.0990, 0.1587, 0.3036, 0.0226, 0.0703, 0.0000],
        [0.0333, 0.0649, 0.0461, 0.0595, 0.1303, 0.0323, 0.4982, 0.1354]])...
	Related Layers:
		- parent layers: softmax_36_474
		- child layers: matmul_72_477
		- shares parents with no other layers
		- shares children with layers: linear_120_476
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.4.sa.heads.3.dropout:1
	Time elapsed:  5.579E-05s
	Output of modules: blocks.4.sa.heads.3.dropout
	Output of bottom-level module: blocks.4.sa.heads.3.dropout:1
	Lookup keys: -183, 515, blocks.4.sa.heads.3.dropout, blocks.4.sa.heads.3.dropout:1, dropout_44, dropout_44:1, dropout_44_475, dropout_44_475:1
--------------------------------------------
Layer linear_120_476, operation 480/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.8225, -0.3555, -0.1299, -0.1402, -0.5921,  0.3409,  0.2440,  0.0026],
        [ 0.6356,  0.0978, -0.2361,  0.3160, -0.9610,  0.2399, -0.5675,  0.0145],
        [ 0.9346,  0.6587,  0.5447, -0.1368, -0.0856,  0.1226,  0.2722, -0.2698],
        [ 0.7679,  0.2078,  0.8093,  0.5649, -0.3236,  0.0547,  0.2171, -0.4095],
        [ 1.7955,  0.0807,  0.1563, -0.0170, -0.3159, -0.3224,  0.6300,  0.0544],
        [-1.4355, -0.3996, -1.4062, -0.2606, -0.3827, -0.0310, -0.5162,  1.1802],
        [-0.8480, -0.4575, -1.5613, -0.4346, -0.7873,  0.9635, -1.1724,  1.2300],
        [ 1.0294,  0.0914,  0.2096,  0.3092, -0.9396,  1.0071, -0.3876,  0.0488]])...
	Related Layers:
		- parent layers: layernorm_9_429
		- child layers: matmul_72_477
		- shares parents with layers: linear_109_430, linear_110_431, linear_111_440, linear_112_442, linear_113_443, linear_114_452, linear_115_454, linear_116_455, linear_117_464, linear_118_466, linear_119_467, linear_121_478, linear_122_479, linear_123_488, linear_124_490, linear_125_491, linear_126_500, linear_127_502, linear_128_503, linear_129_512, linear_130_514, linear_131_515, linear_132_524
		- shares children with layers: dropout_44_475
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.3.value:1
	Time elapsed:  1.183E-04s
	Output of modules: blocks.4.sa.heads.3.value
	Output of bottom-level module: blocks.4.sa.heads.3.value:1
	Lookup keys: -182, 516, blocks.4.sa.heads.3.value, blocks.4.sa.heads.3.value:1, linear_120, linear_120:1, linear_120_476, linear_120_476:1
--------------------------------------------
Layer matmul_72_477, operation 481/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.8225, -0.3555, -0.1299, -0.1402, -0.5921,  0.3409,  0.2440,  0.0026],
        [ 0.7309, -0.1334, -0.1819,  0.0833, -0.7728,  0.2914, -0.1535,  0.0085],
        [ 0.8447,  0.4424,  0.2873, -0.0109, -0.3594,  0.1682,  0.0367, -0.1745],
        [ 0.8159,  0.3503,  0.5277,  0.2332, -0.3297,  0.1166,  0.1280, -0.2816],
        [ 1.3009,  0.2371,  0.3419,  0.0910, -0.3137, -0.0977,  0.3749, -0.1145],
        [ 1.1571,  0.1045,  0.2119,  0.0838, -0.3987, -0.0249,  0.2997, -0.0648],
        [ 0.9309,  0.0093,  0.0315,  0.0371, -0.4658,  0.0965,  0.1521,  0.0402],
        [ 0.0619, -0.1807, -0.7208, -0.1421, -0.6783,  0.6092, -0.5744,  0.6288]])...
	Related Layers:
		- parent layers: dropout_44_475, linear_120_476
		- child layers: cat_5_526
		- shares parents with no other layers
		- shares children with layers: matmul_66_441, matmul_68_453, matmul_70_465, matmul_74_489, matmul_76_501, matmul_78_513, matmul_80_525
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.3:1
	Time elapsed:  8.345E-05s
	Output of modules: blocks.4.sa.heads.3
	Lookup keys: -181, 517, blocks.4.sa.heads.3, blocks.4.sa.heads.3:1, matmul_72, matmul_72:1, matmul_72_477, matmul_72_477:1
--------------------------------------------
Layer linear_121_478, operation 482/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 2.2939e+00, -2.8105e+00,  1.0397e+00, -1.8271e+00,  2.2168e+00,
         -6.3601e-01, -1.4703e+00, -1.7602e+00],
        [ 3.4188e+00,  1.1131e-01, -3.3503e-01,  5.8922e-01,  9.2995e-01,
         -5.9641e-01, -1.5990e+00,  4.5203e-01],
        [ 2.6324e+00, -1.5054e+00,  5.9271e-01, -1.4292e+00,  1.9806e+00,
         -1.2838e+00, -1.0081e+00, -1.0809e+00],
        [ 2.6253e+00, -1.4055e+00,  5.2155e-01, -9.9088e-01,  1.5520e+00,
         -1.0048e+00, -8.8269e-01, -1.0434e+00],
        [ 1.8626e+00, -1.9321e+00,  6.3488e-02, -2.4060e+00,  1.1178e+00,
         -1.0315e+00,  2.0571e-01, -1.3204e+00],
        [ 1.9509e+00, -1.6408e+00,  3.2059e+00,  1.6083e+00,  2.2686e+00,
          1.8073e+00, -4.0454e+00, -1.6332e-01],
        [ 2.6380e+00,  7.0865e-01,  1.9224e+00,  2.6916e+00,  9.1107e-01,
          1.0612e+00, -3.1283e+00,  2.3408e-03],
        [ 2.3105e+00, -1.8432e+00,  1.0828e+00, -7.3160e-01,  1.9598e+00,
         -1.2217e-02, -2.3407e+00, -6.7875e-01]])...
	Related Layers:
		- parent layers: layernorm_9_429
		- child layers: transpose_37_480
		- shares parents with layers: linear_109_430, linear_110_431, linear_111_440, linear_112_442, linear_113_443, linear_114_452, linear_115_454, linear_116_455, linear_117_464, linear_118_466, linear_119_467, linear_120_476, linear_122_479, linear_123_488, linear_124_490, linear_125_491, linear_126_500, linear_127_502, linear_128_503, linear_129_512, linear_130_514, linear_131_515, linear_132_524
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.4.key:1
	Time elapsed:  1.128E-04s
	Output of modules: blocks.4.sa.heads.4.key
	Output of bottom-level module: blocks.4.sa.heads.4.key:1
	Lookup keys: -180, 518, blocks.4.sa.heads.4.key, blocks.4.sa.heads.4.key:1, linear_121, linear_121:1, linear_121_478, linear_121_478:1
--------------------------------------------
Layer linear_122_479, operation 483/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.4842, -0.6761, -0.0908, -2.0776, -1.4717, -0.8595,  0.6003, -0.8908],
        [-1.9939, -1.0004,  1.0101, -1.5795, -0.2987,  0.9937, -1.1754,  1.2947],
        [-0.2581,  0.0096, -0.9152, -1.9726, -2.2370, -1.4502,  0.6369,  0.1847],
        [ 0.1444, -0.1675, -0.6150, -1.6886, -2.3635, -1.5260,  0.7772,  0.0494],
        [-0.3883, -0.8297, -0.0147, -0.9724, -1.8721, -0.7652,  0.7932, -0.5627],
        [ 0.5657,  2.3994, -1.3479, -0.6995, -0.3387, -0.3143, -0.9207,  0.0602],
        [-2.0878, -0.0239, -0.1744, -2.5208,  0.8309,  1.7777, -0.8709, -0.2893],
        [-1.8310, -1.2370, -0.8430, -2.3613,  0.1494,  0.4204,  0.2018, -0.1600]])...
	Related Layers:
		- parent layers: layernorm_9_429
		- child layers: matmul_73_481
		- shares parents with layers: linear_109_430, linear_110_431, linear_111_440, linear_112_442, linear_113_443, linear_114_452, linear_115_454, linear_116_455, linear_117_464, linear_118_466, linear_119_467, linear_120_476, linear_121_478, linear_123_488, linear_124_490, linear_125_491, linear_126_500, linear_127_502, linear_128_503, linear_129_512, linear_130_514, linear_131_515, linear_132_524
		- shares children with layers: transpose_37_480
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.4.query:1
	Time elapsed:  1.173E-04s
	Output of modules: blocks.4.sa.heads.4.query
	Output of bottom-level module: blocks.4.sa.heads.4.query:1
	Lookup keys: -179, 519, blocks.4.sa.heads.4.query, blocks.4.sa.heads.4.query:1, linear_122, linear_122:1, linear_122_479, linear_122_479:1
--------------------------------------------
Layer transpose_37_480, operation 484/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 2.2939e+00,  3.4188e+00,  2.6324e+00,  2.6253e+00,  1.8626e+00,
          1.9509e+00,  2.6380e+00,  2.3105e+00],
        [-2.8105e+00,  1.1131e-01, -1.5054e+00, -1.4055e+00, -1.9321e+00,
         -1.6408e+00,  7.0865e-01, -1.8432e+00],
        [ 1.0397e+00, -3.3503e-01,  5.9271e-01,  5.2155e-01,  6.3488e-02,
          3.2059e+00,  1.9224e+00,  1.0828e+00],
        [-1.8271e+00,  5.8922e-01, -1.4292e+00, -9.9088e-01, -2.4060e+00,
          1.6083e+00,  2.6916e+00, -7.3160e-01],
        [ 2.2168e+00,  9.2995e-01,  1.9806e+00,  1.5520e+00,  1.1178e+00,
          2.2686e+00,  9.1107e-01,  1.9598e+00],
        [-6.3601e-01, -5.9641e-01, -1.2838e+00, -1.0048e+00, -1.0315e+00,
          1.8073e+00,  1.0612e+00, -1.2217e-02],
        [-1.4703e+00, -1.5990e+00, -1.0081e+00, -8.8269e-01,  2.0571e-01,
         -4.0454e+00, -3.1283e+00, -2.3407e+00],
        [-1.7602e+00,  4.5203e-01, -1.0809e+00, -1.0434e+00, -1.3204e+00,
         -1.6332e-01,  2.3408e-03, -6.7875e-01]])...
	Related Layers:
		- parent layers: linear_121_478
		- child layers: matmul_73_481
		- shares parents with no other layers
		- shares children with layers: linear_122_479
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.4.sa.heads.4:1
	Time elapsed:  5.794E-05s
	Output of modules: none
	Lookup keys: -178, 520, transpose_37, transpose_37:1, transpose_37_480, transpose_37_480:1
--------------------------------------------
Layer matmul_73_481, operation 485/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ 46.3657, -18.5630,  33.2477,  32.5145,  46.3726, -11.2399, -29.9549,
          24.7378],
        [  0.5870, -26.8844,  -6.0683,  -8.4452,   3.6854,   4.1066, -25.9597,
           0.2064],
        [ 15.4723, -13.5944,  14.3945,  12.9022,  24.1163, -31.3174, -30.5393,
           4.7878],
        [ 17.7184, -11.9627,  16.2969,  15.4284,  25.0585, -26.4253, -26.1863,
           6.8709],
        [  4.6925, -31.8693,   2.3753,   2.6949,  14.8644, -31.7908, -41.8764,
          -4.8886],
        [ -0.2473,  40.3751,  10.9523,  12.1490,  -3.0903,  18.8975,  45.6587,
          18.5511],
        [  6.4171, -31.6644,  -0.5889,  -3.9090,  11.0417, -17.0780, -40.5374,
          -3.5818],
        [ 14.7170, -40.7695,   5.0548,   2.8023,  25.4113, -32.5151, -60.1607,
          -1.2604]])...
	Related Layers:
		- parent layers: linear_122_479, transpose_37_480
		- child layers: mul_37_482
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.4:1
	Time elapsed:  9.274E-05s
	Output of modules: none
	Lookup keys: -177, 521, matmul_73, matmul_73:1, matmul_73_481, matmul_73_481:1
--------------------------------------------
Layer mul_37_482, operation 486/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ 2.3661, -0.9473,  1.6967,  1.6592,  2.3664, -0.5736, -1.5286,  1.2624],
        [ 0.0300, -1.3719, -0.3097, -0.4310,  0.1881,  0.2096, -1.3247,  0.0105],
        [ 0.7896, -0.6937,  0.7346,  0.6584,  1.2307, -1.5982, -1.5585,  0.2443],
        [ 0.9042, -0.6105,  0.8316,  0.7873,  1.2788, -1.3485, -1.3363,  0.3506],
        [ 0.2395, -1.6263,  0.1212,  0.1375,  0.7585, -1.6223, -2.1370, -0.2495],
        [-0.0126,  2.0604,  0.5589,  0.6200, -0.1577,  0.9644,  2.3300,  0.9467],
        [ 0.3275, -1.6159, -0.0301, -0.1995,  0.5635, -0.8715, -2.0687, -0.1828],
        [ 0.7510, -2.0805,  0.2580,  0.1430,  1.2968, -1.6593, -3.0701, -0.0643]])...
	Related Layers:
		- parent layers: matmul_73_481
		- child layers: maskedfill_37_485
		- shares parents with no other layers
		- shares children with layers: eq_37_484
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.4.sa.heads.4:1
	Time elapsed:  6.819E-05s
	Output of modules: none
	Lookup keys: -176, 522, mul_37, mul_37:1, mul_37_482, mul_37_482:1
--------------------------------------------
Layer getitem_37_483, operation 487/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_37
		- child layers: eq_37_484
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.227E-05s
	Output of modules: none
	Lookup keys: -174, 524, getitem_37, getitem_37:1, getitem_37_483, getitem_37_483:1
--------------------------------------------
Layer eq_37_484, operation 488/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_37_483
		- child layers: maskedfill_37_485
		- shares parents with no other layers
		- shares children with layers: mul_37_482
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  7.868E-05s
	Output of modules: none
	Lookup keys: -173, 525, eq_37, eq_37:1, eq_37_484, eq_37_484:1
--------------------------------------------
Layer maskedfill_37_485, operation 489/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ 2.3661,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [ 0.0300, -1.3719,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [ 0.7896, -0.6937,  0.7346,    -inf,    -inf,    -inf,    -inf,    -inf],
        [ 0.9042, -0.6105,  0.8316,  0.7873,    -inf,    -inf,    -inf,    -inf],
        [ 0.2395, -1.6263,  0.1212,  0.1375,  0.7585,    -inf,    -inf,    -inf],
        [-0.0126,  2.0604,  0.5589,  0.6200, -0.1577,  0.9644,    -inf,    -inf],
        [ 0.3275, -1.6159, -0.0301, -0.1995,  0.5635, -0.8715, -2.0687,    -inf],
        [ 0.7510, -2.0805,  0.2580,  0.1430,  1.2968, -1.6593, -3.0701, -0.0643]])...
	Related Layers:
		- parent layers: mul_37_482, eq_37_484
		- child layers: softmax_37_486
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.4.sa.heads.4:1
	Time elapsed:  8.249E-05s
	Output of modules: none
	Lookup keys: -172, 526, maskedfill_37, maskedfill_37:1, maskedfill_37_485, maskedfill_37_485:1
--------------------------------------------
Layer softmax_37_486, operation 490/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.8025, 0.1975, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.4601, 0.1044, 0.4355, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3290, 0.0723, 0.3060, 0.2927, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2161, 0.0335, 0.1920, 0.1952, 0.3632, 0.0000, 0.0000, 0.0000],
        [0.0620, 0.4930, 0.1098, 0.1168, 0.0536, 0.1648, 0.0000, 0.0000],
        [0.2444, 0.0350, 0.1709, 0.1443, 0.3094, 0.0737, 0.0223, 0.0000],
        [0.2225, 0.0131, 0.1359, 0.1211, 0.3840, 0.0200, 0.0049, 0.0985]])...
	Related Layers:
		- parent layers: maskedfill_37_485
		- child layers: dropout_45_487
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.4.sa.heads.4:1
	Time elapsed:  7.224E-05s
	Output of modules: none
	Lookup keys: -171, 527, softmax_37, softmax_37:1, softmax_37_486, softmax_37_486:1
--------------------------------------------
Layer dropout_45_487, operation 491/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.8025, 0.1975, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.4601, 0.1044, 0.4355, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3290, 0.0723, 0.3060, 0.2927, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2161, 0.0335, 0.1920, 0.1952, 0.3632, 0.0000, 0.0000, 0.0000],
        [0.0620, 0.4930, 0.1098, 0.1168, 0.0536, 0.1648, 0.0000, 0.0000],
        [0.2444, 0.0350, 0.1709, 0.1443, 0.3094, 0.0737, 0.0223, 0.0000],
        [0.2225, 0.0131, 0.1359, 0.1211, 0.3840, 0.0200, 0.0049, 0.0985]])...
	Related Layers:
		- parent layers: softmax_37_486
		- child layers: matmul_74_489
		- shares parents with no other layers
		- shares children with layers: linear_123_488
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.4.sa.heads.4.dropout:1
	Time elapsed:  5.412E-05s
	Output of modules: blocks.4.sa.heads.4.dropout
	Output of bottom-level module: blocks.4.sa.heads.4.dropout:1
	Lookup keys: -170, 528, blocks.4.sa.heads.4.dropout, blocks.4.sa.heads.4.dropout:1, dropout_45, dropout_45:1, dropout_45_487, dropout_45_487:1
--------------------------------------------
Layer linear_123_488, operation 492/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.2012, -0.0401, -0.4388,  0.3890, -0.6674,  1.1482,  0.1266,  0.4488],
        [-0.0108, -0.7060, -0.9616, -0.2779, -0.2704,  1.4234,  0.2359, -0.1871],
        [ 0.3489, -0.1870, -0.6824,  0.4401, -0.9041,  1.4832, -0.1098,  0.1658],
        [ 0.2396,  0.1816, -0.7485,  0.3978, -1.0239,  1.4448,  0.0203,  0.4999],
        [-0.3141,  0.4609, -1.3867,  0.2408, -0.7468,  1.0045, -0.4542, -0.2857],
        [ 0.5531, -0.8708,  0.6284, -0.0344, -0.8545, -0.7716,  0.5351,  0.5997],
        [-0.1393, -0.4269, -0.0829,  0.1239, -0.6309,  0.4763,  0.3168,  0.1748],
        [-0.4026,  0.6489, -0.9082,  1.1630, -1.0273,  1.2591,  0.1767, -0.4762]])...
	Related Layers:
		- parent layers: layernorm_9_429
		- child layers: matmul_74_489
		- shares parents with layers: linear_109_430, linear_110_431, linear_111_440, linear_112_442, linear_113_443, linear_114_452, linear_115_454, linear_116_455, linear_117_464, linear_118_466, linear_119_467, linear_120_476, linear_121_478, linear_122_479, linear_124_490, linear_125_491, linear_126_500, linear_127_502, linear_128_503, linear_129_512, linear_130_514, linear_131_515, linear_132_524
		- shares children with layers: dropout_45_487
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.4.value:1
	Time elapsed:  1.194E-04s
	Output of modules: blocks.4.sa.heads.4.value
	Output of bottom-level module: blocks.4.sa.heads.4.value:1
	Lookup keys: -169, 529, blocks.4.sa.heads.4.value, blocks.4.sa.heads.4.value:1, linear_123, linear_123:1, linear_123_488, linear_123_488:1
--------------------------------------------
Layer matmul_74_489, operation 493/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.2012, -0.0401, -0.4388,  0.3890, -0.6674,  1.1482,  0.1266,  0.4488],
        [-0.1636, -0.1716, -0.5420,  0.2573, -0.5890,  1.2026,  0.1482,  0.3232],
        [ 0.0582, -0.1736, -0.5994,  0.3417, -0.7290,  1.3228,  0.0350,  0.2592],
        [ 0.1099, -0.0683, -0.6418,  0.3590, -0.8155,  1.3574,  0.0310,  0.3312],
        [-0.0442,  0.1346, -0.9078,  0.3244, -0.7980,  1.2275, -0.1468,  0.1164],
        [ 0.1228, -0.4686, -0.6345, -0.0108, -0.5744,  1.0313,  0.1783,  0.0956],
        [-0.0149,  0.0287, -0.7502,  0.2927, -0.7829,  1.0570, -0.0707,  0.1633],
        [-0.1184,  0.1998, -0.9034,  0.3978, -0.8070,  1.1474, -0.1260,  0.0367]])...
	Related Layers:
		- parent layers: dropout_45_487, linear_123_488
		- child layers: cat_5_526
		- shares parents with no other layers
		- shares children with layers: matmul_66_441, matmul_68_453, matmul_70_465, matmul_72_477, matmul_76_501, matmul_78_513, matmul_80_525
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.4:1
	Time elapsed:  8.154E-05s
	Output of modules: blocks.4.sa.heads.4
	Lookup keys: -168, 530, blocks.4.sa.heads.4, blocks.4.sa.heads.4:1, matmul_74, matmul_74:1, matmul_74_489, matmul_74_489:1
--------------------------------------------
Layer linear_124_490, operation 494/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.8829, -0.4810, -0.8042, -1.5614,  1.4414,  1.0997,  0.9483, -0.3857],
        [-0.9437, -1.2674,  1.3424, -0.9880,  1.8011,  1.9004,  1.3656, -1.7047],
        [-0.9464, -0.1459,  0.2865, -1.7754,  0.8599, -0.3073,  2.2364, -0.5262],
        [-1.0046,  0.0064,  0.2120, -1.3707,  0.8047, -0.3697,  2.0326, -0.7588],
        [-1.3471,  0.2159, -1.3466, -1.2485,  0.1182,  0.3150,  1.8739,  0.8736],
        [ 1.6021, -2.6504,  4.0931, -0.6913,  3.3750,  1.5816,  1.2710, -3.8097],
        [-0.2985, -1.9843,  3.4308, -1.1361,  2.7820,  0.9277,  1.1690, -2.7080],
        [-1.7575, -0.0316,  0.4554, -0.0255,  1.0075,  0.1079,  1.6667, -0.0324]])...
	Related Layers:
		- parent layers: layernorm_9_429
		- child layers: transpose_38_492
		- shares parents with layers: linear_109_430, linear_110_431, linear_111_440, linear_112_442, linear_113_443, linear_114_452, linear_115_454, linear_116_455, linear_117_464, linear_118_466, linear_119_467, linear_120_476, linear_121_478, linear_122_479, linear_123_488, linear_125_491, linear_126_500, linear_127_502, linear_128_503, linear_129_512, linear_130_514, linear_131_515, linear_132_524
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.5.key:1
	Time elapsed:  1.142E-04s
	Output of modules: blocks.4.sa.heads.5.key
	Output of bottom-level module: blocks.4.sa.heads.5.key:1
	Lookup keys: -167, 531, blocks.4.sa.heads.5.key, blocks.4.sa.heads.5.key:1, linear_124, linear_124:1, linear_124_490, linear_124_490:1
--------------------------------------------
Layer linear_125_491, operation 495/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-1.3069,  1.5159, -0.6955,  0.2844, -0.0679,  0.4061,  0.0051, -0.4277],
        [-1.8747,  0.6083,  0.9722, -0.4236,  0.4047,  1.4302,  1.5385, -1.2881],
        [-1.9254,  0.9559, -1.8148,  0.1561, -1.6928, -0.4856,  0.1610,  0.3550],
        [-1.6135,  0.7276, -2.1780,  0.7097, -1.4769, -0.2697, -0.2429,  0.0371],
        [-1.4202,  0.6256, -1.0967,  0.7400,  0.1645,  0.4064,  0.4550, -0.8983],
        [-0.6938,  2.4884, -3.2151, -1.9739, -2.4829, -1.1130, -2.4825,  2.7286],
        [-0.5594,  0.7928, -1.0275, -2.1557, -0.6496,  0.1708, -0.3411,  0.6461],
        [-1.4305,  0.5851,  0.3191, -0.4265, -0.2570,  1.3807,  0.8990, -1.0378]])...
	Related Layers:
		- parent layers: layernorm_9_429
		- child layers: matmul_75_493
		- shares parents with layers: linear_109_430, linear_110_431, linear_111_440, linear_112_442, linear_113_443, linear_114_452, linear_115_454, linear_116_455, linear_117_464, linear_118_466, linear_119_467, linear_120_476, linear_121_478, linear_122_479, linear_123_488, linear_124_490, linear_126_500, linear_127_502, linear_128_503, linear_129_512, linear_130_514, linear_131_515, linear_132_524
		- shares children with layers: transpose_38_492
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.5.query:1
	Time elapsed:  1.116E-04s
	Output of modules: blocks.4.sa.heads.5.query
	Output of bottom-level module: blocks.4.sa.heads.5.query:1
	Lookup keys: -166, 532, blocks.4.sa.heads.5.query, blocks.4.sa.heads.5.query:1, linear_125, linear_125:1, linear_125_491, linear_125_491:1
--------------------------------------------
Layer transpose_38_492, operation 496/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.8829, -0.9437, -0.9464, -1.0046, -1.3471,  1.6021, -0.2985, -1.7575],
        [-0.4810, -1.2674, -0.1459,  0.0064,  0.2159, -2.6504, -1.9843, -0.0316],
        [-0.8042,  1.3424,  0.2865,  0.2120, -1.3466,  4.0931,  3.4308,  0.4554],
        [-1.5614, -0.9880, -1.7754, -1.3707, -1.2485, -0.6913, -1.1361, -0.0255],
        [ 1.4414,  1.8011,  0.8599,  0.8047,  0.1182,  3.3750,  2.7820,  1.0075],
        [ 1.0997,  1.9004, -0.3073, -0.3697,  0.3150,  1.5816,  0.9277,  0.1079],
        [ 0.9483,  1.3656,  2.2364,  2.0326,  1.8739,  1.2710,  1.1690,  1.6667],
        [-0.3857, -1.7047, -0.5262, -0.7588,  0.8736, -3.8097, -2.7080, -0.0324]])...
	Related Layers:
		- parent layers: linear_124_490
		- child layers: matmul_75_493
		- shares parents with no other layers
		- shares children with layers: linear_125_491
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.4.sa.heads.5:1
	Time elapsed:  6.008E-05s
	Output of modules: none
	Lookup keys: -165, 533, transpose_38, transpose_38:1, transpose_38_492, transpose_38_492:1
--------------------------------------------
Layer matmul_75_493, operation 497/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -12.2191,   -2.1964,   -4.6710,   -4.1508,   -6.2621,   -7.8472,
           -0.9465,    5.2632],
        [  24.1884,   68.6677,   30.7488,   29.0723,   10.1119,   73.7286,
           78.7736,   26.0334],
        [ -33.8622,  -42.8314,  -19.6680,  -17.5408,   -5.7571,  -73.8354,
          -53.3177,   -2.0551],
        [ -31.6572,  -39.9570,  -22.5993,  -20.1615,   -7.8973,  -69.3628,
          -51.9341,   -6.3240],
        [  -3.8740,   15.7920,    1.0032,    1.7182,   -4.8928,   15.0444,
           20.8259,   11.3334],
        [ -34.3304,  -89.8419,  -26.9863,  -22.3111,   11.9172, -163.7294,
         -136.7660,  -27.4018],
        [   9.1958,    0.6397,    8.1178,    7.8088,   17.0818,  -21.6993,
          -18.6489,   -4.3661],
        [  21.9913,   59.1150,   27.7055,   25.7958,   10.2093,   66.1618,
           67.7464,   21.6580]])...
	Related Layers:
		- parent layers: linear_125_491, transpose_38_492
		- child layers: mul_38_494
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.5:1
	Time elapsed:  8.678E-05s
	Output of modules: none
	Lookup keys: -164, 534, matmul_75, matmul_75:1, matmul_75_493, matmul_75_493:1
--------------------------------------------
Layer mul_38_494, operation 498/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-0.6236, -0.1121, -0.2384, -0.2118, -0.3196, -0.4005, -0.0483,  0.2686],
        [ 1.2344,  3.5042,  1.5691,  1.4836,  0.5160,  3.7624,  4.0199,  1.3285],
        [-1.7280, -2.1857, -1.0037, -0.8951, -0.2938, -3.7679, -2.7209, -0.1049],
        [-1.6155, -2.0390, -1.1533, -1.0289, -0.4030, -3.5397, -2.6503, -0.3227],
        [-0.1977,  0.8059,  0.0512,  0.0877, -0.2497,  0.7677,  1.0628,  0.5784],
        [-1.7519, -4.5847, -1.3771, -1.1386,  0.6081, -8.3553, -6.9793, -1.3983],
        [ 0.4693,  0.0326,  0.4143,  0.3985,  0.8717, -1.1073, -0.9517, -0.2228],
        [ 1.1222,  3.0167,  1.4138,  1.3164,  0.5210,  3.3763,  3.4572,  1.1052]])...
	Related Layers:
		- parent layers: matmul_75_493
		- child layers: maskedfill_38_497
		- shares parents with no other layers
		- shares children with layers: eq_38_496
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.4.sa.heads.5:1
	Time elapsed:  6.723E-05s
	Output of modules: none
	Lookup keys: -163, 535, mul_38, mul_38:1, mul_38_494, mul_38_494:1
--------------------------------------------
Layer getitem_38_495, operation 499/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_38
		- child layers: eq_38_496
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.370E-05s
	Output of modules: none
	Lookup keys: -161, 537, getitem_38, getitem_38:1, getitem_38_495, getitem_38_495:1
--------------------------------------------
Layer eq_38_496, operation 500/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_38_495
		- child layers: maskedfill_38_497
		- shares parents with no other layers
		- shares children with layers: mul_38_494
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  7.796E-05s
	Output of modules: none
	Lookup keys: -160, 538, eq_38, eq_38:1, eq_38_496, eq_38_496:1
--------------------------------------------
Layer maskedfill_38_497, operation 501/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-0.6236,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [ 1.2344,  3.5042,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-1.7280, -2.1857, -1.0037,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-1.6155, -2.0390, -1.1533, -1.0289,    -inf,    -inf,    -inf,    -inf],
        [-0.1977,  0.8059,  0.0512,  0.0877, -0.2497,    -inf,    -inf,    -inf],
        [-1.7519, -4.5847, -1.3771, -1.1386,  0.6081, -8.3553,    -inf,    -inf],
        [ 0.4693,  0.0326,  0.4143,  0.3985,  0.8717, -1.1073, -0.9517,    -inf],
        [ 1.1222,  3.0167,  1.4138,  1.3164,  0.5210,  3.3763,  3.4572,  1.1052]])...
	Related Layers:
		- parent layers: mul_38_494, eq_38_496
		- child layers: softmax_38_498
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.4.sa.heads.5:1
	Time elapsed:  8.273E-05s
	Output of modules: none
	Lookup keys: -159, 539, maskedfill_38, maskedfill_38:1, maskedfill_38_497, maskedfill_38_497:1
--------------------------------------------
Layer softmax_38_498, operation 502/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [9.3653e-02, 9.0635e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.7055e-01, 1.7119e-01, 5.5826e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.9840e-01, 1.2990e-01, 3.1499e-01, 3.5671e-01, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.3717e-01, 3.7420e-01, 1.7593e-01, 1.8247e-01, 1.3022e-01, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [6.6876e-02, 3.9355e-03, 9.7282e-02, 1.2349e-01, 7.0832e-01, 9.0670e-05,
         0.0000e+00, 0.0000e+00],
        [1.8288e-01, 1.1818e-01, 1.7309e-01, 1.7039e-01, 2.7349e-01, 3.7797e-02,
         4.4164e-02, 0.0000e+00],
        [3.1657e-02, 2.1049e-01, 4.2376e-02, 3.8441e-02, 1.7352e-02, 3.0158e-01,
         3.2698e-01, 3.1123e-02]])...
	Related Layers:
		- parent layers: maskedfill_38_497
		- child layers: dropout_46_499
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.4.sa.heads.5:1
	Time elapsed:  6.866E-05s
	Output of modules: none
	Lookup keys: -158, 540, softmax_38, softmax_38:1, softmax_38_498, softmax_38_498:1
--------------------------------------------
Layer dropout_46_499, operation 503/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [9.3653e-02, 9.0635e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.7055e-01, 1.7119e-01, 5.5826e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.9840e-01, 1.2990e-01, 3.1499e-01, 3.5671e-01, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.3717e-01, 3.7420e-01, 1.7593e-01, 1.8247e-01, 1.3022e-01, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [6.6876e-02, 3.9355e-03, 9.7282e-02, 1.2349e-01, 7.0832e-01, 9.0670e-05,
         0.0000e+00, 0.0000e+00],
        [1.8288e-01, 1.1818e-01, 1.7309e-01, 1.7039e-01, 2.7349e-01, 3.7797e-02,
         4.4164e-02, 0.0000e+00],
        [3.1657e-02, 2.1049e-01, 4.2376e-02, 3.8441e-02, 1.7352e-02, 3.0158e-01,
         3.2698e-01, 3.1123e-02]])...
	Related Layers:
		- parent layers: softmax_38_498
		- child layers: matmul_76_501
		- shares parents with no other layers
		- shares children with layers: linear_126_500
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.4.sa.heads.5.dropout:1
	Time elapsed:  5.412E-05s
	Output of modules: blocks.4.sa.heads.5.dropout
	Output of bottom-level module: blocks.4.sa.heads.5.dropout:1
	Lookup keys: -157, 541, blocks.4.sa.heads.5.dropout, blocks.4.sa.heads.5.dropout:1, dropout_46, dropout_46:1, dropout_46_499, dropout_46_499:1
--------------------------------------------
Layer linear_126_500, operation 504/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-1.0556,  0.2229, -0.3849,  0.7367,  0.6347,  0.2608, -0.4849, -1.1135],
        [-0.9254,  0.2211,  0.4191,  0.0330,  0.3791,  0.1457, -0.3822, -0.1663],
        [-0.6268, -0.4090,  0.3159,  0.5601,  0.6246,  0.7686, -0.1271, -1.8914],
        [-0.4204, -0.1965,  0.0189,  0.8771,  0.7196,  0.9790, -0.4029, -2.0052],
        [-0.9003,  0.3348, -0.1482,  1.1763,  1.7728,  0.6512, -0.3870, -1.6413],
        [ 0.1987, -0.6081, -0.0678, -0.6875, -1.1279,  0.1591, -0.9095,  0.5658],
        [-0.6454, -0.1887, -0.1906, -1.2269, -1.3147, -0.5168, -1.5066,  0.3437],
        [-1.5872,  0.4970, -0.6628, -0.5430,  0.3669,  0.0489, -0.4478, -0.9634]])...
	Related Layers:
		- parent layers: layernorm_9_429
		- child layers: matmul_76_501
		- shares parents with layers: linear_109_430, linear_110_431, linear_111_440, linear_112_442, linear_113_443, linear_114_452, linear_115_454, linear_116_455, linear_117_464, linear_118_466, linear_119_467, linear_120_476, linear_121_478, linear_122_479, linear_123_488, linear_124_490, linear_125_491, linear_127_502, linear_128_503, linear_129_512, linear_130_514, linear_131_515, linear_132_524
		- shares children with layers: dropout_46_499
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.5.value:1
	Time elapsed:  1.242E-04s
	Output of modules: blocks.4.sa.heads.5.value
	Output of bottom-level module: blocks.4.sa.heads.5.value:1
	Lookup keys: -156, 542, blocks.4.sa.heads.5.value, blocks.4.sa.heads.5.value:1, linear_126, linear_126:1, linear_126_500, linear_126_500:1
--------------------------------------------
Layer matmul_76_501, operation 505/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-1.0556e+00,  2.2287e-01, -3.8492e-01,  7.3667e-01,  6.3473e-01,
          2.6082e-01, -4.8487e-01, -1.1135e+00],
        [-9.3756e-01,  2.2127e-01,  3.4381e-01,  9.8929e-02,  4.0303e-01,
          1.5646e-01, -3.9181e-01, -2.5504e-01],
        [-7.9394e-01, -1.3017e-01,  1.4396e-01,  5.1763e-01,  5.8534e-01,
          5.2457e-01, -2.6758e-01, -1.3856e+00],
        [-6.7703e-01, -1.2597e-01,  8.4325e-02,  6.3973e-01,  6.2861e-01,
          6.6198e-01, -3.2959e-01, -1.5536e+00],
        [-7.9529e-01,  4.9106e-02,  1.4377e-01,  5.2516e-01,  7.0098e-01,
          4.8895e-01, -3.5580e-01, -1.1274e+00],
        [-8.2478e-01,  1.8885e-01, -9.6006e-02,  1.0453e+00,  1.4492e+00,
          6.7497e-01, -3.7024e-01, -1.6693e+00],
        [-7.4974e-01,  2.2878e-02, -1.4476e-02,  6.2656e-01,  7.7577e-01,
          5.2605e-01, -4.3124e-01, -1.3047e+00],
        [-4.8704e-01, -1.9511e-01, -1.5845e-02, -5.1726e-01, -5.7383e-01,
          9.4125e-04, -9.0424e-01, -2.9308e-03]])...
	Related Layers:
		- parent layers: dropout_46_499, linear_126_500
		- child layers: cat_5_526
		- shares parents with no other layers
		- shares children with layers: matmul_66_441, matmul_68_453, matmul_70_465, matmul_72_477, matmul_74_489, matmul_78_513, matmul_80_525
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.5:1
	Time elapsed:  8.607E-05s
	Output of modules: blocks.4.sa.heads.5
	Lookup keys: -155, 543, blocks.4.sa.heads.5, blocks.4.sa.heads.5:1, matmul_76, matmul_76:1, matmul_76_501, matmul_76_501:1
--------------------------------------------
Layer linear_127_502, operation 506/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 5.0860,  3.5953, -4.4692, -4.0914, -4.1158,  0.4522, -4.2888,  4.7328],
        [ 3.4547, -0.2283, -1.2783, -0.5929, -1.8682, -2.5560, -0.4586,  1.5265],
        [ 4.2824,  2.9322, -4.1067, -3.0130, -4.5913,  0.5563, -3.2557,  3.8106],
        [ 4.6653,  3.2738, -3.7887, -2.8864, -4.0816,  0.4107, -3.3567,  3.9843],
        [ 5.0090,  3.8743, -4.2733, -4.4980, -4.3369, -0.1418, -3.5261,  4.7164],
        [-0.7143, -2.3591,  1.2318,  2.4151,  0.8207,  0.0592,  1.9092, -1.2219],
        [ 0.6580, -2.5793,  0.0142,  1.7517, -0.1235, -1.4131,  1.4839, -1.5901],
        [ 4.0874,  2.2505, -2.1220, -2.3126, -2.3786, -1.3097, -2.2014,  2.3427]])...
	Related Layers:
		- parent layers: layernorm_9_429
		- child layers: transpose_39_504
		- shares parents with layers: linear_109_430, linear_110_431, linear_111_440, linear_112_442, linear_113_443, linear_114_452, linear_115_454, linear_116_455, linear_117_464, linear_118_466, linear_119_467, linear_120_476, linear_121_478, linear_122_479, linear_123_488, linear_124_490, linear_125_491, linear_126_500, linear_128_503, linear_129_512, linear_130_514, linear_131_515, linear_132_524
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.6.key:1
	Time elapsed:  1.116E-04s
	Output of modules: blocks.4.sa.heads.6.key
	Output of bottom-level module: blocks.4.sa.heads.6.key:1
	Lookup keys: -154, 544, blocks.4.sa.heads.6.key, blocks.4.sa.heads.6.key:1, linear_127, linear_127:1, linear_127_502, linear_127_502:1
--------------------------------------------
Layer linear_128_503, operation 507/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.5460, -1.8171,  0.9948,  1.5188,  2.2375, -2.0899,  0.8823, -1.1244],
        [-1.1218, -2.3489,  0.0603,  2.2217,  1.3658, -1.1016,  2.4212, -1.3626],
        [ 0.1381, -2.5642,  1.1547,  2.2182,  2.3748, -1.4939,  1.8504, -1.2610],
        [ 0.8015, -2.3610,  1.0422,  2.4878,  2.1353, -1.9244,  1.6824, -0.9753],
        [-0.4644, -2.3046,  2.4627,  2.2830,  3.5922, -1.8853,  1.7975, -1.1170],
        [ 0.0717, -0.0348, -2.1706, -0.0999, -0.7076,  0.8653,  0.1317,  1.0421],
        [-1.6632,  0.0214, -0.5950,  0.1571,  0.8662,  0.8503,  1.5102,  0.0447],
        [-2.3906, -3.1026,  2.3791,  2.9873,  3.7571, -0.3832,  2.7644, -2.4720]])...
	Related Layers:
		- parent layers: layernorm_9_429
		- child layers: matmul_77_505
		- shares parents with layers: linear_109_430, linear_110_431, linear_111_440, linear_112_442, linear_113_443, linear_114_452, linear_115_454, linear_116_455, linear_117_464, linear_118_466, linear_119_467, linear_120_476, linear_121_478, linear_122_479, linear_123_488, linear_124_490, linear_125_491, linear_126_500, linear_127_502, linear_129_512, linear_130_514, linear_131_515, linear_132_524
		- shares children with layers: transpose_39_504
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.6.query:1
	Time elapsed:  1.132E-04s
	Output of modules: blocks.4.sa.heads.6.query
	Output of bottom-level module: blocks.4.sa.heads.6.query:1
	Lookup keys: -153, 545, blocks.4.sa.heads.6.query, blocks.4.sa.heads.6.query:1, linear_128, linear_128:1, linear_128_503, linear_128_503:1
--------------------------------------------
Layer transpose_39_504, operation 508/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 5.0860,  3.4547,  4.2824,  4.6653,  5.0090, -0.7143,  0.6580,  4.0874],
        [ 3.5953, -0.2283,  2.9322,  3.2738,  3.8743, -2.3591, -2.5793,  2.2505],
        [-4.4692, -1.2783, -4.1067, -3.7887, -4.2733,  1.2318,  0.0142, -2.1220],
        [-4.0914, -0.5929, -3.0130, -2.8864, -4.4980,  2.4151,  1.7517, -2.3126],
        [-4.1158, -1.8682, -4.5913, -4.0816, -4.3369,  0.8207, -0.1235, -2.3786],
        [ 0.4522, -2.5560,  0.5563,  0.4107, -0.1418,  0.0592, -1.4131, -1.3097],
        [-4.2888, -0.4586, -3.2557, -3.3567, -3.5261,  1.9092,  1.4839, -2.2014],
        [ 4.7328,  1.5265,  3.8106,  3.9843,  4.7164, -1.2219, -1.5901,  2.3427]])...
	Related Layers:
		- parent layers: linear_127_502
		- child layers: matmul_77_505
		- shares parents with no other layers
		- shares children with layers: linear_128_503
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.4.sa.heads.6:1
	Time elapsed:  5.937E-05s
	Output of modules: none
	Lookup keys: -152, 546, transpose_39, transpose_39:1, transpose_39_504, transpose_39_504:1
--------------------------------------------
Layer matmul_77_505, operation 509/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-193.3911,  -27.1557, -172.7857, -167.8725, -201.5490,   71.4198,
           77.6602, -110.0413],
        [-198.4319,  -26.5098, -172.9456, -171.9940, -202.8882,   80.6072,
           85.0622, -113.3363],
        [-242.7438,  -24.4390, -218.1223, -213.0520, -253.3071,  103.5088,
          111.9699, -135.5033],
        [-223.9512,  -12.9683, -201.0021, -195.9245, -234.6101,  100.8952,
          114.0162, -120.7416],
        [-281.5299,  -59.0727, -256.2488, -247.4230, -290.7591,   91.0859,
           84.5281, -171.6529],
        [  80.9354,   -2.1983,   79.4038,   72.4345,   92.1315,  -35.8762,
          -47.0355,   38.5902],
        [ -73.0860,  -64.1917,  -61.4393,  -64.4997,  -65.3909,   -3.5174,
          -27.2059,  -64.1036],
        [-347.2346,  -92.1166, -306.9420, -301.0394, -351.6775,  107.8795,
           91.1418, -217.6160]])...
	Related Layers:
		- parent layers: linear_128_503, transpose_39_504
		- child layers: mul_39_506
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.6:1
	Time elapsed:  8.559E-05s
	Output of modules: none
	Lookup keys: -151, 547, matmul_77, matmul_77:1, matmul_77_505, matmul_77_505:1
--------------------------------------------
Layer mul_39_506, operation 510/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -9.8689,  -1.3858,  -8.8174,  -8.5667, -10.2853,   3.6446,   3.9631,
          -5.6155],
        [-10.1262,  -1.3528,  -8.8256,  -8.7770, -10.3536,   4.1135,   4.3408,
          -5.7837],
        [-12.3875,  -1.2471, -11.1310, -10.8723, -12.9265,   5.2822,   5.7139,
          -6.9149],
        [-11.4285,  -0.6618, -10.2573,  -9.9982, -11.9724,   5.1488,   5.8184,
          -6.1616],
        [-14.3668,  -3.0145, -13.0766, -12.6263, -14.8377,   4.6482,   4.3136,
          -8.7596],
        [  4.1302,  -0.1122,   4.0521,   3.6964,   4.7016,  -1.8308,  -2.4003,
           1.9693],
        [ -3.7297,  -3.2758,  -3.1353,  -3.2915,  -3.3370,  -0.1795,  -1.3883,
          -3.2713],
        [-17.7197,  -4.7008, -15.6636, -15.3624, -17.9465,   5.5052,   4.6511,
         -11.1052]])...
	Related Layers:
		- parent layers: matmul_77_505
		- child layers: maskedfill_39_509
		- shares parents with no other layers
		- shares children with layers: eq_39_508
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.4.sa.heads.6:1
	Time elapsed:  6.866E-05s
	Output of modules: none
	Lookup keys: -150, 548, mul_39, mul_39:1, mul_39_506, mul_39_506:1
--------------------------------------------
Layer getitem_39_507, operation 511/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_39
		- child layers: eq_39_508
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.704E-05s
	Output of modules: none
	Lookup keys: -148, 550, getitem_39, getitem_39:1, getitem_39_507, getitem_39_507:1
--------------------------------------------
Layer eq_39_508, operation 512/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_39_507
		- child layers: maskedfill_39_509
		- shares parents with no other layers
		- shares children with layers: mul_39_506
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.035E-05s
	Output of modules: none
	Lookup keys: -147, 551, eq_39, eq_39:1, eq_39_508, eq_39_508:1
--------------------------------------------
Layer maskedfill_39_509, operation 513/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -9.8689,     -inf,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [-10.1262,  -1.3528,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [-12.3875,  -1.2471, -11.1310,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [-11.4285,  -0.6618, -10.2573,  -9.9982,     -inf,     -inf,     -inf,
             -inf],
        [-14.3668,  -3.0145, -13.0766, -12.6263, -14.8377,     -inf,     -inf,
             -inf],
        [  4.1302,  -0.1122,   4.0521,   3.6964,   4.7016,  -1.8308,     -inf,
             -inf],
        [ -3.7297,  -3.2758,  -3.1353,  -3.2915,  -3.3370,  -0.1795,  -1.3883,
             -inf],
        [-17.7197,  -4.7008, -15.6636, -15.3624, -17.9465,   5.5052,   4.6511,
         -11.1052]])...
	Related Layers:
		- parent layers: mul_39_506, eq_39_508
		- child layers: softmax_39_510
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.4.sa.heads.6:1
	Time elapsed:  8.249E-05s
	Output of modules: none
	Lookup keys: -146, 552, maskedfill_39, maskedfill_39:1, maskedfill_39_509, maskedfill_39_509:1
--------------------------------------------
Layer softmax_39_510, operation 514/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.5478e-04, 9.9985e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.4514e-05, 9.9993e-01, 5.0988e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.1087e-05, 9.9982e-01, 6.8018e-05, 8.8136e-05, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.1742e-05, 9.9987e-01, 4.2661e-05, 6.6931e-05, 7.3315e-06, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.2933e-01, 3.2962e-03, 2.1209e-01, 1.4862e-01, 4.0607e-01, 5.9106e-04,
         0.0000e+00, 0.0000e+00],
        [1.9000e-02, 2.9914e-02, 3.4426e-02, 2.9448e-02, 2.8139e-02, 6.6157e-01,
         1.9750e-01, 0.0000e+00],
        [5.7479e-11, 2.5916e-05, 4.4926e-10, 6.0717e-10, 4.5819e-11, 7.0142e-01,
         2.9856e-01, 4.2873e-08]])...
	Related Layers:
		- parent layers: maskedfill_39_509
		- child layers: dropout_47_511
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.4.sa.heads.6:1
	Time elapsed:  7.606E-05s
	Output of modules: none
	Lookup keys: -145, 553, softmax_39, softmax_39:1, softmax_39_510, softmax_39_510:1
--------------------------------------------
Layer dropout_47_511, operation 515/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.5478e-04, 9.9985e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.4514e-05, 9.9993e-01, 5.0988e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.1087e-05, 9.9982e-01, 6.8018e-05, 8.8136e-05, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.1742e-05, 9.9987e-01, 4.2661e-05, 6.6931e-05, 7.3315e-06, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.2933e-01, 3.2962e-03, 2.1209e-01, 1.4862e-01, 4.0607e-01, 5.9106e-04,
         0.0000e+00, 0.0000e+00],
        [1.9000e-02, 2.9914e-02, 3.4426e-02, 2.9448e-02, 2.8139e-02, 6.6157e-01,
         1.9750e-01, 0.0000e+00],
        [5.7479e-11, 2.5916e-05, 4.4926e-10, 6.0717e-10, 4.5819e-11, 7.0142e-01,
         2.9856e-01, 4.2873e-08]])...
	Related Layers:
		- parent layers: softmax_39_510
		- child layers: matmul_78_513
		- shares parents with no other layers
		- shares children with layers: linear_129_512
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.4.sa.heads.6.dropout:1
	Time elapsed:  5.364E-05s
	Output of modules: blocks.4.sa.heads.6.dropout
	Output of bottom-level module: blocks.4.sa.heads.6.dropout:1
	Lookup keys: -144, 554, blocks.4.sa.heads.6.dropout, blocks.4.sa.heads.6.dropout:1, dropout_47, dropout_47:1, dropout_47_511, dropout_47_511:1
--------------------------------------------
Layer linear_129_512, operation 516/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.0605,  0.4022,  0.8243, -0.9142, -0.5400, -0.4372,  0.7662,  0.8354],
        [ 0.4833,  0.0231,  0.3821, -0.2337, -0.3213, -0.5406,  0.3107,  0.6858],
        [ 0.1633,  0.0500,  0.1271, -0.7989, -0.5729, -1.0762,  0.4870,  0.8227],
        [ 0.3518,  0.1518,  0.2416, -1.0190, -0.1399, -0.9476,  0.6079,  0.8048],
        [ 0.0854,  0.0942,  0.3893, -1.3674, -0.7158, -0.9792,  0.6994,  0.7895],
        [ 1.3242,  0.1292,  0.1362,  1.7880,  0.7541, -0.0715, -0.6836,  0.0550],
        [ 0.0578, -0.7606,  0.6738,  1.6421,  0.0508, -0.5047, -1.1751,  0.7782],
        [ 0.0767,  0.6261,  0.5638, -0.5668, -0.1317, -0.7803, -0.0959, -0.0218]])...
	Related Layers:
		- parent layers: layernorm_9_429
		- child layers: matmul_78_513
		- shares parents with layers: linear_109_430, linear_110_431, linear_111_440, linear_112_442, linear_113_443, linear_114_452, linear_115_454, linear_116_455, linear_117_464, linear_118_466, linear_119_467, linear_120_476, linear_121_478, linear_122_479, linear_123_488, linear_124_490, linear_125_491, linear_126_500, linear_127_502, linear_128_503, linear_130_514, linear_131_515, linear_132_524
		- shares children with layers: dropout_47_511
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.6.value:1
	Time elapsed:  1.202E-04s
	Output of modules: blocks.4.sa.heads.6.value
	Output of bottom-level module: blocks.4.sa.heads.6.value:1
	Lookup keys: -143, 555, blocks.4.sa.heads.6.value, blocks.4.sa.heads.6.value:1, linear_129, linear_129:1, linear_129_512, linear_129_512:1
--------------------------------------------
Layer matmul_78_513, operation 517/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.0605,  0.4022,  0.8243, -0.9142, -0.5400, -0.4372,  0.7662,  0.8354],
        [ 0.4832,  0.0232,  0.3822, -0.2338, -0.3214, -0.5405,  0.3108,  0.6858],
        [ 0.4832,  0.0231,  0.3821, -0.2337, -0.3213, -0.5406,  0.3107,  0.6858],
        [ 0.4832,  0.0231,  0.3821, -0.2338, -0.3213, -0.5406,  0.3107,  0.6858],
        [ 0.4832,  0.0231,  0.3821, -0.2338, -0.3213, -0.5406,  0.3107,  0.6858],
        [ 0.1101,  0.1638,  0.4113, -1.0855, -0.5574, -0.8688,  0.6540,  0.8086],
        [ 0.9191, -0.0476,  0.2727,  1.3869,  0.4451, -0.2640, -0.6061,  0.3007],
        [ 0.9461, -0.1364,  0.2967,  1.7444,  0.5441, -0.2008, -0.8303,  0.2709]])...
	Related Layers:
		- parent layers: dropout_47_511, linear_129_512
		- child layers: cat_5_526
		- shares parents with no other layers
		- shares children with layers: matmul_66_441, matmul_68_453, matmul_70_465, matmul_72_477, matmul_74_489, matmul_76_501, matmul_80_525
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.6:1
	Time elapsed:  8.416E-05s
	Output of modules: blocks.4.sa.heads.6
	Lookup keys: -142, 556, blocks.4.sa.heads.6, blocks.4.sa.heads.6:1, matmul_78, matmul_78:1, matmul_78_513, matmul_78_513:1
--------------------------------------------
Layer linear_130_514, operation 518/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.4749e+00, -1.4959e+00, -1.6168e+00,  1.1233e+00, -1.1729e+00,
         -1.2210e+00, -2.0169e+00,  1.4042e+00],
        [ 3.9878e+00,  6.0566e-01, -3.5793e+00, -9.2520e-01, -3.4356e+00,
         -1.7009e+00, -3.3029e+00,  4.8879e+00],
        [-1.0140e-01, -1.7361e+00, -1.0293e+00,  1.1874e+00, -6.6025e-01,
          4.0445e-01, -8.4245e-01,  8.3516e-01],
        [-5.4204e-01, -1.8592e+00, -1.0813e+00,  1.2353e+00, -6.1741e-01,
         -1.3936e-01, -9.9661e-01,  5.6825e-01],
        [ 6.0409e-01, -6.2308e-01, -4.8976e-01,  1.5496e+00, -9.6981e-01,
          7.3097e-01, -1.1597e+00,  8.3662e-01],
        [ 1.2283e+00, -3.1975e+00, -2.6694e+00, -8.3341e-01, -1.2705e+00,
         -3.3186e+00, -1.9822e+00,  2.0023e+00],
        [ 2.4112e+00, -2.3485e-01, -2.3901e+00, -7.4911e-01, -1.8266e+00,
         -2.7347e+00, -2.3781e+00,  3.9951e+00],
        [ 1.5546e+00,  4.1844e-01, -2.1086e+00,  2.2085e-03, -1.1936e+00,
         -4.6936e-01, -1.6718e+00,  2.1798e+00]])...
	Related Layers:
		- parent layers: layernorm_9_429
		- child layers: transpose_40_516
		- shares parents with layers: linear_109_430, linear_110_431, linear_111_440, linear_112_442, linear_113_443, linear_114_452, linear_115_454, linear_116_455, linear_117_464, linear_118_466, linear_119_467, linear_120_476, linear_121_478, linear_122_479, linear_123_488, linear_124_490, linear_125_491, linear_126_500, linear_127_502, linear_128_503, linear_129_512, linear_131_515, linear_132_524
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.7.key:1
	Time elapsed:  1.132E-04s
	Output of modules: blocks.4.sa.heads.7.key
	Output of bottom-level module: blocks.4.sa.heads.7.key:1
	Lookup keys: -141, 557, blocks.4.sa.heads.7.key, blocks.4.sa.heads.7.key:1, linear_130, linear_130:1, linear_130_514, linear_130_514:1
--------------------------------------------
Layer linear_131_515, operation 519/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.2222,  1.7691,  0.1610,  0.1210, -0.1476,  0.7979,  0.4468, -0.1918],
        [ 2.5093, -0.3193, -0.9239, -1.8562, -1.5978, -0.8115,  0.2819,  1.3228],
        [ 1.1750,  3.0388, -0.7180, -1.3711, -1.2638, -1.0683, -0.5066,  0.9548],
        [ 1.3247,  3.1471, -1.1910, -1.4510, -1.9222, -1.1477, -0.7118,  1.5539],
        [ 1.2494,  3.2125, -1.1314, -0.4985, -2.0547, -0.3101, -0.5264,  1.5548],
        [-0.3735, -2.1598,  2.1428, -0.3108,  1.3474, -0.9530,  1.9066, -1.8702],
        [ 0.6310, -2.3121,  0.3342, -0.4530, -0.0574, -0.6539,  0.9934, -0.9409],
        [ 1.7859,  2.6750, -0.4554, -1.3362, -1.3502, -0.4054, -0.2253,  1.6758]])...
	Related Layers:
		- parent layers: layernorm_9_429
		- child layers: matmul_79_517
		- shares parents with layers: linear_109_430, linear_110_431, linear_111_440, linear_112_442, linear_113_443, linear_114_452, linear_115_454, linear_116_455, linear_117_464, linear_118_466, linear_119_467, linear_120_476, linear_121_478, linear_122_479, linear_123_488, linear_124_490, linear_125_491, linear_126_500, linear_127_502, linear_128_503, linear_129_512, linear_130_514, linear_132_524
		- shares children with layers: transpose_40_516
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.7.query:1
	Time elapsed:  1.109E-04s
	Output of modules: blocks.4.sa.heads.7.query
	Output of bottom-level module: blocks.4.sa.heads.7.query:1
	Lookup keys: -140, 558, blocks.4.sa.heads.7.query, blocks.4.sa.heads.7.query:1, linear_131, linear_131:1, linear_131_515, linear_131_515:1
--------------------------------------------
Layer transpose_40_516, operation 520/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.4749e+00,  3.9878e+00, -1.0140e-01, -5.4204e-01,  6.0409e-01,
          1.2283e+00,  2.4112e+00,  1.5546e+00],
        [-1.4959e+00,  6.0566e-01, -1.7361e+00, -1.8592e+00, -6.2308e-01,
         -3.1975e+00, -2.3485e-01,  4.1844e-01],
        [-1.6168e+00, -3.5793e+00, -1.0293e+00, -1.0813e+00, -4.8976e-01,
         -2.6694e+00, -2.3901e+00, -2.1086e+00],
        [ 1.1233e+00, -9.2520e-01,  1.1874e+00,  1.2353e+00,  1.5496e+00,
         -8.3341e-01, -7.4911e-01,  2.2085e-03],
        [-1.1729e+00, -3.4356e+00, -6.6025e-01, -6.1741e-01, -9.6981e-01,
         -1.2705e+00, -1.8266e+00, -1.1936e+00],
        [-1.2210e+00, -1.7009e+00,  4.0445e-01, -1.3936e-01,  7.3097e-01,
         -3.3186e+00, -2.7347e+00, -4.6936e-01],
        [-2.0169e+00, -3.3029e+00, -8.4245e-01, -9.9661e-01, -1.1597e+00,
         -1.9822e+00, -2.3781e+00, -1.6718e+00],
        [ 1.4042e+00,  4.8879e+00,  8.3516e-01,  5.6825e-01,  8.3662e-01,
          2.0023e+00,  3.9951e+00,  2.1798e+00]])...
	Related Layers:
		- parent layers: linear_130_514
		- child layers: matmul_79_517
		- shares parents with no other layers
		- shares children with layers: linear_131_515
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.4.sa.heads.7:1
	Time elapsed:  5.865E-05s
	Output of modules: none
	Lookup keys: -139, 559, transpose_40, transpose_40:1, transpose_40_516, transpose_40_516:1
--------------------------------------------
Layer matmul_79_517, operation 521/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -40.5454,  -17.0272,  -18.6044,  -17.6767,  -18.1154,  -55.2560,
          -22.6825,  -16.8718],
        [  -9.3015,   56.8518,  -12.5495,  -14.2515,  -12.5284,    7.0809,
           31.6990,   16.8188],
        [ -50.7221,   56.4714,  -31.1503,  -35.2613,  -29.1615,  -27.7763,
           40.3450,   17.3004],
        [ -36.3596,   86.2651,  -22.9900,  -28.2489,  -18.9881,  -19.8808,
           56.9107,   32.4552],
        [  -8.5998,  103.2678,   -2.2651,   -7.0448,    7.2909,  -29.1451,
           52.6462,   45.3922],
        [-115.5591, -198.6601,  -69.7278,  -60.2405,  -98.1268,  -31.8265,
         -105.4696, -128.9194],
        [ -37.2974,  -92.7439,  -28.8705,  -22.1958,  -43.3369,   -7.9885,
          -55.4050,  -63.5530],
        [ -40.7462,   54.4550,  -30.2121,  -33.6997,  -22.9413,  -34.0141,
           32.4050,   14.6390]])...
	Related Layers:
		- parent layers: linear_131_515, transpose_40_516
		- child layers: mul_40_518
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.7:1
	Time elapsed:  8.774E-05s
	Output of modules: none
	Lookup keys: -138, 560, matmul_79, matmul_79:1, matmul_79_517, matmul_79_517:1
--------------------------------------------
Layer mul_40_518, operation 522/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -2.0691,  -0.8689,  -0.9494,  -0.9021,  -0.9244,  -2.8198,  -1.1575,
          -0.8610],
        [ -0.4747,   2.9012,  -0.6404,  -0.7273,  -0.6393,   0.3613,   1.6176,
           0.8583],
        [ -2.5884,   2.8818,  -1.5896,  -1.7994,  -1.4881,  -1.4175,   2.0588,
           0.8829],
        [ -1.8555,   4.4022,  -1.1732,  -1.4416,  -0.9690,  -1.0145,   2.9042,
           1.6562],
        [ -0.4389,   5.2699,  -0.1156,  -0.3595,   0.3721,  -1.4873,   2.6866,
           2.3164],
        [ -5.8971, -10.1378,  -3.5583,  -3.0741,  -5.0075,  -1.6241,  -5.3822,
          -6.5789],
        [ -1.9033,  -4.7328,  -1.4733,  -1.1327,  -2.2115,  -0.4077,  -2.8274,
          -3.2432],
        [ -2.0793,   2.7789,  -1.5418,  -1.7197,  -1.1707,  -1.7358,   1.6537,
           0.7470]])...
	Related Layers:
		- parent layers: matmul_79_517
		- child layers: maskedfill_40_521
		- shares parents with no other layers
		- shares children with layers: eq_40_520
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.4.sa.heads.7:1
	Time elapsed:  6.843E-05s
	Output of modules: none
	Lookup keys: -137, 561, mul_40, mul_40:1, mul_40_518, mul_40_518:1
--------------------------------------------
Layer getitem_40_519, operation 523/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_40
		- child layers: eq_40_520
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.155E-05s
	Output of modules: none
	Lookup keys: -135, 563, getitem_40, getitem_40:1, getitem_40_519, getitem_40_519:1
--------------------------------------------
Layer eq_40_520, operation 524/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_40_519
		- child layers: maskedfill_40_521
		- shares parents with no other layers
		- shares children with layers: mul_40_518
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.345E-05s
	Output of modules: none
	Lookup keys: -134, 564, eq_40, eq_40:1, eq_40_520, eq_40_520:1
--------------------------------------------
Layer maskedfill_40_521, operation 525/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -2.0691,     -inf,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -0.4747,   2.9012,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -2.5884,   2.8818,  -1.5896,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -1.8555,   4.4022,  -1.1732,  -1.4416,     -inf,     -inf,     -inf,
             -inf],
        [ -0.4389,   5.2699,  -0.1156,  -0.3595,   0.3721,     -inf,     -inf,
             -inf],
        [ -5.8971, -10.1378,  -3.5583,  -3.0741,  -5.0075,  -1.6241,     -inf,
             -inf],
        [ -1.9033,  -4.7328,  -1.4733,  -1.1327,  -2.2115,  -0.4077,  -2.8274,
             -inf],
        [ -2.0793,   2.7789,  -1.5418,  -1.7197,  -1.1707,  -1.7358,   1.6537,
           0.7470]])...
	Related Layers:
		- parent layers: mul_40_518, eq_40_520
		- child layers: softmax_40_522
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.4.sa.heads.7:1
	Time elapsed:  8.345E-05s
	Output of modules: none
	Lookup keys: -133, 565, maskedfill_40, maskedfill_40:1, maskedfill_40_521, maskedfill_40_521:1
--------------------------------------------
Layer softmax_40_522, operation 526/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [3.3058e-02, 9.6694e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [4.1456e-03, 9.8460e-01, 1.1255e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.8994e-03, 9.9147e-01, 3.7576e-03, 2.8732e-03, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [3.2552e-03, 9.8140e-01, 4.4975e-03, 3.5241e-03, 7.3242e-03, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [9.7677e-03, 1.4063e-04, 1.0128e-01, 1.6436e-01, 2.3776e-02, 7.0068e-01,
         0.0000e+00, 0.0000e+00],
        [9.6604e-02, 5.7038e-03, 1.4851e-01, 2.0878e-01, 7.0982e-02, 4.3108e-01,
         3.8343e-02, 0.0000e+00],
        [5.1147e-03, 6.5874e-01, 8.7554e-03, 7.3280e-03, 1.2689e-02, 7.2113e-03,
         2.1381e-01, 8.6356e-02]])...
	Related Layers:
		- parent layers: maskedfill_40_521
		- child layers: dropout_48_523
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.4.sa.heads.7:1
	Time elapsed:  6.819E-05s
	Output of modules: none
	Lookup keys: -132, 566, softmax_40, softmax_40:1, softmax_40_522, softmax_40_522:1
--------------------------------------------
Layer dropout_48_523, operation 527/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [3.3058e-02, 9.6694e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [4.1456e-03, 9.8460e-01, 1.1255e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.8994e-03, 9.9147e-01, 3.7576e-03, 2.8732e-03, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [3.2552e-03, 9.8140e-01, 4.4975e-03, 3.5241e-03, 7.3242e-03, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [9.7677e-03, 1.4063e-04, 1.0128e-01, 1.6436e-01, 2.3776e-02, 7.0068e-01,
         0.0000e+00, 0.0000e+00],
        [9.6604e-02, 5.7038e-03, 1.4851e-01, 2.0878e-01, 7.0982e-02, 4.3108e-01,
         3.8343e-02, 0.0000e+00],
        [5.1147e-03, 6.5874e-01, 8.7554e-03, 7.3280e-03, 1.2689e-02, 7.2113e-03,
         2.1381e-01, 8.6356e-02]])...
	Related Layers:
		- parent layers: softmax_40_522
		- child layers: matmul_80_525
		- shares parents with no other layers
		- shares children with layers: linear_132_524
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.4.sa.heads.7.dropout:1
	Time elapsed:  5.555E-05s
	Output of modules: blocks.4.sa.heads.7.dropout
	Output of bottom-level module: blocks.4.sa.heads.7.dropout:1
	Lookup keys: -131, 567, blocks.4.sa.heads.7.dropout, blocks.4.sa.heads.7.dropout:1, dropout_48, dropout_48:1, dropout_48_523, dropout_48_523:1
--------------------------------------------
Layer linear_132_524, operation 528/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.5348,  0.8528, -0.8642, -0.3890,  0.9864, -1.1946, -0.3930, -1.4585],
        [ 0.1854,  0.5395, -0.0875,  0.0284,  0.3367,  0.0543, -0.5677, -0.3244],
        [ 0.4550,  1.0311,  0.2780, -0.3722,  0.9075, -0.9239, -0.4903, -1.7371],
        [ 0.7620,  0.4155,  0.5130, -0.4626,  1.2660, -0.8839, -0.4391, -1.3538],
        [ 1.0630,  0.3360, -0.0909, -0.8746,  1.0164, -1.2803, -0.2362, -2.6568],
        [-1.4736, -0.5439, -1.0708,  0.6949, -0.3105,  0.3371, -0.8817,  1.7967],
        [-0.8348, -0.3252, -0.4370,  0.0824, -0.0481,  0.9269, -0.5550,  0.5851],
        [-0.0494,  0.0577, -1.0195,  0.0641,  0.4319, -0.3251,  0.5666, -1.0964]])...
	Related Layers:
		- parent layers: layernorm_9_429
		- child layers: matmul_80_525
		- shares parents with layers: linear_109_430, linear_110_431, linear_111_440, linear_112_442, linear_113_443, linear_114_452, linear_115_454, linear_116_455, linear_117_464, linear_118_466, linear_119_467, linear_120_476, linear_121_478, linear_122_479, linear_123_488, linear_124_490, linear_125_491, linear_126_500, linear_127_502, linear_128_503, linear_129_512, linear_130_514, linear_131_515
		- shares children with layers: dropout_48_523
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.7.value:1
	Time elapsed:  1.168E-04s
	Output of modules: blocks.4.sa.heads.7.value
	Output of bottom-level module: blocks.4.sa.heads.7.value:1
	Lookup keys: -130, 568, blocks.4.sa.heads.7.value, blocks.4.sa.heads.7.value:1, linear_132, linear_132:1, linear_132_524, linear_132_524:1
--------------------------------------------
Layer matmul_80_525, operation 529/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.5348,  0.8528, -0.8642, -0.3890,  0.9864, -1.1946, -0.3930, -1.4585],
        [ 0.1969,  0.5499, -0.1132,  0.0146,  0.3581,  0.0130, -0.5619, -0.3619],
        [ 0.1899,  0.5464, -0.0866,  0.0221,  0.3458,  0.0381, -0.5661, -0.3450],
        [ 0.1887,  0.5416, -0.0859,  0.0247,  0.3427,  0.0456, -0.5667, -0.3348],
        [ 0.1962,  0.5408, -0.0863,  0.0169,  0.3496,  0.0328, -0.5639, -0.3552],
        [-0.8307, -0.1920, -0.6484,  0.3486,  0.1163, -0.0447, -0.7491,  0.7830],
        [-0.3124,  0.1023, -0.4204,  0.0514,  0.4328, -0.3469, -0.6238, -0.0750],
        [-0.0455,  0.3076, -0.2462,  0.0271,  0.2817,  0.1714, -0.4626, -0.2366]])...
	Related Layers:
		- parent layers: dropout_48_523, linear_132_524
		- child layers: cat_5_526
		- shares parents with no other layers
		- shares children with layers: matmul_66_441, matmul_68_453, matmul_70_465, matmul_72_477, matmul_74_489, matmul_76_501, matmul_78_513
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.4.sa.heads.7:1
	Time elapsed:  8.297E-05s
	Output of modules: blocks.4.sa.heads.7
	Lookup keys: -129, 569, blocks.4.sa.heads.7, blocks.4.sa.heads.7:1, matmul_80, matmul_80:1, matmul_80_525, matmul_80_525:1
--------------------------------------------
Layer cat_5_526, operation 530/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[ 0.7832, -0.0449,  0.0546, -0.2030,  0.4132,  0.4058, -1.4607, -0.8329],
        [ 0.8286, -0.1154,  0.0937, -0.1692,  0.4768,  0.1091, -1.3389, -0.7977],
        [ 0.8440, -0.0830, -0.3117,  0.4860,  0.3859,  0.5177, -1.2691, -0.6930],
        [ 0.9313, -0.1507, -0.3795,  0.6430,  0.4103,  0.5876, -1.1010, -0.4847],
        [ 0.6509, -0.1520, -0.2761,  0.8232,  0.5129,  0.8111, -1.4522, -0.6124],
        [ 0.8287,  0.0383, -0.1022,  0.4705,  0.4189,  0.3512, -1.1499, -0.5652],
        [ 0.9468,  0.1035, -0.1481,  0.4985,  0.3573,  0.4379, -1.1852, -0.4635],
        [ 0.7096, -0.0994, -0.2477,  0.7214,  0.4201,  0.6220, -1.3324, -0.5659]])...
	Related Layers:
		- parent layers: matmul_66_441, matmul_68_453, matmul_70_465, matmul_72_477, matmul_74_489, matmul_76_501, matmul_78_513, matmul_80_525
		- child layers: linear_133_527
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: cat (grad_fn: CatBackward0) 
	Computed inside module: blocks.4.sa:1
	Time elapsed:  1.018E-04s
	Output of modules: none
	Lookup keys: -128, 570, cat_5, cat_5:1, cat_5_526, cat_5_526:1
--------------------------------------------
Layer linear_133_527, operation 531/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[ 2.1608e-01, -3.2050e-01, -1.3729e-01,  5.5842e-01, -8.0839e-01,
          2.5797e-01,  6.9984e-02, -1.4143e+00],
        [ 1.6735e-01, -3.2717e-01,  1.4852e-01, -4.5627e-02, -9.1020e-01,
          9.3419e-02, -4.4929e-01, -8.5083e-01],
        [ 2.9874e-01, -2.7419e-01,  1.7585e-01, -1.6416e-01, -7.4323e-01,
         -3.2145e-02, -5.2596e-01, -9.1314e-01],
        [ 3.6529e-01, -2.3691e-01,  2.2595e-01, -1.5443e-01, -6.8306e-01,
         -1.9123e-02, -5.3747e-01, -9.3981e-01],
        [ 5.9155e-01, -2.6971e-01,  1.9766e-01, -1.1257e-01, -8.8742e-01,
          7.4317e-03, -6.3379e-01, -9.7708e-01],
        [ 6.3773e-01,  4.1780e-01,  2.1666e-01,  4.9626e-01, -8.3623e-01,
         -3.6822e-04,  7.1946e-02, -2.7226e-01],
        [-1.2427e-01,  5.3158e-01,  3.2873e-01,  8.5523e-02, -6.5562e-01,
         -2.8728e-01,  6.5401e-02, -4.6300e-01],
        [-6.5246e-02, -2.3893e-01, -4.5819e-01, -6.4322e-01, -7.0754e-01,
         -2.0859e-01, -8.2146e-01, -6.0127e-01]])...
	Related Layers:
		- parent layers: cat_5_526
		- child layers: dropout_49_528
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (384, 384), (384,); 147840 params total (577.7 KB)
	Function: linear (grad_fn: ViewBackward0) 
	Computed inside module: blocks.4.sa.proj:1
	Time elapsed:  7.100E-04s
	Output of modules: blocks.4.sa.proj
	Output of bottom-level module: blocks.4.sa.proj:1
	Lookup keys: -127, 571, blocks.4.sa.proj, blocks.4.sa.proj:1, linear_133, linear_133:1, linear_133_527, linear_133_527:1
--------------------------------------------
Layer dropout_49_528, operation 532/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[ 2.1608e-01, -3.2050e-01, -1.3729e-01,  5.5842e-01, -8.0839e-01,
          2.5797e-01,  6.9984e-02, -1.4143e+00],
        [ 1.6735e-01, -3.2717e-01,  1.4852e-01, -4.5627e-02, -9.1020e-01,
          9.3419e-02, -4.4929e-01, -8.5083e-01],
        [ 2.9874e-01, -2.7419e-01,  1.7585e-01, -1.6416e-01, -7.4323e-01,
         -3.2145e-02, -5.2596e-01, -9.1314e-01],
        [ 3.6529e-01, -2.3691e-01,  2.2595e-01, -1.5443e-01, -6.8306e-01,
         -1.9123e-02, -5.3747e-01, -9.3981e-01],
        [ 5.9155e-01, -2.6971e-01,  1.9766e-01, -1.1257e-01, -8.8742e-01,
          7.4317e-03, -6.3379e-01, -9.7708e-01],
        [ 6.3773e-01,  4.1780e-01,  2.1666e-01,  4.9626e-01, -8.3623e-01,
         -3.6822e-04,  7.1946e-02, -2.7226e-01],
        [-1.2427e-01,  5.3158e-01,  3.2873e-01,  8.5523e-02, -6.5562e-01,
         -2.8728e-01,  6.5401e-02, -4.6300e-01],
        [-6.5246e-02, -2.3893e-01, -4.5819e-01, -6.4322e-01, -7.0754e-01,
         -2.0859e-01, -8.2146e-01, -6.0127e-01]])...
	Related Layers:
		- parent layers: linear_133_527
		- child layers: add_6_529:1
		- shares parents with no other layers
		- shares children with layers: add_5_423:2
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: ViewBackward0) 
	Computed inside module: blocks.4.sa.dropout:1
	Time elapsed:  7.439E-05s
	Output of modules: blocks.4.sa.dropout, blocks.4.sa
	Output of bottom-level module: blocks.4.sa.dropout:1
	Lookup keys: -126, 572, blocks.4.sa, blocks.4.sa.dropout, blocks.4.sa.dropout:1, blocks.4.sa:1, dropout_49, dropout_49:1, dropout_49_528, dropout_49_528:1
--------------------------------------------
Layer add_6_529 (pass 1/2), operation 533/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-6.8775, -4.1297,  2.4901, -3.2122,  0.7223,  5.9373,  0.8531,  4.8590],
        [-3.5875, -5.3443,  5.3541, -1.8926,  0.7300,  3.7287,  1.0824,  4.2940],
        [-5.1559, -7.4202,  4.7005, -3.4131, -3.3261, -0.6530, -0.5202,  1.8171],
        [-5.2838, -7.4572,  7.2663, -4.7141, -1.5634, -1.0431, -0.1462,  3.7304],
        [-3.3581, -6.0819,  5.1080, -4.7405,  4.0400, -1.3053,  0.0297,  4.8281],
        [-0.1224, -0.1433,  4.0438, -0.2791,  1.4525,  7.3797,  1.3727,  6.3736],
        [-4.7742,  0.9163,  5.3207, -2.0420,  3.4148, 10.0143,  1.5283,  0.6568],
        [-9.3115, -3.2321,  5.5331, -5.8456,  2.7153, -1.2344, -5.0135,  3.7511]])...
	Related Layers:
		- parent layers: add_5_423:2, dropout_49_528
		- child layers: layernorm_10_530, add_6_529:2
		- shares parents with layers: layernorm_9_429
		- shares children with layers: dropout_50_534
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __add__ (grad_fn: AddBackward0) 
	Computed inside module: blocks.4:1
	Time elapsed:  8.416E-05s
	Output of modules: none
	Lookup keys: -125, 573, add_6:1, add_6_529:1
--------------------------------------------
Layer layernorm_10_530, operation 534/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-1.7646, -1.0986,  0.5596, -0.8775,  0.1207,  1.4691,  0.1557,  1.1746],
        [-0.8918, -1.3555,  1.3198, -0.4937,  0.1676,  0.9457,  0.2585,  1.0718],
        [-1.3796, -2.0242,  1.2439, -0.9441, -0.9194, -0.1831, -0.1570,  0.4798],
        [-1.3701, -1.9656,  1.8416, -1.2600, -0.4381, -0.2924, -0.0678,  0.9443],
        [-0.8391, -1.5688,  1.3203, -1.2299,  1.0566, -0.3241,  0.0149,  1.2672],
        [-0.0271, -0.0330,  0.6742, -0.0714,  0.2302,  1.2934,  0.2183,  1.0975],
        [-0.9088,  0.1962,  1.0314, -0.4016,  0.6694,  1.9968,  0.3011,  0.1315],
        [-2.5184, -0.8878,  1.4924, -1.6207,  0.7339, -0.3410, -1.3895,  1.0235]])...
	Related Layers:
		- parent layers: add_6_529:1
		- child layers: linear_134_531
		- shares parents with layers: add_6_529:2
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (384,), (384,); 768 params total (3.2 KB)
	Function: layer_norm (grad_fn: NativeLayerNormBackward0) 
	Computed inside module: blocks.4.ln2:1
	Time elapsed:  2.618E-04s
	Output of modules: blocks.4.ln2
	Output of bottom-level module: blocks.4.ln2:1
	Lookup keys: -124, 574, blocks.4.ln2, blocks.4.ln2:1, layernorm_10, layernorm_10:1, layernorm_10_530, layernorm_10_530:1
--------------------------------------------
Layer linear_134_531, operation 535/648:
	Output tensor: shape=1x11x1536(67584 bytes), dype=torch.float32, Tensor size=67728 bytes, Tensor storage size=67648
		tensor([[-5.5056e-03, -5.4079e-01, -4.1358e-01,  8.4230e-02, -4.2724e-01,
         -5.1818e-01, -6.3029e-01, -3.6060e-01],
        [ 1.1189e-03, -7.0791e-01, -3.3561e-01, -5.2683e-01, -1.6775e-01,
          1.1626e+00, -2.1151e-01, -8.1540e-02],
        [-2.9461e-02, -6.3245e-01, -3.7847e-01, -4.2333e-01, -2.7208e-01,
          2.7492e-01, -7.4722e-01, -6.4252e-01],
        [-1.5335e-01, -9.5825e-01, -8.0401e-01, -1.6280e-01, -2.8266e-01,
          7.3720e-02, -1.6661e-01, -5.3538e-01],
        [ 8.6258e-01, -4.2299e-01, -7.5596e-01,  1.2799e-01, -7.1375e-01,
         -1.7600e-01, -5.6095e-01, -7.5104e-01],
        [-1.6501e+00, -5.4193e-01,  9.9427e-01,  2.6925e-01,  5.7550e-02,
          6.9190e-01, -4.4659e-01, -6.7141e-01],
        [-1.3694e+00, -9.8587e-01,  7.3327e-01,  1.3829e-02,  6.4531e-03,
          1.3095e+00, -1.0583e-02,  3.3886e-01],
        [-7.7447e-01, -7.9160e-01,  4.5504e-01, -8.5642e-01, -3.5361e-02,
          1.0152e+00, -6.8988e-01, -4.4762e-01]])...
	Related Layers:
		- parent layers: layernorm_10_530
		- child layers: relu_5_532
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (1536, 384), (1536,); 591360 params total (2.3 MB)
	Function: linear (grad_fn: ViewBackward0) 
	Computed inside module: blocks.4.ffwd.net.0:1
	Time elapsed:  6.664E-04s
	Output of modules: blocks.4.ffwd.net.0
	Output of bottom-level module: blocks.4.ffwd.net.0:1
	Lookup keys: -123, 575, blocks.4.ffwd.net.0, blocks.4.ffwd.net.0:1, linear_134, linear_134:1, linear_134_531, linear_134_531:1
--------------------------------------------
Layer relu_5_532, operation 536/648:
	Output tensor: shape=1x11x1536(67584 bytes), dype=torch.float32, Tensor size=67728 bytes, Tensor storage size=67648
		tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 8.4230e-02, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.1189e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1626e+00,
         0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.7492e-01,
         0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 7.3720e-02,
         0.0000e+00, 0.0000e+00],
        [8.6258e-01, 0.0000e+00, 0.0000e+00, 1.2799e-01, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 9.9427e-01, 2.6925e-01, 5.7550e-02, 6.9190e-01,
         0.0000e+00, 0.0000e+00],
        [0.0000e+00, 0.0000e+00, 7.3327e-01, 1.3829e-02, 6.4531e-03, 1.3095e+00,
         0.0000e+00, 3.3886e-01],
        [0.0000e+00, 0.0000e+00, 4.5504e-01, 0.0000e+00, 0.0000e+00, 1.0152e+00,
         0.0000e+00, 0.0000e+00]])...
	Related Layers:
		- parent layers: linear_134_531
		- child layers: linear_135_533
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: relu (grad_fn: ReluBackward0) 
	Computed inside module: blocks.4.ffwd.net.1:1
	Time elapsed:  9.394E-05s
	Output of modules: blocks.4.ffwd.net.1
	Output of bottom-level module: blocks.4.ffwd.net.1:1
	Lookup keys: -122, 576, blocks.4.ffwd.net.1, blocks.4.ffwd.net.1:1, relu_5, relu_5:1, relu_5_532, relu_5_532:1
--------------------------------------------
Layer linear_135_533, operation 537/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-0.4945, -0.1101,  0.8377,  0.7029,  0.6530, -0.6839,  0.6235,  0.2518],
        [-0.7067, -0.2526,  1.1326,  0.6514,  0.5845, -0.2841,  1.0980,  0.7843],
        [-0.3976, -0.1934,  0.8885,  0.4380,  0.4461, -0.4143,  0.4359,  0.3674],
        [-0.4176, -0.1901,  0.8577,  0.4561,  0.3761, -0.4993,  0.3946,  0.3783],
        [-0.3511, -0.1239,  0.8586,  0.4970,  0.4033, -0.7906,  0.2972,  0.1415],
        [-0.9360, -0.0763,  2.1742,  1.1978,  0.9462,  0.1542,  1.3714,  1.1277],
        [-0.6835, -0.0286,  1.7207,  1.1368,  0.6677, -0.1938,  1.1399,  0.5748],
        [-0.7510, -0.3758,  1.0438,  0.5747,  0.6562, -0.5741,  0.6120,  0.4682]])...
	Related Layers:
		- parent layers: relu_5_532
		- child layers: dropout_50_534
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (384, 1536), (384,); 590208 params total (2.3 MB)
	Function: linear (grad_fn: ViewBackward0) 
	Computed inside module: blocks.4.ffwd.net.2:1
	Time elapsed:  7.737E-04s
	Output of modules: blocks.4.ffwd.net.2
	Output of bottom-level module: blocks.4.ffwd.net.2:1
	Lookup keys: -121, 577, blocks.4.ffwd.net.2, blocks.4.ffwd.net.2:1, linear_135, linear_135:1, linear_135_533, linear_135_533:1
--------------------------------------------
Layer dropout_50_534, operation 538/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-0.4945, -0.1101,  0.8377,  0.7029,  0.6530, -0.6839,  0.6235,  0.2518],
        [-0.7067, -0.2526,  1.1326,  0.6514,  0.5845, -0.2841,  1.0980,  0.7843],
        [-0.3976, -0.1934,  0.8885,  0.4380,  0.4461, -0.4143,  0.4359,  0.3674],
        [-0.4176, -0.1901,  0.8577,  0.4561,  0.3761, -0.4993,  0.3946,  0.3783],
        [-0.3511, -0.1239,  0.8586,  0.4970,  0.4033, -0.7906,  0.2972,  0.1415],
        [-0.9360, -0.0763,  2.1742,  1.1978,  0.9462,  0.1542,  1.3714,  1.1277],
        [-0.6835, -0.0286,  1.7207,  1.1368,  0.6677, -0.1938,  1.1399,  0.5748],
        [-0.7510, -0.3758,  1.0438,  0.5747,  0.6562, -0.5741,  0.6120,  0.4682]])...
	Related Layers:
		- parent layers: linear_135_533
		- child layers: add_6_529:2
		- shares parents with no other layers
		- shares children with layers: add_6_529:1
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: ViewBackward0) 
	Computed inside module: blocks.4.ffwd.net.3:1
	Time elapsed:  6.509E-05s
	Output of modules: blocks.4.ffwd.net.3, blocks.4.ffwd.net, blocks.4.ffwd
	Output of bottom-level module: blocks.4.ffwd.net.3:1
	Lookup keys: -120, 578, blocks.4.ffwd, blocks.4.ffwd.net, blocks.4.ffwd.net.3, blocks.4.ffwd.net.3:1, blocks.4.ffwd.net:1, blocks.4.ffwd:1, dropout_50, dropout_50:1, dropout_50_534, dropout_50_534:1
--------------------------------------------
Layer add_6_529 (pass 2/2), operation 539/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[ -7.3720,  -4.2398,   3.3278,  -2.5092,   1.3753,   5.2534,   1.4766,
           5.1109],
        [ -4.2942,  -5.5969,   6.4867,  -1.2412,   1.3145,   3.4445,   2.1803,
           5.0783],
        [ -5.5535,  -7.6136,   5.5890,  -2.9751,  -2.8801,  -1.0673,  -0.0843,
           2.1845],
        [ -5.7013,  -7.6473,   8.1239,  -4.2579,  -1.1873,  -1.5424,   0.2484,
           4.1087],
        [ -3.7092,  -6.2058,   5.9665,  -4.2436,   4.4433,  -2.0959,   0.3269,
           4.9697],
        [ -1.0584,  -0.2197,   6.2180,   0.9188,   2.3987,   7.5339,   2.7441,
           7.5014],
        [ -5.4577,   0.8877,   7.0414,  -0.9052,   4.0825,   9.8205,   2.6682,
           1.2316],
        [-10.0625,  -3.6079,   6.5770,  -5.2709,   3.3716,  -1.8084,  -4.4015,
           4.2193]])...
	Related Layers:
		- parent layers: add_6_529:1, dropout_50_534
		- child layers: layernorm_11_535, add_7_635:1
		- shares parents with layers: layernorm_10_530
		- shares children with layers: dropout_59_634
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __add__ (grad_fn: AddBackward0) 
	Computed inside module: blocks.4:1
	Time elapsed:  7.272E-05s
	Output of modules: blocks.4
	Lookup keys: -119, 579, add_6:2, add_6_529:2, blocks.4, blocks.4:1
--------------------------------------------
Layer layernorm_11_535, operation 540/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-1.8640, -1.1067,  0.7603, -0.6710,  0.2869,  1.2575,  0.2991,  1.1985],
        [-1.0170, -1.3386,  1.5043, -0.3019,  0.2987,  0.8104,  0.4881,  1.1752],
        [-1.4644, -2.0271,  1.4473, -0.7923, -0.7556, -0.2899, -0.0420,  0.5599],
        [-1.4494, -1.9586,  2.0014, -1.0933, -0.3150, -0.4111,  0.0285,  0.9999],
        [-0.9100, -1.5503,  1.4918, -1.0485,  1.1140, -0.5122,  0.0836,  1.2445],
        [-0.1844, -0.0558,  0.9498,  0.1261,  0.3567,  1.1796,  0.3981,  1.1517],
        [-0.9797,  0.1637,  1.2645, -0.1600,  0.7355,  1.7964,  0.4704,  0.2238],
        [-2.6119, -0.9512,  1.6887, -1.3769,  0.8628, -0.4771, -1.1493,  1.0806]])...
	Related Layers:
		- parent layers: add_6_529:2
		- child layers: linear_136_536, linear_137_537, linear_138_546, linear_139_548, linear_140_549, linear_141_558, linear_142_560, linear_143_561, linear_144_570, linear_145_572, linear_146_573, linear_147_582, linear_148_584, linear_149_585, linear_150_594, linear_151_596, linear_152_597, linear_153_606, linear_154_608, linear_155_609, linear_156_618, linear_157_620, linear_158_621, linear_159_630
		- shares parents with layers: add_7_635:1
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (384,), (384,); 768 params total (3.2 KB)
	Function: layer_norm (grad_fn: NativeLayerNormBackward0) 
	Computed inside module: blocks.5.ln1:1
	Time elapsed:  2.370E-04s
	Output of modules: blocks.5.ln1
	Output of bottom-level module: blocks.5.ln1:1
	Lookup keys: -118, 580, blocks.5.ln1, blocks.5.ln1:1, layernorm_11, layernorm_11:1, layernorm_11_535, layernorm_11_535:1
--------------------------------------------
Layer linear_136_536, operation 541/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 2.0857,  2.5797, -1.1286,  0.3443,  2.3957,  1.7388, -2.1091, -0.2937],
        [ 1.2276,  1.8238,  0.6722, -0.0621,  2.3817,  2.9417, -1.4251, -1.5398],
        [ 1.8103,  1.8555, -0.0621, -0.3476,  2.2029,  1.0292, -0.7667, -1.8620],
        [ 1.5557,  1.9447,  0.0437, -1.0341,  2.8474,  1.2243, -0.7915, -1.9322],
        [ 0.4993,  1.1631, -0.9118, -0.7266,  2.3173,  0.3312, -1.6878, -0.6731],
        [ 2.7111,  3.2041,  2.7307,  1.2854,  0.3283,  4.3286, -1.2124,  0.1556],
        [ 1.4385,  2.7232,  2.4136, -0.6988,  1.8689,  3.4873, -0.4166, -2.2319],
        [ 1.4517,  1.0056,  0.0266, -1.1557,  3.1363,  2.2514, -0.9741, -1.4336]])...
	Related Layers:
		- parent layers: layernorm_11_535
		- child layers: transpose_41_538
		- shares parents with layers: linear_137_537, linear_138_546, linear_139_548, linear_140_549, linear_141_558, linear_142_560, linear_143_561, linear_144_570, linear_145_572, linear_146_573, linear_147_582, linear_148_584, linear_149_585, linear_150_594, linear_151_596, linear_152_597, linear_153_606, linear_154_608, linear_155_609, linear_156_618, linear_157_620, linear_158_621, linear_159_630
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.0.key:1
	Time elapsed:  1.256E-04s
	Output of modules: blocks.5.sa.heads.0.key
	Output of bottom-level module: blocks.5.sa.heads.0.key:1
	Lookup keys: -117, 581, blocks.5.sa.heads.0.key, blocks.5.sa.heads.0.key:1, linear_136, linear_136:1, linear_136_536, linear_136_536:1
--------------------------------------------
Layer linear_137_537, operation 542/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.9411, -2.3155,  0.2404, -0.4679, -1.2123, -0.1241,  0.5991,  0.6255],
        [-1.6044, -1.8621, -1.3446, -0.2014, -1.4462, -1.1562,  0.8330, -0.1893],
        [-0.9418, -1.8661,  0.4168, -1.0618, -0.5880, -0.5041,  1.2245,  0.7444],
        [-0.8930, -2.1768,  1.1737, -1.1631, -0.3033, -0.4060,  1.0843,  0.3196],
        [-0.9256, -2.2890, -0.6360, -1.4430, -0.6720, -1.3470,  0.7310,  0.0462],
        [-1.1727, -0.9842,  0.8000,  0.5482, -0.3148,  0.0389,  1.2857, -0.0631],
        [ 0.2829, -0.1894,  0.4818, -1.3411,  1.0052, -0.0380,  0.4211, -0.5226],
        [-0.7600, -1.0461, -0.9988, -2.2874, -0.1022, -0.6838,  0.1469, -0.8041]])...
	Related Layers:
		- parent layers: layernorm_11_535
		- child layers: matmul_81_539
		- shares parents with layers: linear_136_536, linear_138_546, linear_139_548, linear_140_549, linear_141_558, linear_142_560, linear_143_561, linear_144_570, linear_145_572, linear_146_573, linear_147_582, linear_148_584, linear_149_585, linear_150_594, linear_151_596, linear_152_597, linear_153_606, linear_154_608, linear_155_609, linear_156_618, linear_157_620, linear_158_621, linear_159_630
		- shares children with layers: transpose_41_538
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.0.query:1
	Time elapsed:  1.137E-04s
	Output of modules: blocks.5.sa.heads.0.query
	Output of bottom-level module: blocks.5.sa.heads.0.query:1
	Lookup keys: -116, 582, blocks.5.sa.heads.0.query, blocks.5.sa.heads.0.query:1, linear_137, linear_137:1, linear_137_537, linear_137_537:1
--------------------------------------------
Layer transpose_41_538, operation 543/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 2.0857,  1.2276,  1.8103,  1.5557,  0.4993,  2.7111,  1.4385,  1.4517],
        [ 2.5797,  1.8238,  1.8555,  1.9447,  1.1631,  3.2041,  2.7232,  1.0056],
        [-1.1286,  0.6722, -0.0621,  0.0437, -0.9118,  2.7307,  2.4136,  0.0266],
        [ 0.3443, -0.0621, -0.3476, -1.0341, -0.7266,  1.2854, -0.6988, -1.1557],
        [ 2.3957,  2.3817,  2.2029,  2.8474,  2.3173,  0.3283,  1.8689,  3.1363],
        [ 1.7388,  2.9417,  1.0292,  1.2243,  0.3312,  4.3286,  3.4873,  2.2514],
        [-2.1091, -1.4251, -0.7667, -0.7915, -1.6878, -1.2124, -0.4166, -0.9741],
        [-0.2937, -1.5398, -1.8620, -1.9322, -0.6731,  0.1556, -2.2319, -1.4336]])...
	Related Layers:
		- parent layers: linear_136_536
		- child layers: matmul_81_539
		- shares parents with no other layers
		- shares children with layers: linear_137_537
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.5.sa.heads.0:1
	Time elapsed:  6.104E-05s
	Output of modules: none
	Lookup keys: -115, 583, transpose_41, transpose_41:1, transpose_41_538, transpose_41_538:1
--------------------------------------------
Layer matmul_81_539, operation 544/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-6.6396e+01, -5.1693e+01, -4.1906e+01, -4.4432e+01, -2.8038e+01,
         -8.4734e+01, -6.4142e+01, -3.6846e+01],
        [-9.2816e+01, -7.5450e+01, -5.9980e+01, -6.5525e+01, -3.8232e+01,
         -1.4359e+02, -1.0811e+02, -5.6351e+01],
        [-7.6221e+01, -3.2423e+01, -3.6309e+01, -3.6415e+01, -2.4952e+01,
         -9.7342e+01, -4.2878e+01, -1.6107e+01],
        [-7.1498e+01, -2.8985e+01, -3.2343e+01, -3.2915e+01, -2.3022e+01,
         -8.4332e+01, -3.3131e+01, -1.2613e+01],
        [-8.9052e+01, -5.5598e+01, -4.6586e+01, -4.7306e+01, -2.6674e+01,
         -1.4216e+02, -7.8652e+01, -3.2910e+01],
        [-4.4348e+01, -2.9830e+01, -3.2155e+01, -3.4976e+01, -2.8928e+01,
         -3.3809e+01, -3.3173e+01, -2.6770e+01],
        [ 8.1462e+00,  1.5358e+01,  1.7150e+01,  2.0722e+01,  1.2007e+01,
          3.3227e+00,  1.6517e+01,  2.0414e+01],
        [-4.9195e+01, -1.6562e+01, -1.2181e+01, -9.3542e+00,  1.0524e-01,
         -1.1350e+02, -3.6197e+01,  2.9610e+00]])...
	Related Layers:
		- parent layers: linear_137_537, transpose_41_538
		- child layers: mul_41_540
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.0:1
	Time elapsed:  9.036E-05s
	Output of modules: none
	Lookup keys: -114, 584, matmul_81, matmul_81:1, matmul_81_539, matmul_81_539:1
--------------------------------------------
Layer mul_41_540, operation 545/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-3.3882e+00, -2.6379e+00, -2.1385e+00, -2.2674e+00, -1.4308e+00,
         -4.3241e+00, -3.2732e+00, -1.8803e+00],
        [-4.7365e+00, -3.8503e+00, -3.0609e+00, -3.3438e+00, -1.9510e+00,
         -7.3274e+00, -5.5170e+00, -2.8756e+00],
        [-3.8896e+00, -1.6546e+00, -1.8529e+00, -1.8583e+00, -1.2733e+00,
         -4.9674e+00, -2.1881e+00, -8.2194e-01],
        [-3.6486e+00, -1.4792e+00, -1.6505e+00, -1.6797e+00, -1.1748e+00,
         -4.3036e+00, -1.6907e+00, -6.4363e-01],
        [-4.5444e+00, -2.8372e+00, -2.3773e+00, -2.4141e+00, -1.3612e+00,
         -7.2543e+00, -4.0137e+00, -1.6794e+00],
        [-2.2631e+00, -1.5222e+00, -1.6409e+00, -1.7849e+00, -1.4762e+00,
         -1.7253e+00, -1.6929e+00, -1.3661e+00],
        [ 4.1571e-01,  7.8373e-01,  8.7517e-01,  1.0575e+00,  6.1275e-01,
          1.6956e-01,  8.4287e-01,  1.0417e+00],
        [-2.5105e+00, -8.4517e-01, -6.2163e-01, -4.7736e-01,  5.3704e-03,
         -5.7921e+00, -1.8472e+00,  1.5110e-01]])...
	Related Layers:
		- parent layers: matmul_81_539
		- child layers: maskedfill_41_543
		- shares parents with no other layers
		- shares children with layers: eq_41_542
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.5.sa.heads.0:1
	Time elapsed:  7.176E-05s
	Output of modules: none
	Lookup keys: -113, 585, mul_41, mul_41:1, mul_41_540, mul_41_540:1
--------------------------------------------
Layer getitem_41_541, operation 546/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_41
		- child layers: eq_41_542
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.559E-05s
	Output of modules: none
	Lookup keys: -111, 587, getitem_41, getitem_41:1, getitem_41_541, getitem_41_541:1
--------------------------------------------
Layer eq_41_542, operation 547/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_41_541
		- child layers: maskedfill_41_543
		- shares parents with no other layers
		- shares children with layers: mul_41_540
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.392E-05s
	Output of modules: none
	Lookup keys: -110, 588, eq_41, eq_41:1, eq_41_542, eq_41_542:1
--------------------------------------------
Layer maskedfill_41_543, operation 548/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-3.3882e+00,        -inf,        -inf,        -inf,        -inf,
                -inf,        -inf,        -inf],
        [-4.7365e+00, -3.8503e+00,        -inf,        -inf,        -inf,
                -inf,        -inf,        -inf],
        [-3.8896e+00, -1.6546e+00, -1.8529e+00,        -inf,        -inf,
                -inf,        -inf,        -inf],
        [-3.6486e+00, -1.4792e+00, -1.6505e+00, -1.6797e+00,        -inf,
                -inf,        -inf,        -inf],
        [-4.5444e+00, -2.8372e+00, -2.3773e+00, -2.4141e+00, -1.3612e+00,
                -inf,        -inf,        -inf],
        [-2.2631e+00, -1.5222e+00, -1.6409e+00, -1.7849e+00, -1.4762e+00,
         -1.7253e+00,        -inf,        -inf],
        [ 4.1571e-01,  7.8373e-01,  8.7517e-01,  1.0575e+00,  6.1275e-01,
          1.6956e-01,  8.4287e-01,        -inf],
        [-2.5105e+00, -8.4517e-01, -6.2163e-01, -4.7736e-01,  5.3704e-03,
         -5.7921e+00, -1.8472e+00,  1.5110e-01]])...
	Related Layers:
		- parent layers: mul_41_540, eq_41_542
		- child layers: softmax_41_544
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.5.sa.heads.0:1
	Time elapsed:  8.464E-05s
	Output of modules: none
	Lookup keys: -109, 589, maskedfill_41, maskedfill_41:1, maskedfill_41_543, maskedfill_41_543:1
--------------------------------------------
Layer softmax_41_544, operation 549/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.9189e-01, 7.0811e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [5.5516e-02, 5.1891e-01, 4.2557e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [4.1166e-02, 3.6035e-01, 3.0362e-01, 2.9487e-01, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.0926e-02, 1.1537e-01, 1.8274e-01, 1.7615e-01, 5.0481e-01, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [9.5392e-02, 2.0011e-01, 1.7773e-01, 1.5390e-01, 2.0953e-01, 1.6334e-01,
         0.0000e+00, 0.0000e+00],
        [1.0570e-01, 1.5273e-01, 1.6735e-01, 2.0082e-01, 1.2872e-01, 8.2639e-02,
         1.6203e-01, 0.0000e+00],
        [2.0320e-02, 1.0744e-01, 1.3435e-01, 1.5520e-01, 2.5151e-01, 7.6338e-04,
         3.9446e-02, 2.9096e-01]])...
	Related Layers:
		- parent layers: maskedfill_41_543
		- child layers: dropout_51_545
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.5.sa.heads.0:1
	Time elapsed:  6.413E-05s
	Output of modules: none
	Lookup keys: -108, 590, softmax_41, softmax_41:1, softmax_41_544, softmax_41_544:1
--------------------------------------------
Layer dropout_51_545, operation 550/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.9189e-01, 7.0811e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [5.5516e-02, 5.1891e-01, 4.2557e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [4.1166e-02, 3.6035e-01, 3.0362e-01, 2.9487e-01, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.0926e-02, 1.1537e-01, 1.8274e-01, 1.7615e-01, 5.0481e-01, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [9.5392e-02, 2.0011e-01, 1.7773e-01, 1.5390e-01, 2.0953e-01, 1.6334e-01,
         0.0000e+00, 0.0000e+00],
        [1.0570e-01, 1.5273e-01, 1.6735e-01, 2.0082e-01, 1.2872e-01, 8.2639e-02,
         1.6203e-01, 0.0000e+00],
        [2.0320e-02, 1.0744e-01, 1.3435e-01, 1.5520e-01, 2.5151e-01, 7.6338e-04,
         3.9446e-02, 2.9096e-01]])...
	Related Layers:
		- parent layers: softmax_41_544
		- child layers: matmul_82_547
		- shares parents with no other layers
		- shares children with layers: linear_138_546
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.5.sa.heads.0.dropout:1
	Time elapsed:  5.484E-05s
	Output of modules: blocks.5.sa.heads.0.dropout
	Output of bottom-level module: blocks.5.sa.heads.0.dropout:1
	Lookup keys: -107, 591, blocks.5.sa.heads.0.dropout, blocks.5.sa.heads.0.dropout:1, dropout_51, dropout_51:1, dropout_51_545, dropout_51_545:1
--------------------------------------------
Layer linear_138_546, operation 551/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.8003,  1.2568, -0.8669, -0.9908, -1.2398,  0.5950, -0.5027,  0.8934],
        [ 0.3404,  0.1321, -0.7349, -0.4245,  0.7430,  0.9163,  0.4687, -0.2626],
        [ 0.9483,  0.0920, -1.6007, -0.4392, -1.1604,  1.3547, -0.6485,  1.0565],
        [ 0.9824,  0.2800, -1.2020, -0.3012, -0.8826,  1.2756, -0.7643,  0.8165],
        [ 1.4491,  1.0442, -1.4565, -0.2877, -0.8498,  0.6588, -0.6645,  0.5356],
        [-2.6980,  0.0074,  0.8650, -0.3771,  1.6167,  0.4697, -0.2345, -1.0472],
        [-2.1036, -0.8421,  0.4820, -0.4196,  1.5004,  0.9160,  0.3339, -0.6913],
        [ 0.4042,  0.1328, -1.2242, -0.9215, -0.8462,  0.9767,  0.0592,  0.0199]])...
	Related Layers:
		- parent layers: layernorm_11_535
		- child layers: matmul_82_547
		- shares parents with layers: linear_136_536, linear_137_537, linear_139_548, linear_140_549, linear_141_558, linear_142_560, linear_143_561, linear_144_570, linear_145_572, linear_146_573, linear_147_582, linear_148_584, linear_149_585, linear_150_594, linear_151_596, linear_152_597, linear_153_606, linear_154_608, linear_155_609, linear_156_618, linear_157_620, linear_158_621, linear_159_630
		- shares children with layers: dropout_51_545
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.0.value:1
	Time elapsed:  1.204E-04s
	Output of modules: blocks.5.sa.heads.0.value
	Output of bottom-level module: blocks.5.sa.heads.0.value:1
	Lookup keys: -106, 592, blocks.5.sa.heads.0.value, blocks.5.sa.heads.0.value:1, linear_138, linear_138:1, linear_138_546, linear_138_546:1
--------------------------------------------
Layer matmul_82_547, operation 552/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.8003,  1.2568, -0.8669, -0.9908, -1.2398,  0.5950, -0.5027,  0.8934],
        [ 0.4746,  0.4604, -0.7734, -0.5898,  0.1642,  0.8225,  0.1851,  0.0749],
        [ 0.6246,  0.1775, -1.1107, -0.4622, -0.1771,  1.0850, -0.0607,  0.3630],
        [ 0.7332,  0.2099, -1.1409, -0.4159, -0.3959,  1.1421, -0.2741,  0.5037],
        [ 1.1339,  0.6348, -1.3424, -0.3482, -0.7367,  0.9230, -0.5450,  0.5957],
        [ 0.3271,  0.4258, -0.8631, -0.4258, -0.2256,  0.8920, -0.3646,  0.2873],
        [ 0.1153,  0.2232, -0.7510, -0.4397, -0.1217,  0.9577, -0.2944,  0.2655],
        [ 0.7298,  0.3636, -1.2010, -0.5288, -0.6378,  0.9769, -0.3025,  0.3710]])...
	Related Layers:
		- parent layers: dropout_51_545, linear_138_546
		- child layers: cat_6_632
		- shares parents with no other layers
		- shares children with layers: matmul_84_559, matmul_86_571, matmul_88_583, matmul_90_595, matmul_92_607, matmul_94_619, matmul_96_631
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.0:1
	Time elapsed:  8.988E-05s
	Output of modules: blocks.5.sa.heads.0
	Lookup keys: -105, 593, blocks.5.sa.heads.0, blocks.5.sa.heads.0:1, matmul_82, matmul_82:1, matmul_82_547, matmul_82_547:1
--------------------------------------------
Layer linear_139_548, operation 553/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 2.8871, -2.0631,  1.1085, -3.4195, -2.4836,  2.0853, -0.1231,  1.3579],
        [ 4.4078, -2.0713,  3.1131, -2.9573, -3.8097,  3.7638, -0.2885,  1.5942],
        [ 2.1303, -2.2433,  1.4425, -2.3127, -1.7147,  1.8305, -0.2518,  1.8785],
        [ 2.5185, -2.2545,  1.6794, -2.4631, -1.7332,  2.0501, -0.4457,  1.7069],
        [ 1.4916, -1.9542,  0.0469, -3.0525, -0.2925,  0.5036, -0.2926,  2.0190],
        [ 4.0359, -0.8214,  2.3258,  0.2755, -5.2027,  4.7844,  0.2026,  0.9949],
        [ 3.7340, -1.3163,  2.3965, -1.1928, -3.5973,  4.3535,  0.1726,  0.4522],
        [ 2.2979, -2.3624,  0.8747, -1.9888, -1.1341,  2.2750,  0.5135,  1.8019]])...
	Related Layers:
		- parent layers: layernorm_11_535
		- child layers: transpose_42_550
		- shares parents with layers: linear_136_536, linear_137_537, linear_138_546, linear_140_549, linear_141_558, linear_142_560, linear_143_561, linear_144_570, linear_145_572, linear_146_573, linear_147_582, linear_148_584, linear_149_585, linear_150_594, linear_151_596, linear_152_597, linear_153_606, linear_154_608, linear_155_609, linear_156_618, linear_157_620, linear_158_621, linear_159_630
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.1.key:1
	Time elapsed:  1.137E-04s
	Output of modules: blocks.5.sa.heads.1.key
	Output of bottom-level module: blocks.5.sa.heads.1.key:1
	Lookup keys: -104, 594, blocks.5.sa.heads.1.key, blocks.5.sa.heads.1.key:1, linear_139, linear_139:1, linear_139_548, linear_139_548:1
--------------------------------------------
Layer linear_140_549, operation 554/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-1.1114,  1.1645, -1.6168,  1.1815,  2.9789, -1.7207, -0.0363, -0.1368],
        [-1.8747,  0.6872, -2.5585,  2.7424,  2.9346, -0.8386,  0.5619,  0.2116],
        [-2.1807,  1.6425, -2.5647,  2.1792,  2.6665, -1.0759,  0.1748, -0.0079],
        [-2.4448,  1.6484, -2.5824,  2.0131,  2.5194, -1.4179,  0.1478,  0.0180],
        [-2.0759,  1.9914, -2.5080,  2.6377,  3.4333, -2.1360,  0.5192, -0.6103],
        [ 0.0285, -0.3991, -0.1607,  0.5088,  0.6440,  0.6843, -0.5650,  0.4594],
        [-1.1755,  0.9106, -1.3443,  1.8835,  1.2634, -0.2481,  0.4719, -0.1263],
        [-2.0466,  1.7175, -2.0888,  2.0285,  3.0383, -1.7847,  0.4920, -0.1446]])...
	Related Layers:
		- parent layers: layernorm_11_535
		- child layers: matmul_83_551
		- shares parents with layers: linear_136_536, linear_137_537, linear_138_546, linear_139_548, linear_141_558, linear_142_560, linear_143_561, linear_144_570, linear_145_572, linear_146_573, linear_147_582, linear_148_584, linear_149_585, linear_150_594, linear_151_596, linear_152_597, linear_153_606, linear_154_608, linear_155_609, linear_156_618, linear_157_620, linear_158_621, linear_159_630
		- shares children with layers: transpose_42_550
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.1.query:1
	Time elapsed:  1.130E-04s
	Output of modules: blocks.5.sa.heads.1.query
	Output of bottom-level module: blocks.5.sa.heads.1.query:1
	Lookup keys: -103, 595, blocks.5.sa.heads.1.query, blocks.5.sa.heads.1.query:1, linear_140, linear_140:1, linear_140_549, linear_140_549:1
--------------------------------------------
Layer transpose_42_550, operation 555/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 2.8871,  4.4078,  2.1303,  2.5185,  1.4916,  4.0359,  3.7340,  2.2979],
        [-2.0631, -2.0713, -2.2433, -2.2545, -1.9542, -0.8214, -1.3163, -2.3624],
        [ 1.1085,  3.1131,  1.4425,  1.6794,  0.0469,  2.3258,  2.3965,  0.8747],
        [-3.4195, -2.9573, -2.3127, -2.4631, -3.0525,  0.2755, -1.1928, -1.9888],
        [-2.4836, -3.8097, -1.7147, -1.7332, -0.2925, -5.2027, -3.5973, -1.1341],
        [ 2.0853,  3.7638,  1.8305,  2.0501,  0.5036,  4.7844,  4.3535,  2.2750],
        [-0.1231, -0.2885, -0.2518, -0.4457, -0.2926,  0.2026,  0.1726,  0.5135],
        [ 1.3579,  1.5942,  1.8785,  1.7069,  2.0190,  0.9949,  0.4522,  1.8019]])...
	Related Layers:
		- parent layers: linear_139_548
		- child layers: matmul_83_551
		- shares parents with no other layers
		- shares children with layers: linear_140_549
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.5.sa.heads.1:1
	Time elapsed:  5.865E-05s
	Output of modules: none
	Lookup keys: -102, 596, transpose_42, transpose_42:1, transpose_42_550, transpose_42_550:1
--------------------------------------------
Layer matmul_83_551, operation 556/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-146.8383, -161.5392, -105.3598, -113.2433,  -68.3057, -146.4527,
         -130.5041, -109.0282],
        [-200.5060, -218.7236, -140.7054, -151.5935, -107.9754, -152.5718,
         -150.9392, -140.2040],
        [-187.9628, -204.3036, -135.5754, -146.1702, -102.7916, -141.8240,
         -140.5135, -134.9429],
        [-198.9025, -215.5318, -144.1924, -156.1731, -108.8450, -154.6932,
         -150.3382, -144.4554],
        [-233.0804, -249.9052, -169.9740, -182.7153, -123.6172, -204.3016,
         -191.0905, -173.5086],
        [ -28.0113,  -18.2611,  -17.6360,  -18.0277,  -21.6478,    7.3022,
            7.1945,  -17.6007],
        [-140.3192, -146.6035,  -97.2673, -102.7920,  -84.2973,  -68.9791,
          -78.2280,  -96.4105],
        [-232.9945, -241.5823, -166.1464, -176.0750, -124.4741, -192.4716,
         -177.3863, -172.1915]])...
	Related Layers:
		- parent layers: linear_140_549, transpose_42_550
		- child layers: mul_42_552
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.1:1
	Time elapsed:  9.251E-05s
	Output of modules: none
	Lookup keys: -101, 597, matmul_83, matmul_83:1, matmul_83_551, matmul_83_551:1
--------------------------------------------
Layer mul_42_552, operation 557/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -7.4933,  -8.2435,  -5.3766,  -5.7789,  -3.4857,  -7.4736,  -6.6598,
          -5.5638],
        [-10.2320, -11.1617,  -7.1803,  -7.7360,  -5.5101,  -7.7859,  -7.7026,
          -7.1548],
        [ -9.5919, -10.4258,  -6.9186,  -7.4592,  -5.2456,  -7.2374,  -7.1705,
          -6.8863],
        [-10.1502, -10.9988,  -7.3583,  -7.9697,  -5.5545,  -7.8942,  -7.6719,
          -7.3717],
        [-11.8943, -12.7529,  -8.6740,  -9.3241,  -6.3083, -10.4257,  -9.7515,
          -8.8543],
        [ -1.4294,  -0.9319,  -0.9000,  -0.9200,  -1.1047,   0.3726,   0.3671,
          -0.8982],
        [ -7.1606,  -7.4813,  -4.9637,  -5.2456,  -4.3018,  -3.5201,  -3.9921,
          -4.9199],
        [-11.8900, -12.3282,  -8.4786,  -8.9853,  -6.3520,  -9.8220,  -9.0522,
          -8.7871]])...
	Related Layers:
		- parent layers: matmul_83_551
		- child layers: maskedfill_42_555
		- shares parents with no other layers
		- shares children with layers: eq_42_554
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.5.sa.heads.1:1
	Time elapsed:  6.819E-05s
	Output of modules: none
	Lookup keys: -100, 598, mul_42, mul_42:1, mul_42_552, mul_42_552:1
--------------------------------------------
Layer getitem_42_553, operation 558/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_42
		- child layers: eq_42_554
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.346E-05s
	Output of modules: none
	Lookup keys: -98, 600, getitem_42, getitem_42:1, getitem_42_553, getitem_42_553:1
--------------------------------------------
Layer eq_42_554, operation 559/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_42_553
		- child layers: maskedfill_42_555
		- shares parents with no other layers
		- shares children with layers: mul_42_552
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  7.772E-05s
	Output of modules: none
	Lookup keys: -97, 601, eq_42, eq_42:1, eq_42_554, eq_42_554:1
--------------------------------------------
Layer maskedfill_42_555, operation 560/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -7.4933,     -inf,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [-10.2320, -11.1617,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -9.5919, -10.4258,  -6.9186,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [-10.1502, -10.9988,  -7.3583,  -7.9697,     -inf,     -inf,     -inf,
             -inf],
        [-11.8943, -12.7529,  -8.6740,  -9.3241,  -6.3083,     -inf,     -inf,
             -inf],
        [ -1.4294,  -0.9319,  -0.9000,  -0.9200,  -1.1047,   0.3726,     -inf,
             -inf],
        [ -7.1606,  -7.4813,  -4.9637,  -5.2456,  -4.3018,  -3.5201,  -3.9921,
             -inf],
        [-11.8900, -12.3282,  -8.4786,  -8.9853,  -6.3520,  -9.8220,  -9.0522,
          -8.7871]])...
	Related Layers:
		- parent layers: mul_42_552, eq_42_554
		- child layers: softmax_42_556
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.5.sa.heads.1:1
	Time elapsed:  8.368E-05s
	Output of modules: none
	Lookup keys: -96, 602, maskedfill_42, maskedfill_42:1, maskedfill_42_555, maskedfill_42_555:1
--------------------------------------------
Layer softmax_42_556, operation 561/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.7170, 0.2830, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0628, 0.0273, 0.9099, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0376, 0.0161, 0.6134, 0.3329, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0033, 0.0014, 0.0818, 0.0427, 0.8709, 0.0000, 0.0000, 0.0000],
        [0.0743, 0.1223, 0.1262, 0.1237, 0.1029, 0.4506, 0.0000, 0.0000],
        [0.0103, 0.0075, 0.0929, 0.0701, 0.1801, 0.3936, 0.2455, 0.0000],
        [0.0028, 0.0018, 0.0862, 0.0519, 0.7228, 0.0225, 0.0486, 0.0633]])...
	Related Layers:
		- parent layers: maskedfill_42_555
		- child layers: dropout_52_557
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.5.sa.heads.1:1
	Time elapsed:  7.057E-05s
	Output of modules: none
	Lookup keys: -95, 603, softmax_42, softmax_42:1, softmax_42_556, softmax_42_556:1
--------------------------------------------
Layer dropout_52_557, operation 562/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.7170, 0.2830, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0628, 0.0273, 0.9099, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0376, 0.0161, 0.6134, 0.3329, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0033, 0.0014, 0.0818, 0.0427, 0.8709, 0.0000, 0.0000, 0.0000],
        [0.0743, 0.1223, 0.1262, 0.1237, 0.1029, 0.4506, 0.0000, 0.0000],
        [0.0103, 0.0075, 0.0929, 0.0701, 0.1801, 0.3936, 0.2455, 0.0000],
        [0.0028, 0.0018, 0.0862, 0.0519, 0.7228, 0.0225, 0.0486, 0.0633]])...
	Related Layers:
		- parent layers: softmax_42_556
		- child layers: matmul_84_559
		- shares parents with no other layers
		- shares children with layers: linear_141_558
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.5.sa.heads.1.dropout:1
	Time elapsed:  5.651E-05s
	Output of modules: blocks.5.sa.heads.1.dropout
	Output of bottom-level module: blocks.5.sa.heads.1.dropout:1
	Lookup keys: -94, 604, blocks.5.sa.heads.1.dropout, blocks.5.sa.heads.1.dropout:1, dropout_52, dropout_52:1, dropout_52_557, dropout_52_557:1
--------------------------------------------
Layer linear_141_558, operation 563/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.9672, -0.0790,  1.5903,  0.1345, -0.6321, -0.4160, -0.6107,  0.3801],
        [-0.2523, -0.0353,  0.1450,  0.1462, -0.2909, -0.2391,  1.3778,  1.3971],
        [-0.2875, -0.3877,  0.8439,  0.5611, -0.3466, -0.1811,  0.1369,  0.6440],
        [-0.2299, -0.0285,  0.8439,  1.0098, -0.4125,  0.0859,  0.6587,  0.9504],
        [-0.9003, -0.1949,  1.3023,  0.2535, -0.5754, -0.1035, -0.3286,  0.3644],
        [ 0.1604,  0.7314, -0.4351,  0.5492, -0.1954, -0.3107,  0.5712,  0.3460],
        [-0.8338, -0.1540, -0.3932,  0.1283, -0.3132, -0.6415,  1.0571,  0.3310],
        [-0.8664, -0.9537,  0.4331,  0.9131, -0.7148,  0.5138, -0.2512,  0.8256]])...
	Related Layers:
		- parent layers: layernorm_11_535
		- child layers: matmul_84_559
		- shares parents with layers: linear_136_536, linear_137_537, linear_138_546, linear_139_548, linear_140_549, linear_142_560, linear_143_561, linear_144_570, linear_145_572, linear_146_573, linear_147_582, linear_148_584, linear_149_585, linear_150_594, linear_151_596, linear_152_597, linear_153_606, linear_154_608, linear_155_609, linear_156_618, linear_157_620, linear_158_621, linear_159_630
		- shares children with layers: dropout_52_557
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.1.value:1
	Time elapsed:  1.221E-04s
	Output of modules: blocks.5.sa.heads.1.value
	Output of bottom-level module: blocks.5.sa.heads.1.value:1
	Lookup keys: -93, 605, blocks.5.sa.heads.1.value, blocks.5.sa.heads.1.value:1, linear_141, linear_141:1, linear_141_558, linear_141_558:1
--------------------------------------------
Layer matmul_84_559, operation 564/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.9672, -0.0790,  1.5903,  0.1345, -0.6321, -0.4160, -0.6107,  0.3801],
        [-0.7649, -0.0666,  1.1813,  0.1378, -0.5355, -0.3660, -0.0479,  0.6679],
        [-0.3292, -0.3587,  0.8717,  0.5230, -0.3630, -0.1974,  0.1238,  0.6480],
        [-0.2933, -0.2508,  0.8607,  0.6877, -0.3784, -0.1020,  0.3024,  0.7482],
        [-0.8209, -0.2029,  1.2446,  0.3104, -0.5495, -0.1029, -0.2470,  0.4138],
        [-0.1878,  0.2469,  0.2847,  0.4972, -0.3246, -0.2230,  0.4454,  0.5913],
        [-0.3584,  0.1759,  0.1219,  0.4187, -0.3272, -0.3153,  0.4880,  0.4239],
        [-0.7824, -0.2274,  1.0612,  0.3611, -0.5344, -0.0932, -0.1424,  0.4480]])...
	Related Layers:
		- parent layers: dropout_52_557, linear_141_558
		- child layers: cat_6_632
		- shares parents with no other layers
		- shares children with layers: matmul_82_547, matmul_86_571, matmul_88_583, matmul_90_595, matmul_92_607, matmul_94_619, matmul_96_631
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.1:1
	Time elapsed:  8.345E-05s
	Output of modules: blocks.5.sa.heads.1
	Lookup keys: -92, 606, blocks.5.sa.heads.1, blocks.5.sa.heads.1:1, matmul_84, matmul_84:1, matmul_84_559, matmul_84_559:1
--------------------------------------------
Layer linear_142_560, operation 565/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-2.7943, -0.7020, -0.8080,  3.7599,  4.2635,  1.7618, -1.9683,  1.2246],
        [ 0.1208,  1.4792, -2.2105,  1.3838,  1.7902,  2.5066, -3.0251,  2.9686],
        [-2.6421, -0.9560, -0.5903,  2.7752,  3.4850,  1.4341, -0.4973,  0.4724],
        [-2.6113, -1.0799, -0.6267,  3.7846,  4.2268,  1.2044, -0.0562, -0.0163],
        [-3.5139, -2.3597,  0.0663,  3.2381,  3.7215,  0.6061, -0.8337,  0.1665],
        [ 2.8800,  4.6963, -4.3529, -0.4129, -0.3176,  3.2712, -3.3721,  4.1480],
        [ 3.3941,  4.0845, -2.2132, -0.6927, -1.1024,  2.0953, -2.2868,  2.5702],
        [-2.3852, -0.6384, -1.2075,  1.4760,  2.7230,  1.4979, -1.6702,  1.3069]])...
	Related Layers:
		- parent layers: layernorm_11_535
		- child layers: transpose_43_562
		- shares parents with layers: linear_136_536, linear_137_537, linear_138_546, linear_139_548, linear_140_549, linear_141_558, linear_143_561, linear_144_570, linear_145_572, linear_146_573, linear_147_582, linear_148_584, linear_149_585, linear_150_594, linear_151_596, linear_152_597, linear_153_606, linear_154_608, linear_155_609, linear_156_618, linear_157_620, linear_158_621, linear_159_630
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.2.key:1
	Time elapsed:  1.166E-04s
	Output of modules: blocks.5.sa.heads.2.key
	Output of bottom-level module: blocks.5.sa.heads.2.key:1
	Lookup keys: -91, 607, blocks.5.sa.heads.2.key, blocks.5.sa.heads.2.key:1, linear_142, linear_142:1, linear_142_560, linear_142_560:1
--------------------------------------------
Layer linear_143_561, operation 566/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 4.5069,  2.4299, -1.9692, -3.8043, -2.1441,  0.9097, -2.1782,  1.9771],
        [ 2.8234,  3.1554, -1.8520, -1.1058,  0.3636,  2.3289, -2.5446,  2.1776],
        [ 4.5144,  3.6454, -2.5268, -2.2101, -1.8375,  1.9300, -2.5061,  1.7223],
        [ 4.7447,  3.7133, -2.3987, -2.1361, -1.7702,  1.8348, -2.5848,  1.5820],
        [ 5.6862,  2.8717, -2.2956, -4.1564, -3.1414,  0.9171, -2.6799,  1.8191],
        [-1.9097,  0.0377,  0.1308,  2.3530,  2.2084,  1.3368,  1.6822, -2.1182],
        [ 0.3914,  1.9276, -1.6349,  1.4709,  1.0678,  1.7339,  0.1357,  0.9192],
        [ 2.8359,  2.5682, -3.2732, -3.0452, -1.4342,  2.1640, -3.7999,  3.1591]])...
	Related Layers:
		- parent layers: layernorm_11_535
		- child layers: matmul_85_563
		- shares parents with layers: linear_136_536, linear_137_537, linear_138_546, linear_139_548, linear_140_549, linear_141_558, linear_142_560, linear_144_570, linear_145_572, linear_146_573, linear_147_582, linear_148_584, linear_149_585, linear_150_594, linear_151_596, linear_152_597, linear_153_606, linear_154_608, linear_155_609, linear_156_618, linear_157_620, linear_158_621, linear_159_630
		- shares children with layers: transpose_43_562
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.2.query:1
	Time elapsed:  1.147E-04s
	Output of modules: blocks.5.sa.heads.2.query
	Output of bottom-level module: blocks.5.sa.heads.2.query:1
	Lookup keys: -90, 608, blocks.5.sa.heads.2.query, blocks.5.sa.heads.2.query:1, linear_143, linear_143:1, linear_143_561, linear_143_561:1
--------------------------------------------
Layer transpose_43_562, operation 567/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-2.7943,  0.1208, -2.6421, -2.6113, -3.5139,  2.8800,  3.3941, -2.3852],
        [-0.7020,  1.4792, -0.9560, -1.0799, -2.3597,  4.6963,  4.0845, -0.6384],
        [-0.8080, -2.2105, -0.5903, -0.6267,  0.0663, -4.3529, -2.2132, -1.2075],
        [ 3.7599,  1.3838,  2.7752,  3.7846,  3.2381, -0.4129, -0.6927,  1.4760],
        [ 4.2635,  1.7902,  3.4850,  4.2268,  3.7215, -0.3176, -1.1024,  2.7230],
        [ 1.7618,  2.5066,  1.4341,  1.2044,  0.6061,  3.2712,  2.0953,  1.4979],
        [-1.9683, -3.0251, -0.4973, -0.0562, -0.8337, -3.3721, -2.2868, -1.6702],
        [ 1.2246,  2.9686,  0.4724, -0.0163,  0.1665,  4.1480,  2.5702,  1.3069]])...
	Related Layers:
		- parent layers: linear_142_560
		- child layers: matmul_85_563
		- shares parents with no other layers
		- shares children with layers: linear_143_561
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.5.sa.heads.2:1
	Time elapsed:  5.960E-05s
	Output of modules: none
	Lookup keys: -89, 609, transpose_43, transpose_43:1, transpose_43_562, transpose_43_562:1
--------------------------------------------
Layer matmul_85_563, operation 568/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ 6.3205e+00,  1.2790e+02, -2.3133e+01, -4.3522e+01, -7.7475e+01,
          2.7287e+02,  2.1319e+02,  2.6442e+01],
        [ 6.8888e+01,  1.9444e+02,  1.9782e+01,  3.0760e+00, -4.5995e+01,
          3.8267e+02,  2.8679e+02,  8.2520e+01],
        [ 3.5104e+01,  1.7601e+02, -8.8150e+00, -2.9193e+01, -7.2183e+01,
          3.5809e+02,  2.7416e+02,  5.1984e+01],
        [ 3.2635e+01,  1.8016e+02, -1.0823e+01, -3.2120e+01, -8.0600e+01,
          3.7339e+02,  2.8921e+02,  4.9143e+01],
        [ 3.3020e-01,  1.5362e+02, -3.3227e+01, -5.6401e+01, -9.8176e+01,
          3.3154e+02,  2.6381e+02,  2.5143e+01],
        [ 1.7341e+01, -3.1351e+01,  3.0226e+01,  4.0248e+01,  3.6852e+01,
         -7.0167e+01, -5.6899e+01,  1.2578e+01],
        [ 5.3834e+01,  1.0036e+02,  2.8567e+01,  2.5175e+01, -1.0382e+01,
          2.0496e+02,  1.5327e+02,  5.6791e+01],
        [ 7.1773e+01,  2.1726e+02,  1.1742e+01, -1.2858e+01, -5.0928e+01,
          4.1246e+02,  3.0130e+02,  8.8386e+01]])...
	Related Layers:
		- parent layers: linear_143_561, transpose_43_562
		- child layers: mul_43_564
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.2:1
	Time elapsed:  8.678E-05s
	Output of modules: none
	Lookup keys: -88, 610, matmul_85, matmul_85:1, matmul_85_563, matmul_85_563:1
--------------------------------------------
Layer mul_43_564, operation 569/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ 3.2254e-01,  6.5268e+00, -1.1805e+00, -2.2210e+00, -3.9536e+00,
          1.3925e+01,  1.0879e+01,  1.3494e+00],
        [ 3.5154e+00,  9.9224e+00,  1.0095e+00,  1.5697e-01, -2.3472e+00,
          1.9528e+01,  1.4635e+01,  4.2111e+00],
        [ 1.7914e+00,  8.9821e+00, -4.4984e-01, -1.4897e+00, -3.6836e+00,
          1.8274e+01,  1.3991e+01,  2.6528e+00],
        [ 1.6654e+00,  9.1935e+00, -5.5231e-01, -1.6391e+00, -4.1131e+00,
          1.9055e+01,  1.4758e+01,  2.5078e+00],
        [ 1.6850e-02,  7.8395e+00, -1.6956e+00, -2.8782e+00, -5.0100e+00,
          1.6919e+01,  1.3463e+01,  1.2831e+00],
        [ 8.8491e-01, -1.5999e+00,  1.5424e+00,  2.0539e+00,  1.8806e+00,
         -3.5807e+00, -2.9036e+00,  6.4184e-01],
        [ 2.7472e+00,  5.1216e+00,  1.4578e+00,  1.2847e+00, -5.2981e-01,
          1.0460e+01,  7.8213e+00,  2.8981e+00],
        [ 3.6627e+00,  1.1087e+01,  5.9921e-01, -6.5615e-01, -2.5989e+00,
          2.1048e+01,  1.5375e+01,  4.5105e+00]])...
	Related Layers:
		- parent layers: matmul_85_563
		- child layers: maskedfill_43_567
		- shares parents with no other layers
		- shares children with layers: eq_43_566
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.5.sa.heads.2:1
	Time elapsed:  6.914E-05s
	Output of modules: none
	Lookup keys: -87, 611, mul_43, mul_43:1, mul_43_564, mul_43_564:1
--------------------------------------------
Layer getitem_43_565, operation 570/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_43
		- child layers: eq_43_566
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.988E-05s
	Output of modules: none
	Lookup keys: -85, 613, getitem_43, getitem_43:1, getitem_43_565, getitem_43_565:1
--------------------------------------------
Layer eq_43_566, operation 571/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_43_565
		- child layers: maskedfill_43_567
		- shares parents with no other layers
		- shares children with layers: mul_43_564
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  7.772E-05s
	Output of modules: none
	Lookup keys: -84, 614, eq_43, eq_43:1, eq_43_566, eq_43_566:1
--------------------------------------------
Layer maskedfill_43_567, operation 572/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ 3.2254e-01,        -inf,        -inf,        -inf,        -inf,
                -inf,        -inf,        -inf],
        [ 3.5154e+00,  9.9224e+00,        -inf,        -inf,        -inf,
                -inf,        -inf,        -inf],
        [ 1.7914e+00,  8.9821e+00, -4.4984e-01,        -inf,        -inf,
                -inf,        -inf,        -inf],
        [ 1.6654e+00,  9.1935e+00, -5.5231e-01, -1.6391e+00,        -inf,
                -inf,        -inf,        -inf],
        [ 1.6850e-02,  7.8395e+00, -1.6956e+00, -2.8782e+00, -5.0100e+00,
                -inf,        -inf,        -inf],
        [ 8.8491e-01, -1.5999e+00,  1.5424e+00,  2.0539e+00,  1.8806e+00,
         -3.5807e+00,        -inf,        -inf],
        [ 2.7472e+00,  5.1216e+00,  1.4578e+00,  1.2847e+00, -5.2981e-01,
          1.0460e+01,  7.8213e+00,        -inf],
        [ 3.6627e+00,  1.1087e+01,  5.9921e-01, -6.5615e-01, -2.5989e+00,
          2.1048e+01,  1.5375e+01,  4.5105e+00]])...
	Related Layers:
		- parent layers: mul_43_564, eq_43_566
		- child layers: softmax_43_568
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.5.sa.heads.2:1
	Time elapsed:  8.297E-05s
	Output of modules: none
	Lookup keys: -83, 615, maskedfill_43, maskedfill_43:1, maskedfill_43_567, maskedfill_43_567:1
--------------------------------------------
Layer softmax_43_568, operation 573/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.6474e-03, 9.9835e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [7.5296e-04, 9.9917e-01, 8.0059e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [5.3742e-04, 9.9938e-01, 5.8503e-05, 1.9733e-05, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [4.0038e-04, 9.9950e-01, 7.2237e-05, 2.2139e-05, 2.6262e-06, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.1173e-01, 9.3121e-03, 2.1564e-01, 3.5963e-01, 3.0241e-01, 1.2847e-03,
         0.0000e+00, 0.0000e+00],
        [4.1532e-04, 4.4624e-03, 1.1439e-04, 9.6212e-05, 1.5674e-05, 9.2852e-01,
         6.6380e-02, 0.0000e+00],
        [2.8051e-08, 4.7023e-05, 1.3107e-09, 3.7352e-10, 5.3527e-11, 9.9653e-01,
         3.4256e-03, 6.5484e-08]])...
	Related Layers:
		- parent layers: maskedfill_43_567
		- child layers: dropout_53_569
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.5.sa.heads.2:1
	Time elapsed:  6.819E-05s
	Output of modules: none
	Lookup keys: -82, 616, softmax_43, softmax_43:1, softmax_43_568, softmax_43_568:1
--------------------------------------------
Layer dropout_53_569, operation 574/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.6474e-03, 9.9835e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [7.5296e-04, 9.9917e-01, 8.0059e-05, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [5.3742e-04, 9.9938e-01, 5.8503e-05, 1.9733e-05, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [4.0038e-04, 9.9950e-01, 7.2237e-05, 2.2139e-05, 2.6262e-06, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.1173e-01, 9.3121e-03, 2.1564e-01, 3.5963e-01, 3.0241e-01, 1.2847e-03,
         0.0000e+00, 0.0000e+00],
        [4.1532e-04, 4.4624e-03, 1.1439e-04, 9.6212e-05, 1.5674e-05, 9.2852e-01,
         6.6380e-02, 0.0000e+00],
        [2.8051e-08, 4.7023e-05, 1.3107e-09, 3.7352e-10, 5.3527e-11, 9.9653e-01,
         3.4256e-03, 6.5484e-08]])...
	Related Layers:
		- parent layers: softmax_43_568
		- child layers: matmul_86_571
		- shares parents with no other layers
		- shares children with layers: linear_144_570
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.5.sa.heads.2.dropout:1
	Time elapsed:  5.317E-05s
	Output of modules: blocks.5.sa.heads.2.dropout
	Output of bottom-level module: blocks.5.sa.heads.2.dropout:1
	Lookup keys: -81, 617, blocks.5.sa.heads.2.dropout, blocks.5.sa.heads.2.dropout:1, dropout_53, dropout_53:1, dropout_53_569, dropout_53_569:1
--------------------------------------------
Layer linear_144_570, operation 575/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.1917, -0.4569,  0.1518, -0.0461, -0.6656,  0.5178,  1.5608, -0.6272],
        [-0.1537, -0.9024, -0.1377, -0.0732, -1.2308,  1.1195,  1.1826, -0.2270],
        [ 0.1717, -0.4429, -0.4004, -0.7255, -0.0171,  1.0001,  0.4463,  0.0022],
        [ 0.2696, -0.4624, -0.6147, -0.3967, -0.0279,  0.5850,  0.8361, -0.2013],
        [ 0.2606, -0.8895,  0.1552, -0.3392,  0.1346,  0.5101,  1.3747,  0.0819],
        [-1.0163, -0.0038, -0.5331,  0.2434, -1.2533, -0.2099,  1.3454, -0.4245],
        [-0.9718, -0.4080, -0.8116, -0.0637, -1.5529,  0.1907,  0.8781, -0.4621],
        [-0.1657, -0.3445, -0.2316, -0.9300, -0.9850,  0.6431,  0.8746,  0.3898]])...
	Related Layers:
		- parent layers: layernorm_11_535
		- child layers: matmul_86_571
		- shares parents with layers: linear_136_536, linear_137_537, linear_138_546, linear_139_548, linear_140_549, linear_141_558, linear_142_560, linear_143_561, linear_145_572, linear_146_573, linear_147_582, linear_148_584, linear_149_585, linear_150_594, linear_151_596, linear_152_597, linear_153_606, linear_154_608, linear_155_609, linear_156_618, linear_157_620, linear_158_621, linear_159_630
		- shares children with layers: dropout_53_569
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.2.value:1
	Time elapsed:  1.175E-04s
	Output of modules: blocks.5.sa.heads.2.value
	Output of bottom-level module: blocks.5.sa.heads.2.value:1
	Lookup keys: -80, 618, blocks.5.sa.heads.2.value, blocks.5.sa.heads.2.value:1, linear_144, linear_144:1, linear_144_570, linear_144_570:1
--------------------------------------------
Layer matmul_86_571, operation 576/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.1917, -0.4569,  0.1518, -0.0461, -0.6656,  0.5178,  1.5608, -0.6272],
        [-0.1531, -0.9016, -0.1372, -0.0732, -1.2298,  1.1185,  1.1833, -0.2277],
        [-0.1534, -0.9020, -0.1375, -0.0732, -1.2302,  1.1190,  1.1829, -0.2273],
        [-0.1535, -0.9021, -0.1375, -0.0732, -1.2304,  1.1191,  1.1828, -0.2272],
        [-0.1535, -0.9022, -0.1376, -0.0733, -1.2304,  1.1192,  1.1827, -0.2272],
        [ 0.2314, -0.5902, -0.2455, -0.4072, -0.0605,  0.6483,  0.9998, -0.1199],
        [-1.0087, -0.0350, -0.5496,  0.2213, -1.2726, -0.1768,  1.3135, -0.4261],
        [-1.0161, -0.0053, -0.5341,  0.2424, -1.2543, -0.2084,  1.3437, -0.4246]])...
	Related Layers:
		- parent layers: dropout_53_569, linear_144_570
		- child layers: cat_6_632
		- shares parents with no other layers
		- shares children with layers: matmul_82_547, matmul_84_559, matmul_88_583, matmul_90_595, matmul_92_607, matmul_94_619, matmul_96_631
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.2:1
	Time elapsed:  8.464E-05s
	Output of modules: blocks.5.sa.heads.2
	Lookup keys: -79, 619, blocks.5.sa.heads.2, blocks.5.sa.heads.2:1, matmul_86, matmul_86:1, matmul_86_571, matmul_86_571:1
--------------------------------------------
Layer linear_145_572, operation 577/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-2.6902,  2.9234,  3.8880,  4.2217,  3.1135,  2.3080,  2.7356,  4.2171],
        [-0.5379,  2.1134,  3.5702,  2.9328,  2.0054,  1.0556,  1.3985,  1.2906],
        [-2.0055,  1.6479,  3.0980,  3.4309,  2.4843,  2.0243,  2.3757,  3.3364],
        [-1.7684,  1.9957,  3.3717,  3.3849,  2.4617,  2.1602,  2.3010,  3.3411],
        [-2.0260,  2.2722,  3.8223,  3.8494,  2.8227,  1.7320,  2.5213,  4.1331],
        [-0.6610, -0.3767,  0.7118,  0.0329, -0.4848, -0.1337,  1.2856, -0.7829],
        [ 0.3237,  0.6332,  2.1361,  1.6917, -0.1537,  0.7361,  1.5466, -0.0787],
        [-1.6371,  1.3850,  3.3170,  3.8668,  2.6179,  2.7508,  3.0408,  2.8313]])...
	Related Layers:
		- parent layers: layernorm_11_535
		- child layers: transpose_44_574
		- shares parents with layers: linear_136_536, linear_137_537, linear_138_546, linear_139_548, linear_140_549, linear_141_558, linear_142_560, linear_143_561, linear_144_570, linear_146_573, linear_147_582, linear_148_584, linear_149_585, linear_150_594, linear_151_596, linear_152_597, linear_153_606, linear_154_608, linear_155_609, linear_156_618, linear_157_620, linear_158_621, linear_159_630
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.3.key:1
	Time elapsed:  1.135E-04s
	Output of modules: blocks.5.sa.heads.3.key
	Output of bottom-level module: blocks.5.sa.heads.3.key:1
	Lookup keys: -78, 620, blocks.5.sa.heads.3.key, blocks.5.sa.heads.3.key:1, linear_145, linear_145:1, linear_145_572, linear_145_572:1
--------------------------------------------
Layer linear_146_573, operation 578/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-4.3125e-01,  1.9638e-01, -7.2115e-01,  2.8538e-01,  3.4341e-01,
         -1.4765e+00, -1.1994e+00,  1.5973e+00],
        [-1.1162e+00,  1.3648e+00,  1.6946e+00,  2.8784e+00,  1.9383e+00,
          7.3721e-01,  4.8948e-01,  2.5629e+00],
        [ 2.8864e-01,  1.2511e+00,  3.0159e-01,  2.1497e+00,  1.1768e+00,
         -4.8173e-02, -5.7566e-01,  1.9627e+00],
        [ 4.3678e-01,  9.8879e-01,  4.5609e-01,  2.0852e+00,  7.9391e-01,
         -2.8704e-03, -6.3113e-01,  1.5738e+00],
        [-5.0058e-01,  3.5317e-01, -5.9851e-01,  1.1471e+00,  9.8516e-01,
         -2.2517e-01, -4.2454e-01,  1.7259e+00],
        [ 6.4588e-01,  1.0242e+00,  9.0098e-01,  1.3667e+00, -8.4357e-01,
          7.0903e-02, -8.0409e-01, -1.5000e+00],
        [-9.8892e-01,  1.7987e+00,  2.3163e+00,  2.8284e+00,  1.0746e+00,
          1.0979e+00,  8.1626e-01,  1.0129e+00],
        [-1.0807e+00,  2.2879e+00,  1.6151e+00,  3.3626e+00,  2.1546e+00,
          7.6297e-01,  3.5701e-01,  3.6372e+00]])...
	Related Layers:
		- parent layers: layernorm_11_535
		- child layers: matmul_87_575
		- shares parents with layers: linear_136_536, linear_137_537, linear_138_546, linear_139_548, linear_140_549, linear_141_558, linear_142_560, linear_143_561, linear_144_570, linear_145_572, linear_147_582, linear_148_584, linear_149_585, linear_150_594, linear_151_596, linear_152_597, linear_153_606, linear_154_608, linear_155_609, linear_156_618, linear_157_620, linear_158_621, linear_159_630
		- shares children with layers: transpose_44_574
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.3.query:1
	Time elapsed:  1.106E-04s
	Output of modules: blocks.5.sa.heads.3.query
	Output of bottom-level module: blocks.5.sa.heads.3.query:1
	Lookup keys: -77, 621, blocks.5.sa.heads.3.query, blocks.5.sa.heads.3.query:1, linear_146, linear_146:1, linear_146_573, linear_146_573:1
--------------------------------------------
Layer transpose_44_574, operation 579/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-2.6902, -0.5379, -2.0055, -1.7684, -2.0260, -0.6610,  0.3237, -1.6371],
        [ 2.9234,  2.1134,  1.6479,  1.9957,  2.2722, -0.3767,  0.6332,  1.3850],
        [ 3.8880,  3.5702,  3.0980,  3.3717,  3.8223,  0.7118,  2.1361,  3.3170],
        [ 4.2217,  2.9328,  3.4309,  3.3849,  3.8494,  0.0329,  1.6917,  3.8668],
        [ 3.1135,  2.0054,  2.4843,  2.4617,  2.8227, -0.4848, -0.1537,  2.6179],
        [ 2.3080,  1.0556,  2.0243,  2.1602,  1.7320, -0.1337,  0.7361,  2.7508],
        [ 2.7356,  1.3985,  2.3757,  2.3010,  2.5213,  1.2856,  1.5466,  3.0408],
        [ 4.2171,  1.2906,  3.3364,  3.3411,  4.1331, -0.7829, -0.0787,  2.8313]])...
	Related Layers:
		- parent layers: linear_145_572
		- child layers: matmul_87_575
		- shares parents with no other layers
		- shares children with layers: linear_146_573
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.5.sa.heads.3:1
	Time elapsed:  5.841E-05s
	Output of modules: none
	Lookup keys: -76, 622, transpose_44, transpose_44:1, transpose_44_574, transpose_44_574:1
--------------------------------------------
Layer matmul_87_575, operation 580/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-57.6579, -28.2858, -48.0773, -52.6839, -55.9806,  -8.4205,  -4.8977,
         -37.3094],
        [130.3761,  91.6372, 108.5063, 109.3201, 115.7329, -14.8789,  46.6524,
         116.8319],
        [  6.2436,  26.1310,   4.6012,   3.4443,  -2.4212,  -8.7682,  30.9785,
          18.9102],
        [ -5.6952,  20.1682,  -4.7582,  -5.4889, -13.8684,  -4.3463,  32.4998,
           9.8639],
        [-41.8563, -13.0082, -36.9111, -38.5382, -47.5942,  -0.4217,  14.6990,
         -23.8358],
        [ 22.4991,  51.9255,  29.7641,  23.7706,  23.4498, -16.7816,  16.4072,
          21.4181],
        [191.8259, 139.1116, 165.8511, 165.7819, 178.1834, -24.9664,  48.1178,
         157.5185],
        [187.2960, 126.8896, 158.6226, 160.5639, 168.9351, -24.0036,  56.9625,
         157.8334]])...
	Related Layers:
		- parent layers: linear_146_573, transpose_44_574
		- child layers: mul_44_576
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.3:1
	Time elapsed:  8.702E-05s
	Output of modules: none
	Lookup keys: -75, 623, matmul_87, matmul_87:1, matmul_87_575, matmul_87_575:1
--------------------------------------------
Layer mul_44_576, operation 581/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-2.9423, -1.4435, -2.4534, -2.6885, -2.8567, -0.4297, -0.2499, -1.9039],
        [ 6.6532,  4.6763,  5.5372,  5.5787,  5.9060, -0.7593,  2.3807,  5.9621],
        [ 0.3186,  1.3335,  0.2348,  0.1758, -0.1236, -0.4474,  1.5809,  0.9650],
        [-0.2906,  1.0292, -0.2428, -0.2801, -0.7077, -0.2218,  1.6585,  0.5034],
        [-2.1360, -0.6638, -1.8836, -1.9666, -2.4288, -0.0215,  0.7501, -1.2164],
        [ 1.1482,  2.6498,  1.5189,  1.2130,  1.1967, -0.8564,  0.8373,  1.0930],
        [ 9.7891,  7.0990,  8.4636,  8.4600,  9.0929, -1.2741,  2.4555,  8.0383],
        [ 9.5579,  6.4753,  8.0947,  8.1937,  8.6209, -1.2249,  2.9069,  8.0544]])...
	Related Layers:
		- parent layers: matmul_87_575
		- child layers: maskedfill_44_579
		- shares parents with no other layers
		- shares children with layers: eq_44_578
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.5.sa.heads.3:1
	Time elapsed:  7.224E-05s
	Output of modules: none
	Lookup keys: -74, 624, mul_44, mul_44:1, mul_44_576, mul_44_576:1
--------------------------------------------
Layer getitem_44_577, operation 582/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_44
		- child layers: eq_44_578
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.489E-05s
	Output of modules: none
	Lookup keys: -72, 626, getitem_44, getitem_44:1, getitem_44_577, getitem_44_577:1
--------------------------------------------
Layer eq_44_578, operation 583/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_44_577
		- child layers: maskedfill_44_579
		- shares parents with no other layers
		- shares children with layers: mul_44_576
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  7.725E-05s
	Output of modules: none
	Lookup keys: -71, 627, eq_44, eq_44:1, eq_44_578, eq_44_578:1
--------------------------------------------
Layer maskedfill_44_579, operation 584/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-2.9423,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [ 6.6532,  4.6763,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [ 0.3186,  1.3335,  0.2348,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-0.2906,  1.0292, -0.2428, -0.2801,    -inf,    -inf,    -inf,    -inf],
        [-2.1360, -0.6638, -1.8836, -1.9666, -2.4288,    -inf,    -inf,    -inf],
        [ 1.1482,  2.6498,  1.5189,  1.2130,  1.1967, -0.8564,    -inf,    -inf],
        [ 9.7891,  7.0990,  8.4636,  8.4600,  9.0929, -1.2741,  2.4555,    -inf],
        [ 9.5579,  6.4753,  8.0947,  8.1937,  8.6209, -1.2249,  2.9069,  8.0544]])...
	Related Layers:
		- parent layers: mul_44_576, eq_44_578
		- child layers: softmax_44_580
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.5.sa.heads.3:1
	Time elapsed:  9.060E-05s
	Output of modules: none
	Lookup keys: -70, 628, maskedfill_44, maskedfill_44:1, maskedfill_44_579, maskedfill_44_579:1
--------------------------------------------
Layer softmax_44_580, operation 585/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [8.7835e-01, 1.2165e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.1374e-01, 5.8971e-01, 1.9655e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.4701e-01, 5.5022e-01, 1.5421e-01, 1.4856e-01, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.1660e-01, 5.0821e-01, 1.5007e-01, 1.3811e-01, 8.7003e-02, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.0882e-01, 4.8851e-01, 1.5766e-01, 1.1612e-01, 1.1423e-01, 1.4661e-02,
         0.0000e+00, 0.0000e+00],
        [4.7678e-01, 3.2362e-02, 1.2666e-01, 1.2622e-01, 2.3766e-01, 7.4758e-06,
         3.1145e-04, 0.0000e+00],
        [4.6546e-01, 2.1337e-02, 1.0775e-01, 1.1897e-01, 1.8237e-01, 9.6596e-06,
         6.0169e-04, 1.0350e-01]])...
	Related Layers:
		- parent layers: maskedfill_44_579
		- child layers: dropout_54_581
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.5.sa.heads.3:1
	Time elapsed:  6.938E-05s
	Output of modules: none
	Lookup keys: -69, 629, softmax_44, softmax_44:1, softmax_44_580, softmax_44_580:1
--------------------------------------------
Layer dropout_54_581, operation 586/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [8.7835e-01, 1.2165e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [2.1374e-01, 5.8971e-01, 1.9655e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.4701e-01, 5.5022e-01, 1.5421e-01, 1.4856e-01, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.1660e-01, 5.0821e-01, 1.5007e-01, 1.3811e-01, 8.7003e-02, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [1.0882e-01, 4.8851e-01, 1.5766e-01, 1.1612e-01, 1.1423e-01, 1.4661e-02,
         0.0000e+00, 0.0000e+00],
        [4.7678e-01, 3.2362e-02, 1.2666e-01, 1.2622e-01, 2.3766e-01, 7.4758e-06,
         3.1145e-04, 0.0000e+00],
        [4.6546e-01, 2.1337e-02, 1.0775e-01, 1.1897e-01, 1.8237e-01, 9.6596e-06,
         6.0169e-04, 1.0350e-01]])...
	Related Layers:
		- parent layers: softmax_44_580
		- child layers: matmul_88_583
		- shares parents with no other layers
		- shares children with layers: linear_147_582
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.5.sa.heads.3.dropout:1
	Time elapsed:  5.507E-05s
	Output of modules: blocks.5.sa.heads.3.dropout
	Output of bottom-level module: blocks.5.sa.heads.3.dropout:1
	Lookup keys: -68, 630, blocks.5.sa.heads.3.dropout, blocks.5.sa.heads.3.dropout:1, dropout_54, dropout_54:1, dropout_54_581, dropout_54_581:1
--------------------------------------------
Layer linear_147_582, operation 587/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.6996, -0.3887,  0.5705, -0.9403,  0.0355, -0.5421,  0.7062, -0.7294],
        [-0.0491, -0.4337,  1.1692, -0.5016, -1.3413, -0.5327,  0.0063, -0.5237],
        [ 0.0324, -0.3038,  0.5764, -0.9500, -0.1904, -0.0178,  0.5511, -0.8029],
        [ 0.2452, -0.0730,  0.2598, -1.1302, -0.3056, -0.1397,  0.5404, -0.7607],
        [-0.0712, -0.6682,  0.3967, -0.7297,  0.8949,  0.4114,  1.1365, -1.1324],
        [-3.8045,  0.8926,  0.9574, -0.4774, -1.4781, -1.2463, -1.6469,  0.6147],
        [-1.2806,  0.8656,  1.0190, -0.3585, -1.9472, -1.0132, -1.1002, -0.0896],
        [-0.4061, -0.3712,  0.8216,  0.1220, -0.6184, -0.3098,  0.1256, -0.3630]])...
	Related Layers:
		- parent layers: layernorm_11_535
		- child layers: matmul_88_583
		- shares parents with layers: linear_136_536, linear_137_537, linear_138_546, linear_139_548, linear_140_549, linear_141_558, linear_142_560, linear_143_561, linear_144_570, linear_145_572, linear_146_573, linear_148_584, linear_149_585, linear_150_594, linear_151_596, linear_152_597, linear_153_606, linear_154_608, linear_155_609, linear_156_618, linear_157_620, linear_158_621, linear_159_630
		- shares children with layers: dropout_54_581
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.3.value:1
	Time elapsed:  1.197E-04s
	Output of modules: blocks.5.sa.heads.3.value
	Output of bottom-level module: blocks.5.sa.heads.3.value:1
	Lookup keys: -67, 631, blocks.5.sa.heads.3.value, blocks.5.sa.heads.3.value:1, linear_147, linear_147:1, linear_147_582, linear_147_582:1
--------------------------------------------
Layer matmul_88_583, operation 588/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-0.6996, -0.3887,  0.5705, -0.9403,  0.0355, -0.5421,  0.7062, -0.7294],
        [-0.6205, -0.3942,  0.6433, -0.8870, -0.1320, -0.5410,  0.6211, -0.7043],
        [-0.1721, -0.3986,  0.9247, -0.6835, -0.8208, -0.4335,  0.2630, -0.6226],
        [-0.0885, -0.3535,  0.8547, -0.7286, -0.8076, -0.3963,  0.2725, -0.6322],
        [-0.0740, -0.3796,  0.8176, -0.7267, -0.6705, -0.3201,  0.3417, -0.6753],
        [-0.1305, -0.3738,  0.8137, -0.7187, -0.6363, -0.3095,  0.3352, -0.6705],
        [-0.3174, -0.4056,  0.5102, -0.9011,  0.1229, -0.1981,  0.7447, -0.8316],
        [-0.3499, -0.3914,  0.5415, -0.8059,  0.0291, -0.2399,  0.6721, -0.7718]])...
	Related Layers:
		- parent layers: dropout_54_581, linear_147_582
		- child layers: cat_6_632
		- shares parents with no other layers
		- shares children with layers: matmul_82_547, matmul_84_559, matmul_86_571, matmul_90_595, matmul_92_607, matmul_94_619, matmul_96_631
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.3:1
	Time elapsed:  8.440E-05s
	Output of modules: blocks.5.sa.heads.3
	Lookup keys: -66, 632, blocks.5.sa.heads.3, blocks.5.sa.heads.3:1, matmul_88, matmul_88:1, matmul_88_583, matmul_88_583:1
--------------------------------------------
Layer linear_148_584, operation 589/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-2.2852, -2.7047,  2.2598,  0.5786,  1.3630, -1.5517,  0.4006,  0.9222],
        [ 0.5766, -1.3503,  0.9274, -0.3326,  0.6146, -0.3163, -1.7137, -2.2099],
        [-0.8800, -3.0262,  2.5092,  0.4691,  1.1714, -0.5040, -0.5580, -0.3549],
        [-1.0056, -2.8415,  2.4021,  0.5464,  1.0681, -0.3152, -0.4478, -0.0485],
        [-2.2600, -2.6509,  3.2834,  0.8593,  1.6345, -1.4827, -0.3784,  0.4949],
        [ 2.3198, -0.3231, -2.6769, -0.9808, -1.3424,  1.1711, -1.8802, -3.1219],
        [ 2.2603, -0.9758, -1.8813, -0.6623,  0.2831,  0.5185, -1.7888, -3.6383],
        [-1.4645, -1.9488,  1.9193,  1.0218,  1.6699, -1.3029, -1.1155,  0.1233]])...
	Related Layers:
		- parent layers: layernorm_11_535
		- child layers: transpose_45_586
		- shares parents with layers: linear_136_536, linear_137_537, linear_138_546, linear_139_548, linear_140_549, linear_141_558, linear_142_560, linear_143_561, linear_144_570, linear_145_572, linear_146_573, linear_147_582, linear_149_585, linear_150_594, linear_151_596, linear_152_597, linear_153_606, linear_154_608, linear_155_609, linear_156_618, linear_157_620, linear_158_621, linear_159_630
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.4.key:1
	Time elapsed:  1.121E-04s
	Output of modules: blocks.5.sa.heads.4.key
	Output of bottom-level module: blocks.5.sa.heads.4.key:1
	Lookup keys: -65, 633, blocks.5.sa.heads.4.key, blocks.5.sa.heads.4.key:1, linear_148, linear_148:1, linear_148_584, linear_148_584:1
--------------------------------------------
Layer linear_149_585, operation 590/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.2002,  1.2547, -0.5543, -0.8808, -0.4852,  1.0027, -0.2045, -1.5575],
        [ 0.9927,  1.3872, -1.1410, -0.5342, -1.1553,  0.7214, -0.6375, -0.7793],
        [ 0.7131,  0.4467, -1.7846, -0.7539, -0.0572,  1.1837, -0.5120, -1.7112],
        [ 0.6640,  0.5133, -1.7129, -0.9710, -0.0860,  1.1433, -0.5807, -1.6646],
        [ 1.9339,  0.6215, -1.8901, -2.1243, -0.6379,  1.0767, -0.9488, -2.9638],
        [-2.7346,  1.9063,  1.1111,  2.1194, -0.8860, -0.8230,  2.2403,  3.3409],
        [-1.1289,  1.5601,  0.5176,  1.9924, -0.8147, -0.9949,  1.4924,  1.9573],
        [ 1.1328,  1.0878, -0.6263, -0.7785, -0.8093,  0.5516,  0.2268, -1.2115]])...
	Related Layers:
		- parent layers: layernorm_11_535
		- child layers: matmul_89_587
		- shares parents with layers: linear_136_536, linear_137_537, linear_138_546, linear_139_548, linear_140_549, linear_141_558, linear_142_560, linear_143_561, linear_144_570, linear_145_572, linear_146_573, linear_147_582, linear_148_584, linear_150_594, linear_151_596, linear_152_597, linear_153_606, linear_154_608, linear_155_609, linear_156_618, linear_157_620, linear_158_621, linear_159_630
		- shares children with layers: transpose_45_586
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.4.query:1
	Time elapsed:  1.173E-04s
	Output of modules: blocks.5.sa.heads.4.query
	Output of bottom-level module: blocks.5.sa.heads.4.query:1
	Lookup keys: -64, 634, blocks.5.sa.heads.4.query, blocks.5.sa.heads.4.query:1, linear_149, linear_149:1, linear_149_585, linear_149_585:1
--------------------------------------------
Layer transpose_45_586, operation 591/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-2.2852,  0.5766, -0.8800, -1.0056, -2.2600,  2.3198,  2.2603, -1.4645],
        [-2.7047, -1.3503, -3.0262, -2.8415, -2.6509, -0.3231, -0.9758, -1.9488],
        [ 2.2598,  0.9274,  2.5092,  2.4021,  3.2834, -2.6769, -1.8813,  1.9193],
        [ 0.5786, -0.3326,  0.4691,  0.5464,  0.8593, -0.9808, -0.6623,  1.0218],
        [ 1.3630,  0.6146,  1.1714,  1.0681,  1.6345, -1.3424,  0.2831,  1.6699],
        [-1.5517, -0.3163, -0.5040, -0.3152, -1.4827,  1.1711,  0.5185, -1.3029],
        [ 0.4006, -1.7137, -0.5580, -0.4478, -0.3784, -1.8802, -1.7888, -1.1155],
        [ 0.9222, -2.2099, -0.3549, -0.0485,  0.4949, -3.1219, -3.6383,  0.1233]])...
	Related Layers:
		- parent layers: linear_148_584
		- child layers: matmul_89_587
		- shares parents with no other layers
		- shares children with layers: linear_149_585
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.5.sa.heads.4:1
	Time elapsed:  6.342E-05s
	Output of modules: none
	Lookup keys: -63, 635, transpose_45, transpose_45:1, transpose_45_586, transpose_45_586:1
--------------------------------------------
Layer matmul_89_587, operation 592/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-120.4589,   26.7151,  -69.1403,  -80.9690, -122.4155,  136.9339,
          113.1091,  -61.5764],
        [-131.6116,    9.8226,  -86.4138,  -97.0175, -133.0259,  120.7488,
           93.5267,  -79.3185],
        [-108.1821,   40.7741,  -59.0068,  -70.8381, -114.6344,  158.1234,
          135.7798,  -45.8421],
        [-110.7044,   38.4846,  -61.0955,  -73.0856, -116.6897,  156.2188,
          133.8989,  -48.3759],
        [-180.6658,   61.8049,  -97.3623, -117.0275, -187.3080,  238.6282,
          212.6547,  -80.2093],
        [  88.6285, -109.2442,   17.6843,   31.5380,   95.4402, -216.8321,
         -238.6530,   -2.5513],
        [  19.0316,  -84.6445,  -18.0309,  -12.2150,   25.6664, -127.4936,
         -155.8006,  -32.3016],
        [-118.2725,   24.1939,  -69.5446,  -80.3092, -116.9913,  110.8177,
          101.8899,  -62.9522]])...
	Related Layers:
		- parent layers: linear_149_585, transpose_45_586
		- child layers: mul_45_588
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.4:1
	Time elapsed:  9.322E-05s
	Output of modules: none
	Lookup keys: -62, 636, matmul_89, matmul_89:1, matmul_89_587, matmul_89_587:1
--------------------------------------------
Layer mul_45_588, operation 593/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -6.1471,   1.3633,  -3.5283,  -4.1319,  -6.2470,   6.9879,   5.7721,
          -3.1423],
        [ -6.7163,   0.5013,  -4.4098,  -4.9509,  -6.7884,   6.1619,   4.7728,
          -4.0477],
        [ -5.5206,   2.0807,  -3.0112,  -3.6149,  -5.8499,   8.0692,   6.9290,
          -2.3394],
        [ -5.6494,   1.9639,  -3.1178,  -3.7296,  -5.9548,   7.9720,   6.8330,
          -2.4687],
        [ -9.2196,   3.1540,  -4.9685,  -5.9720,  -9.5585,  12.1774,  10.8520,
          -4.0932],
        [  4.5228,  -5.5748,   0.9025,   1.6094,   4.8704, -11.0652, -12.1787,
          -0.1302],
        [  0.9712,  -4.3195,  -0.9201,  -0.6233,   1.3098,  -6.5061,  -7.9507,
          -1.6484],
        [ -6.0356,   1.2346,  -3.5489,  -4.0983,  -5.9702,   5.6551,   5.1995,
          -3.2125]])...
	Related Layers:
		- parent layers: matmul_89_587
		- child layers: maskedfill_45_591
		- shares parents with no other layers
		- shares children with layers: eq_45_590
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.5.sa.heads.4:1
	Time elapsed:  7.129E-05s
	Output of modules: none
	Lookup keys: -61, 637, mul_45, mul_45:1, mul_45_588, mul_45_588:1
--------------------------------------------
Layer getitem_45_589, operation 594/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_45
		- child layers: eq_45_590
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.060E-05s
	Output of modules: none
	Lookup keys: -59, 639, getitem_45, getitem_45:1, getitem_45_589, getitem_45_589:1
--------------------------------------------
Layer eq_45_590, operation 595/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_45_589
		- child layers: maskedfill_45_591
		- shares parents with no other layers
		- shares children with layers: mul_45_588
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.011E-05s
	Output of modules: none
	Lookup keys: -58, 640, eq_45, eq_45:1, eq_45_590, eq_45_590:1
--------------------------------------------
Layer maskedfill_45_591, operation 596/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -6.1471,     -inf,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -6.7163,   0.5013,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -5.5206,   2.0807,  -3.0112,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -5.6494,   1.9639,  -3.1178,  -3.7296,     -inf,     -inf,     -inf,
             -inf],
        [ -9.2196,   3.1540,  -4.9685,  -5.9720,  -9.5585,     -inf,     -inf,
             -inf],
        [  4.5228,  -5.5748,   0.9025,   1.6094,   4.8704, -11.0652,     -inf,
             -inf],
        [  0.9712,  -4.3195,  -0.9201,  -0.6233,   1.3098,  -6.5061,  -7.9507,
             -inf],
        [ -6.0356,   1.2346,  -3.5489,  -4.0983,  -5.9702,   5.6551,   5.1995,
          -3.2125]])...
	Related Layers:
		- parent layers: mul_45_588, eq_45_590
		- child layers: softmax_45_592
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.5.sa.heads.4:1
	Time elapsed:  8.559E-05s
	Output of modules: none
	Lookup keys: -57, 641, maskedfill_45, maskedfill_45:1, maskedfill_45_591, maskedfill_45_591:1
--------------------------------------------
Layer softmax_45_592, operation 597/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [7.3307e-04, 9.9927e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [4.9646e-04, 9.9340e-01, 6.1056e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [4.8893e-04, 9.9003e-01, 6.1476e-03, 3.3341e-03, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [4.2273e-06, 9.9959e-01, 2.9667e-04, 1.0875e-04, 3.0120e-06, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [4.0051e-01, 1.6492e-05, 1.0723e-02, 2.1745e-02, 5.6700e-01, 6.8053e-08,
         0.0000e+00, 0.0000e+00],
        [3.6198e-01, 1.8238e-03, 5.4612e-02, 7.3483e-02, 5.0785e-01, 2.0480e-04,
         4.8302e-05, 0.0000e+00],
        [5.0845e-06, 7.3057e-03, 6.1120e-05, 3.5287e-05, 5.4280e-06, 6.0738e-01,
         3.8512e-01, 8.5563e-05]])...
	Related Layers:
		- parent layers: maskedfill_45_591
		- child layers: dropout_55_593
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.5.sa.heads.4:1
	Time elapsed:  7.081E-05s
	Output of modules: none
	Lookup keys: -56, 642, softmax_45, softmax_45:1, softmax_45_592, softmax_45_592:1
--------------------------------------------
Layer dropout_55_593, operation 598/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [7.3307e-04, 9.9927e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [4.9646e-04, 9.9340e-01, 6.1056e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [4.8893e-04, 9.9003e-01, 6.1476e-03, 3.3341e-03, 0.0000e+00, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [4.2273e-06, 9.9959e-01, 2.9667e-04, 1.0875e-04, 3.0120e-06, 0.0000e+00,
         0.0000e+00, 0.0000e+00],
        [4.0051e-01, 1.6492e-05, 1.0723e-02, 2.1745e-02, 5.6700e-01, 6.8053e-08,
         0.0000e+00, 0.0000e+00],
        [3.6198e-01, 1.8238e-03, 5.4612e-02, 7.3483e-02, 5.0785e-01, 2.0480e-04,
         4.8302e-05, 0.0000e+00],
        [5.0845e-06, 7.3057e-03, 6.1120e-05, 3.5287e-05, 5.4280e-06, 6.0738e-01,
         3.8512e-01, 8.5563e-05]])...
	Related Layers:
		- parent layers: softmax_45_592
		- child layers: matmul_90_595
		- shares parents with no other layers
		- shares children with layers: linear_150_594
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.5.sa.heads.4.dropout:1
	Time elapsed:  5.460E-05s
	Output of modules: blocks.5.sa.heads.4.dropout
	Output of bottom-level module: blocks.5.sa.heads.4.dropout:1
	Lookup keys: -55, 643, blocks.5.sa.heads.4.dropout, blocks.5.sa.heads.4.dropout:1, dropout_55, dropout_55:1, dropout_55_593, dropout_55_593:1
--------------------------------------------
Layer linear_150_594, operation 599/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.1007, -1.1435,  0.0759,  0.1153, -0.2767,  0.9552,  0.1524,  0.2081],
        [ 0.2725, -0.1931,  0.3596,  0.1289, -0.1936,  0.5087, -0.3183,  0.6537],
        [-0.5431, -0.9722, -0.4882,  0.0450,  0.1552,  1.0065, -0.2056, -0.4821],
        [-0.6562, -1.0201, -0.2373,  0.0604,  0.1469,  1.0962, -0.1567, -0.9145],
        [-0.7245, -1.7924,  0.2351,  0.1281, -0.2504,  1.8495, -0.1984, -0.1365],
        [ 1.0781,  1.1495,  1.7265, -0.0769,  0.0943, -1.0614, -0.7253,  0.1342],
        [-0.1714,  0.5588,  1.7997, -0.3513, -0.7629, -0.8252, -1.0165, -0.1070],
        [ 0.6506, -0.0511,  0.1215,  0.5758, -0.1815,  1.1679, -0.7657,  0.1038]])...
	Related Layers:
		- parent layers: layernorm_11_535
		- child layers: matmul_90_595
		- shares parents with layers: linear_136_536, linear_137_537, linear_138_546, linear_139_548, linear_140_549, linear_141_558, linear_142_560, linear_143_561, linear_144_570, linear_145_572, linear_146_573, linear_147_582, linear_148_584, linear_149_585, linear_151_596, linear_152_597, linear_153_606, linear_154_608, linear_155_609, linear_156_618, linear_157_620, linear_158_621, linear_159_630
		- shares children with layers: dropout_55_593
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.4.value:1
	Time elapsed:  1.197E-04s
	Output of modules: blocks.5.sa.heads.4.value
	Output of bottom-level module: blocks.5.sa.heads.4.value:1
	Lookup keys: -54, 644, blocks.5.sa.heads.4.value, blocks.5.sa.heads.4.value:1, linear_150, linear_150:1, linear_150_594, linear_150_594:1
--------------------------------------------
Layer matmul_90_595, operation 600/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.1007, -1.1435,  0.0759,  0.1153, -0.2767,  0.9552,  0.1524,  0.2081],
        [ 0.2724, -0.1938,  0.3594,  0.1289, -0.1937,  0.5090, -0.3180,  0.6534],
        [ 0.2675, -0.1983,  0.3543,  0.1284, -0.1916,  0.5120, -0.3174,  0.6466],
        [ 0.2644, -0.2011,  0.3522,  0.1281, -0.1904,  0.5139, -0.3169,  0.6413],
        [ 0.2722, -0.1934,  0.3593,  0.1289, -0.1935,  0.5089, -0.3183,  0.6532],
        [-0.3906, -1.5069,  0.1533,  0.1206, -0.2479,  1.4659, -0.0571, -0.0191],
        [-0.4087, -1.4523,  0.1039,  0.1139, -0.2084,  1.4212, -0.0691, -0.0863],
        [ 0.5908,  0.9119,  1.7444, -0.1810, -0.2380, -0.9585, -0.8344,  0.0450]])...
	Related Layers:
		- parent layers: dropout_55_593, linear_150_594
		- child layers: cat_6_632
		- shares parents with no other layers
		- shares children with layers: matmul_82_547, matmul_84_559, matmul_86_571, matmul_88_583, matmul_92_607, matmul_94_619, matmul_96_631
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.4:1
	Time elapsed:  8.297E-05s
	Output of modules: blocks.5.sa.heads.4
	Lookup keys: -53, 645, blocks.5.sa.heads.4, blocks.5.sa.heads.4:1, matmul_90, matmul_90:1, matmul_90_595, matmul_90_595:1
--------------------------------------------
Layer linear_151_596, operation 601/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 3.8116e-01,  2.0038e+00,  1.7486e+00, -1.3924e+00,  3.8133e+00,
          4.8827e+00, -2.5933e+00,  1.6742e+00],
        [-1.4177e+00,  1.0035e+00,  5.6108e-01, -1.9033e+00,  3.3107e+00,
          3.6512e+00, -2.1580e+00,  1.1099e-01],
        [-2.9384e-01,  1.5155e+00, -1.9336e-01, -1.1234e+00,  2.5351e+00,
          2.9759e+00, -1.9885e+00,  1.3229e+00],
        [-5.2930e-01,  1.5079e+00, -7.4856e-02, -1.0028e+00,  1.8315e+00,
          2.9241e+00, -1.7550e+00,  9.0652e-01],
        [-1.8623e+00, -1.6981e-01, -5.9771e-01, -3.0144e-01,  2.0938e+00,
          3.3123e+00, -5.7882e-01,  4.1837e-01],
        [ 4.4858e+00,  5.0174e+00,  4.2991e+00, -6.6081e-01,  4.3022e+00,
          1.6871e+00, -4.3881e+00,  3.2076e+00],
        [ 1.0277e+00,  2.1661e+00,  2.4484e+00, -1.4360e+00,  3.7568e+00,
          1.1890e+00, -2.1627e+00,  8.2837e-01],
        [-7.6095e-01,  6.6173e-01,  2.9555e-03, -1.2331e+00,  3.1323e+00,
          2.1114e+00, -1.3798e+00, -3.6306e-01]])...
	Related Layers:
		- parent layers: layernorm_11_535
		- child layers: transpose_46_598
		- shares parents with layers: linear_136_536, linear_137_537, linear_138_546, linear_139_548, linear_140_549, linear_141_558, linear_142_560, linear_143_561, linear_144_570, linear_145_572, linear_146_573, linear_147_582, linear_148_584, linear_149_585, linear_150_594, linear_152_597, linear_153_606, linear_154_608, linear_155_609, linear_156_618, linear_157_620, linear_158_621, linear_159_630
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.5.key:1
	Time elapsed:  1.166E-04s
	Output of modules: blocks.5.sa.heads.5.key
	Output of bottom-level module: blocks.5.sa.heads.5.key:1
	Lookup keys: -52, 646, blocks.5.sa.heads.5.key, blocks.5.sa.heads.5.key:1, linear_151, linear_151:1, linear_151_596, linear_151_596:1
--------------------------------------------
Layer linear_152_597, operation 602/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-3.0343,  0.4911, -0.1136,  1.5139, -0.6001, -0.8343,  1.6829, -0.7657],
        [-1.0441,  0.0444,  1.2382,  0.6220,  0.4790, -0.1260, -0.1390, -0.8136],
        [-2.7983, -0.6248, -0.4393,  0.3568,  0.4664, -1.3390,  0.9257, -1.4458],
        [-2.9223, -0.4434, -0.0707, -0.0850,  0.4574, -1.4094,  1.1845, -1.4231],
        [-2.9861,  0.5278, -1.0528,  0.7581, -0.4552, -1.5992,  1.5405, -1.5289],
        [-3.0037, -2.2288, -0.8108,  0.7774,  0.0852,  1.3259,  1.7407, -1.5958],
        [-3.1550, -1.7968, -0.2160,  0.6204,  0.7179,  0.9824,  0.6889, -1.8472],
        [-3.6824, -1.0447, -0.9544, -0.0161, -0.4146, -0.8297,  2.2174, -1.9536]])...
	Related Layers:
		- parent layers: layernorm_11_535
		- child layers: matmul_91_599
		- shares parents with layers: linear_136_536, linear_137_537, linear_138_546, linear_139_548, linear_140_549, linear_141_558, linear_142_560, linear_143_561, linear_144_570, linear_145_572, linear_146_573, linear_147_582, linear_148_584, linear_149_585, linear_150_594, linear_151_596, linear_153_606, linear_154_608, linear_155_609, linear_156_618, linear_157_620, linear_158_621, linear_159_630
		- shares children with layers: transpose_46_598
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.5.query:1
	Time elapsed:  1.125E-04s
	Output of modules: blocks.5.sa.heads.5.query
	Output of bottom-level module: blocks.5.sa.heads.5.query:1
	Lookup keys: -51, 647, blocks.5.sa.heads.5.query, blocks.5.sa.heads.5.query:1, linear_152, linear_152:1, linear_152_597, linear_152_597:1
--------------------------------------------
Layer transpose_46_598, operation 603/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 3.8116e-01, -1.4177e+00, -2.9384e-01, -5.2930e-01, -1.8623e+00,
          4.4858e+00,  1.0277e+00, -7.6095e-01],
        [ 2.0038e+00,  1.0035e+00,  1.5155e+00,  1.5079e+00, -1.6981e-01,
          5.0174e+00,  2.1661e+00,  6.6173e-01],
        [ 1.7486e+00,  5.6108e-01, -1.9336e-01, -7.4856e-02, -5.9771e-01,
          4.2991e+00,  2.4484e+00,  2.9555e-03],
        [-1.3924e+00, -1.9033e+00, -1.1234e+00, -1.0028e+00, -3.0144e-01,
         -6.6081e-01, -1.4360e+00, -1.2331e+00],
        [ 3.8133e+00,  3.3107e+00,  2.5351e+00,  1.8315e+00,  2.0938e+00,
          4.3022e+00,  3.7568e+00,  3.1323e+00],
        [ 4.8827e+00,  3.6512e+00,  2.9759e+00,  2.9241e+00,  3.3123e+00,
          1.6871e+00,  1.1890e+00,  2.1114e+00],
        [-2.5933e+00, -2.1580e+00, -1.9885e+00, -1.7550e+00, -5.7882e-01,
         -4.3881e+00, -2.1627e+00, -1.3798e+00],
        [ 1.6742e+00,  1.1099e-01,  1.3229e+00,  9.0652e-01,  4.1837e-01,
          3.2076e+00,  8.2837e-01, -3.6306e-01]])...
	Related Layers:
		- parent layers: linear_151_596
		- child layers: matmul_91_599
		- shares parents with no other layers
		- shares children with layers: linear_152_597
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.5.sa.heads.5:1
	Time elapsed:  6.032E-05s
	Output of modules: none
	Lookup keys: -50, 648, transpose_46, transpose_46:1, transpose_46_598, transpose_46_598:1
--------------------------------------------
Layer matmul_91_599, operation 604/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -54.1195,  -39.9303,  -30.6118,  -25.8105,   -8.3152,  -90.2425,
          -62.1365,  -31.8747],
        [ -10.5297,    1.6367,  -16.0815,  -15.9673,  -16.7410,   21.7714,
           13.8378,    2.6529],
        [ -55.5474,  -15.0778,  -31.9693,  -32.1366,  -10.8719,  -77.3440,
          -40.7376,   -4.4772],
        [ -51.0770,  -11.3512,  -29.1201,  -29.7852,   -9.0101,  -72.7877,
          -38.0386,   -0.9634],
        [ -66.8276,  -34.0694,  -40.7073,  -40.9490,  -27.9782,  -76.5766,
          -51.2943,  -26.0641],
        [ -46.2879,  -34.3863,  -25.3942,  -13.5205,    7.6658, -100.2193,
          -44.3909,  -20.1446],
        [ -10.4293,   -2.3147,   -2.2448,    6.7657,   27.3080,  -77.5017,
          -26.9547,    6.4319],
        [ -70.5907,  -22.2621,  -40.9813,  -36.9914,  -14.2724, -103.4852,
          -46.9042,   -6.7770]])...
	Related Layers:
		- parent layers: linear_152_597, transpose_46_598
		- child layers: mul_46_600
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.5:1
	Time elapsed:  8.750E-05s
	Output of modules: none
	Lookup keys: -49, 649, matmul_91, matmul_91:1, matmul_91_599, matmul_91_599:1
--------------------------------------------
Layer mul_46_600, operation 605/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-2.7618, -2.0377, -1.5622, -1.3171, -0.4243, -4.6052, -3.1709, -1.6266],
        [-0.5373,  0.0835, -0.8207, -0.8148, -0.8543,  1.1110,  0.7062,  0.1354],
        [-2.8346, -0.7694, -1.6314, -1.6400, -0.5548, -3.9469, -2.0789, -0.2285],
        [-2.6065, -0.5793, -1.4860, -1.5200, -0.4598, -3.7144, -1.9411, -0.0492],
        [-3.4103, -1.7386, -2.0773, -2.0897, -1.4278, -3.9078, -2.6176, -1.3301],
        [-2.3621, -1.7548, -1.2959, -0.6900,  0.3912, -5.1143, -2.2653, -1.0280],
        [-0.5322, -0.1181, -0.1146,  0.3453,  1.3936, -3.9550, -1.3755,  0.3282],
        [-3.6023, -1.1361, -2.0913, -1.8877, -0.7283, -5.2810, -2.3936, -0.3458]])...
	Related Layers:
		- parent layers: matmul_91_599
		- child layers: maskedfill_46_603
		- shares parents with no other layers
		- shares children with layers: eq_46_602
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.5.sa.heads.5:1
	Time elapsed:  6.962E-05s
	Output of modules: none
	Lookup keys: -48, 650, mul_46, mul_46:1, mul_46_600, mul_46_600:1
--------------------------------------------
Layer getitem_46_601, operation 606/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_46
		- child layers: eq_46_602
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  8.988E-05s
	Output of modules: none
	Lookup keys: -46, 652, getitem_46, getitem_46:1, getitem_46_601, getitem_46_601:1
--------------------------------------------
Layer eq_46_602, operation 607/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_46_601
		- child layers: maskedfill_46_603
		- shares parents with no other layers
		- shares children with layers: mul_46_600
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  7.892E-05s
	Output of modules: none
	Lookup keys: -45, 653, eq_46, eq_46:1, eq_46_602, eq_46_602:1
--------------------------------------------
Layer maskedfill_46_603, operation 608/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-2.7618,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-0.5373,  0.0835,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-2.8346, -0.7694, -1.6314,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-2.6065, -0.5793, -1.4860, -1.5200,    -inf,    -inf,    -inf,    -inf],
        [-3.4103, -1.7386, -2.0773, -2.0897, -1.4278,    -inf,    -inf,    -inf],
        [-2.3621, -1.7548, -1.2959, -0.6900,  0.3912, -5.1143,    -inf,    -inf],
        [-0.5322, -0.1181, -0.1146,  0.3453,  1.3936, -3.9550, -1.3755,    -inf],
        [-3.6023, -1.1361, -2.0913, -1.8877, -0.7283, -5.2810, -2.3936, -0.3458]])...
	Related Layers:
		- parent layers: mul_46_600, eq_46_602
		- child layers: softmax_46_604
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.5.sa.heads.5:1
	Time elapsed:  8.273E-05s
	Output of modules: none
	Lookup keys: -44, 654, maskedfill_46, maskedfill_46:1, maskedfill_46_603, maskedfill_46_603:1
--------------------------------------------
Layer softmax_46_604, operation 609/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3496, 0.6504, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0818, 0.6455, 0.2726, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0684, 0.5192, 0.2097, 0.2027, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0473, 0.2519, 0.1796, 0.1774, 0.3438, 0.0000, 0.0000, 0.0000],
        [0.0373, 0.0684, 0.1083, 0.1985, 0.5851, 0.0024, 0.0000, 0.0000],
        [0.0727, 0.1100, 0.1104, 0.1748, 0.4986, 0.0024, 0.0313, 0.0000],
        [0.0143, 0.1681, 0.0647, 0.0793, 0.2527, 0.0027, 0.0478, 0.3705]])...
	Related Layers:
		- parent layers: maskedfill_46_603
		- child layers: dropout_56_605
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.5.sa.heads.5:1
	Time elapsed:  6.723E-05s
	Output of modules: none
	Lookup keys: -43, 655, softmax_46, softmax_46:1, softmax_46_604, softmax_46_604:1
--------------------------------------------
Layer dropout_56_605, operation 610/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3496, 0.6504, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0818, 0.6455, 0.2726, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0684, 0.5192, 0.2097, 0.2027, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0473, 0.2519, 0.1796, 0.1774, 0.3438, 0.0000, 0.0000, 0.0000],
        [0.0373, 0.0684, 0.1083, 0.1985, 0.5851, 0.0024, 0.0000, 0.0000],
        [0.0727, 0.1100, 0.1104, 0.1748, 0.4986, 0.0024, 0.0313, 0.0000],
        [0.0143, 0.1681, 0.0647, 0.0793, 0.2527, 0.0027, 0.0478, 0.3705]])...
	Related Layers:
		- parent layers: softmax_46_604
		- child layers: matmul_92_607
		- shares parents with no other layers
		- shares children with layers: linear_153_606
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.5.sa.heads.5.dropout:1
	Time elapsed:  5.364E-05s
	Output of modules: blocks.5.sa.heads.5.dropout
	Output of bottom-level module: blocks.5.sa.heads.5.dropout:1
	Lookup keys: -42, 656, blocks.5.sa.heads.5.dropout, blocks.5.sa.heads.5.dropout:1, dropout_56, dropout_56:1, dropout_56_605, dropout_56_605:1
--------------------------------------------
Layer linear_153_606, operation 611/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.2013,  2.1902,  1.7984, -0.0098,  0.7010,  0.4441,  0.7225,  0.9449],
        [-0.1923,  0.8554,  0.1585,  0.4491, -0.6332, -0.3288,  0.3729, -0.0243],
        [ 0.1037,  1.4928,  2.1903, -0.8087,  0.5358,  1.1113,  0.2402,  0.8656],
        [ 0.1438,  1.5781,  2.4482, -0.9229,  0.4528,  0.9508,  0.3584,  1.0269],
        [ 0.3334,  2.5378,  3.0290, -0.3254,  0.7319,  1.1455,  0.0236,  0.9018],
        [-0.2007, -2.2693, -2.1309,  2.2527, -1.5019, -2.1759,  0.0341, -0.8542],
        [-0.2103, -1.0231, -1.4699,  1.1592, -0.8166, -1.5044,  0.3221, -0.3471],
        [ 0.0722,  1.9317,  1.6506,  0.0705, -0.0387,  0.1923,  0.5818,  0.4969]])...
	Related Layers:
		- parent layers: layernorm_11_535
		- child layers: matmul_92_607
		- shares parents with layers: linear_136_536, linear_137_537, linear_138_546, linear_139_548, linear_140_549, linear_141_558, linear_142_560, linear_143_561, linear_144_570, linear_145_572, linear_146_573, linear_147_582, linear_148_584, linear_149_585, linear_150_594, linear_151_596, linear_152_597, linear_154_608, linear_155_609, linear_156_618, linear_157_620, linear_158_621, linear_159_630
		- shares children with layers: dropout_56_605
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.5.value:1
	Time elapsed:  1.192E-04s
	Output of modules: blocks.5.sa.heads.5.value
	Output of bottom-level module: blocks.5.sa.heads.5.value:1
	Lookup keys: -41, 657, blocks.5.sa.heads.5.value, blocks.5.sa.heads.5.value:1, linear_153, linear_153:1, linear_153_606, linear_153_606:1
--------------------------------------------
Layer matmul_92_607, operation 612/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.2013,  2.1902,  1.7984, -0.0098,  0.7010,  0.4441,  0.7225,  0.9449],
        [-0.0547,  1.3220,  0.7318,  0.2887, -0.1668, -0.0586,  0.4951,  0.3145],
        [-0.0794,  1.1384,  0.8466,  0.0686, -0.2053,  0.1271,  0.3653,  0.2976],
        [-0.0352,  1.2268,  1.1607, -0.1241, -0.0767,  0.2854,  0.3660,  0.4416],
        [ 0.1198,  1.7396,  1.9939, -0.3081,  0.3018,  0.7002,  0.2430,  0.6862],
        [ 0.2287,  2.0947,  2.5683, -0.4254,  0.5554,  0.9682,  0.1635,  0.8568],
        [ 0.1893,  1.9218,  2.2769, -0.3225,  0.4554,  0.8038,  0.2046,  0.7778],
        [ 0.0891,  1.6988,  1.6891, -0.0448,  0.1017,  0.3813,  0.3540,  0.5399]])...
	Related Layers:
		- parent layers: dropout_56_605, linear_153_606
		- child layers: cat_6_632
		- shares parents with no other layers
		- shares children with layers: matmul_82_547, matmul_84_559, matmul_86_571, matmul_88_583, matmul_90_595, matmul_94_619, matmul_96_631
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.5:1
	Time elapsed:  8.321E-05s
	Output of modules: blocks.5.sa.heads.5
	Lookup keys: -40, 658, blocks.5.sa.heads.5, blocks.5.sa.heads.5:1, matmul_92, matmul_92:1, matmul_92_607, matmul_92_607:1
--------------------------------------------
Layer linear_154_608, operation 613/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.1236, -0.2115,  1.4451, -1.9698, -0.8356,  2.4068,  1.2587, -1.6631],
        [ 0.0325, -1.4778,  1.9828, -0.6600, -0.5012,  2.0908,  2.4612, -2.0344],
        [ 1.2267, -0.6032,  2.1811, -1.0306,  0.2447,  1.2177,  1.9020, -1.1667],
        [ 1.2343, -0.8918,  2.3998, -0.6661,  0.3436,  1.1689,  1.9871, -0.7574],
        [ 0.3702, -0.1056,  1.5487, -0.3618, -0.3704,  0.9350,  1.1818, -0.6333],
        [ 0.7812, -0.6387,  2.0769, -2.2122, -1.4795,  2.9598,  1.1454, -4.3164],
        [-0.1298, -0.3468,  0.7601, -1.5392, -0.8208,  2.6744,  1.0658, -2.2000],
        [ 0.2659, -0.6716,  1.4029, -0.1519,  0.0288,  1.9429,  2.2507, -0.3786]])...
	Related Layers:
		- parent layers: layernorm_11_535
		- child layers: transpose_47_610
		- shares parents with layers: linear_136_536, linear_137_537, linear_138_546, linear_139_548, linear_140_549, linear_141_558, linear_142_560, linear_143_561, linear_144_570, linear_145_572, linear_146_573, linear_147_582, linear_148_584, linear_149_585, linear_150_594, linear_151_596, linear_152_597, linear_153_606, linear_155_609, linear_156_618, linear_157_620, linear_158_621, linear_159_630
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.6.key:1
	Time elapsed:  1.132E-04s
	Output of modules: blocks.5.sa.heads.6.key
	Output of bottom-level module: blocks.5.sa.heads.6.key:1
	Lookup keys: -39, 659, blocks.5.sa.heads.6.key, blocks.5.sa.heads.6.key:1, linear_154, linear_154:1, linear_154_608, linear_154_608:1
--------------------------------------------
Layer linear_155_609, operation 614/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.6596, -1.7211,  2.7248, -0.1624,  0.7727, -1.2180,  2.4304,  1.2813],
        [-0.0851, -0.9110,  1.9669,  1.5488,  1.8253, -1.6413,  1.9853,  1.6478],
        [-0.0216, -2.1037,  2.6253,  0.1019,  1.9422, -0.9395,  2.3866,  2.5969],
        [ 0.1613, -1.9912,  2.6330, -0.2987,  1.9291, -0.4947,  2.3149,  2.1415],
        [ 1.2101, -1.1190,  2.0764, -0.7436,  1.2093, -1.4410,  2.3111,  2.1503],
        [ 0.7618, -1.4867,  1.6831,  1.9486,  1.9678, -1.0346,  1.4856,  1.0474],
        [-0.0103, -1.4195,  1.1807,  1.3761,  0.9404, -0.9119,  1.5435,  1.2416],
        [-0.1847, -1.6113,  1.0841,  1.6101,  2.6086, -1.0276,  2.7973,  2.7852]])...
	Related Layers:
		- parent layers: layernorm_11_535
		- child layers: matmul_93_611
		- shares parents with layers: linear_136_536, linear_137_537, linear_138_546, linear_139_548, linear_140_549, linear_141_558, linear_142_560, linear_143_561, linear_144_570, linear_145_572, linear_146_573, linear_147_582, linear_148_584, linear_149_585, linear_150_594, linear_151_596, linear_152_597, linear_153_606, linear_154_608, linear_156_618, linear_157_620, linear_158_621, linear_159_630
		- shares children with layers: transpose_47_610
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.6.query:1
	Time elapsed:  1.163E-04s
	Output of modules: blocks.5.sa.heads.6.query
	Output of bottom-level module: blocks.5.sa.heads.6.query:1
	Lookup keys: -38, 660, blocks.5.sa.heads.6.query, blocks.5.sa.heads.6.query:1, linear_155, linear_155:1, linear_155_609, linear_155_609:1
--------------------------------------------
Layer transpose_47_610, operation 615/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 1.1236,  0.0325,  1.2267,  1.2343,  0.3702,  0.7812, -0.1298,  0.2659],
        [-0.2115, -1.4778, -0.6032, -0.8918, -0.1056, -0.6387, -0.3468, -0.6716],
        [ 1.4451,  1.9828,  2.1811,  2.3998,  1.5487,  2.0769,  0.7601,  1.4029],
        [-1.9698, -0.6600, -1.0306, -0.6661, -0.3618, -2.2122, -1.5392, -0.1519],
        [-0.8356, -0.5012,  0.2447,  0.3436, -0.3704, -1.4795, -0.8208,  0.0288],
        [ 2.4068,  2.0908,  1.2177,  1.1689,  0.9350,  2.9598,  2.6744,  1.9429],
        [ 1.2587,  2.4612,  1.9020,  1.9871,  1.1818,  1.1454,  1.0658,  2.2507],
        [-1.6631, -2.0344, -1.1667, -0.7574, -0.6333, -4.3164, -2.2000, -0.3786]])...
	Related Layers:
		- parent layers: linear_154_608
		- child layers: matmul_93_611
		- shares parents with no other layers
		- shares children with layers: linear_155_609
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.5.sa.heads.6:1
	Time elapsed:  5.817E-05s
	Output of modules: none
	Lookup keys: -37, 661, transpose_47, transpose_47:1, transpose_47_610, transpose_47_610:1
--------------------------------------------
Layer matmul_93_611, operation 616/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -1.1783,  22.3150,  34.3419,  39.1710,  19.4272, -10.6644,   0.8034,
          34.1663],
        [-18.0692,  13.3057,  25.0817,  30.0114,  12.6889, -34.6266,  -9.1914,
          33.2338],
        [-24.1474,  11.0100,  22.3713,  28.3167,  11.6479, -50.7096, -18.7561,
          27.1355],
        [-21.7426,  12.6375,  26.2195,  32.0588,  12.8614, -44.0886, -12.5809,
          30.7514],
        [-43.9071, -31.2087,  14.6499,  19.9107,  -5.0936, -59.3941, -39.8193,
           3.8345],
        [-17.2311,  16.5315,  11.0615,  16.0281,   8.9019, -35.8200, -12.3162,
          20.7669],
        [ -9.8827,  17.5770,  11.3175,  15.6864,  10.8618, -25.0956, -10.0771,
          17.4115],
        [-45.9966,  -7.1846,  12.5258,  19.0703,   0.8302, -85.1754, -33.8370,
          24.4888]])...
	Related Layers:
		- parent layers: linear_155_609, transpose_47_610
		- child layers: mul_47_612
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.6:1
	Time elapsed:  9.346E-05s
	Output of modules: none
	Lookup keys: -36, 662, matmul_93, matmul_93:1, matmul_93_611, matmul_93_611:1
--------------------------------------------
Layer mul_47_612, operation 617/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-0.0601,  1.1388,  1.7525,  1.9989,  0.9914, -0.5442,  0.0410,  1.7435],
        [-0.9221,  0.6790,  1.2799,  1.5315,  0.6475, -1.7670, -0.4690,  1.6960],
        [-1.2323,  0.5619,  1.1416,  1.4450,  0.5944, -2.5878, -0.9571,  1.3848],
        [-1.1095,  0.6449,  1.3380,  1.6360,  0.6563, -2.2499, -0.6420,  1.5693],
        [-2.2406, -1.5926,  0.7476,  1.0161, -0.2599, -3.0309, -2.0320,  0.1957],
        [-0.8793,  0.8436,  0.5645,  0.8179,  0.4543, -1.8279, -0.6285,  1.0598],
        [-0.5043,  0.8970,  0.5775,  0.8005,  0.5543, -1.2807, -0.5142,  0.8885],
        [-2.3473, -0.3666,  0.6392,  0.9732,  0.0424, -4.3466, -1.7267,  1.2497]])...
	Related Layers:
		- parent layers: matmul_93_611
		- child layers: maskedfill_47_615
		- shares parents with no other layers
		- shares children with layers: eq_47_614
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.5.sa.heads.6:1
	Time elapsed:  6.962E-05s
	Output of modules: none
	Lookup keys: -35, 663, mul_47, mul_47:1, mul_47_612, mul_47_612:1
--------------------------------------------
Layer getitem_47_613, operation 618/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_47
		- child layers: eq_47_614
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.131E-05s
	Output of modules: none
	Lookup keys: -33, 665, getitem_47, getitem_47:1, getitem_47_613, getitem_47_613:1
--------------------------------------------
Layer eq_47_614, operation 619/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_47_613
		- child layers: maskedfill_47_615
		- shares parents with no other layers
		- shares children with layers: mul_47_612
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  7.725E-05s
	Output of modules: none
	Lookup keys: -32, 666, eq_47, eq_47:1, eq_47_614, eq_47_614:1
--------------------------------------------
Layer maskedfill_47_615, operation 620/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-0.0601,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-0.9221,  0.6790,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-1.2323,  0.5619,  1.1416,    -inf,    -inf,    -inf,    -inf,    -inf],
        [-1.1095,  0.6449,  1.3380,  1.6360,    -inf,    -inf,    -inf,    -inf],
        [-2.2406, -1.5926,  0.7476,  1.0161, -0.2599,    -inf,    -inf,    -inf],
        [-0.8793,  0.8436,  0.5645,  0.8179,  0.4543, -1.8279,    -inf,    -inf],
        [-0.5043,  0.8970,  0.5775,  0.8005,  0.5543, -1.2807, -0.5142,    -inf],
        [-2.3473, -0.3666,  0.6392,  0.9732,  0.0424, -4.3466, -1.7267,  1.2497]])...
	Related Layers:
		- parent layers: mul_47_612, eq_47_614
		- child layers: softmax_47_616
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.5.sa.heads.6:1
	Time elapsed:  8.321E-05s
	Output of modules: none
	Lookup keys: -31, 667, maskedfill_47, maskedfill_47:1, maskedfill_47_615, maskedfill_47_615:1
--------------------------------------------
Layer softmax_47_616, operation 621/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1678, 0.8322, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0563, 0.3388, 0.6049, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0295, 0.1704, 0.3409, 0.4592, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0179, 0.0342, 0.3546, 0.4639, 0.1295, 0.0000, 0.0000, 0.0000],
        [0.0488, 0.2735, 0.2069, 0.2666, 0.1853, 0.0189, 0.0000, 0.0000],
        [0.0624, 0.2533, 0.1840, 0.2300, 0.1798, 0.0287, 0.0618, 0.0000],
        [0.0095, 0.0689, 0.1885, 0.2632, 0.1038, 0.0013, 0.0177, 0.3471]])...
	Related Layers:
		- parent layers: maskedfill_47_615
		- child layers: dropout_57_617
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.5.sa.heads.6:1
	Time elapsed:  7.200E-05s
	Output of modules: none
	Lookup keys: -30, 668, softmax_47, softmax_47:1, softmax_47_616, softmax_47_616:1
--------------------------------------------
Layer dropout_57_617, operation 622/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1678, 0.8322, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0563, 0.3388, 0.6049, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0295, 0.1704, 0.3409, 0.4592, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0179, 0.0342, 0.3546, 0.4639, 0.1295, 0.0000, 0.0000, 0.0000],
        [0.0488, 0.2735, 0.2069, 0.2666, 0.1853, 0.0189, 0.0000, 0.0000],
        [0.0624, 0.2533, 0.1840, 0.2300, 0.1798, 0.0287, 0.0618, 0.0000],
        [0.0095, 0.0689, 0.1885, 0.2632, 0.1038, 0.0013, 0.0177, 0.3471]])...
	Related Layers:
		- parent layers: softmax_47_616
		- child layers: matmul_94_619
		- shares parents with no other layers
		- shares children with layers: linear_156_618
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.5.sa.heads.6.dropout:1
	Time elapsed:  5.317E-05s
	Output of modules: blocks.5.sa.heads.6.dropout
	Output of bottom-level module: blocks.5.sa.heads.6.dropout:1
	Lookup keys: -29, 669, blocks.5.sa.heads.6.dropout, blocks.5.sa.heads.6.dropout:1, dropout_57, dropout_57:1, dropout_57_617, dropout_57_617:1
--------------------------------------------
Layer linear_156_618, operation 623/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.4793, -0.3774, -0.4229, -0.0685,  1.6505,  0.6886,  0.3294, -0.4641],
        [ 0.0386,  0.4722, -0.3060, -1.1788,  0.1960,  0.2133, -0.1529, -0.2990],
        [ 0.0039, -0.2924, -0.9193,  0.5787,  1.1513,  0.7786,  0.4494, -0.8472],
        [ 0.1096, -0.6149, -0.9983,  0.5048,  0.8184,  0.4990,  0.5882, -0.9352],
        [ 0.8415, -0.8810, -1.2506,  0.9650,  2.3179,  1.1610,  0.9039, -0.9254],
        [ 0.2213,  0.3968,  1.2001, -2.7397, -0.9599, -1.3271, -1.5506,  0.5263],
        [ 0.4830,  0.2309,  1.4022, -2.3624, -0.4646, -0.7132, -0.9315,  0.7617],
        [ 0.5798,  0.5944, -0.2898, -0.1486,  1.5463,  0.3656,  0.3799, -0.5633]])...
	Related Layers:
		- parent layers: layernorm_11_535
		- child layers: matmul_94_619
		- shares parents with layers: linear_136_536, linear_137_537, linear_138_546, linear_139_548, linear_140_549, linear_141_558, linear_142_560, linear_143_561, linear_144_570, linear_145_572, linear_146_573, linear_147_582, linear_148_584, linear_149_585, linear_150_594, linear_151_596, linear_152_597, linear_153_606, linear_154_608, linear_155_609, linear_157_620, linear_158_621, linear_159_630
		- shares children with layers: dropout_57_617
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.6.value:1
	Time elapsed:  1.194E-04s
	Output of modules: blocks.5.sa.heads.6.value
	Output of bottom-level module: blocks.5.sa.heads.6.value:1
	Lookup keys: -28, 670, blocks.5.sa.heads.6.value, blocks.5.sa.heads.6.value:1, linear_156, linear_156:1, linear_156_618, linear_156_618:1
--------------------------------------------
Layer matmul_94_619, operation 624/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.4793, -0.3774, -0.4229, -0.0685,  1.6505,  0.6886,  0.3294, -0.4641],
        [ 0.1126,  0.3296, -0.3256, -0.9924,  0.4401,  0.2930, -0.0720, -0.3267],
        [ 0.0425, -0.0382, -0.6836, -0.0531,  0.8558,  0.5820,  0.2386, -0.6399],
        [ 0.0724, -0.3127, -0.8364,  0.2261,  0.8503,  0.5512,  0.4069, -0.7829],
        [ 0.1711, -0.4936, -0.9690,  0.5229,  1.1242,  0.6775,  0.5499, -0.8726],
        [ 0.2241, -0.2694, -0.7697,  0.0556,  1.0019,  0.5761,  0.3622, -0.6905],
        [ 0.2531, -0.2319, -0.6065, -0.1313,  0.9133,  0.4817,  0.2603, -0.5800],
        [ 0.3342, -0.0685, -0.6652,  0.1633,  1.2294,  0.5324,  0.4393, -0.7083]])...
	Related Layers:
		- parent layers: dropout_57_617, linear_156_618
		- child layers: cat_6_632
		- shares parents with no other layers
		- shares children with layers: matmul_82_547, matmul_84_559, matmul_86_571, matmul_88_583, matmul_90_595, matmul_92_607, matmul_96_631
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.6:1
	Time elapsed:  8.416E-05s
	Output of modules: blocks.5.sa.heads.6
	Lookup keys: -27, 671, blocks.5.sa.heads.6, blocks.5.sa.heads.6:1, matmul_94, matmul_94:1, matmul_94_619, matmul_94_619:1
--------------------------------------------
Layer linear_157_620, operation 625/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-2.8858, -3.0312, -1.0458,  3.0655,  2.5356,  3.2500,  2.8367, -1.1823],
        [-2.4122, -1.1106, -0.1266,  2.1714,  1.1910,  2.7448,  3.5207, -1.2983],
        [-2.0936, -3.0242, -0.8606,  2.3653,  0.9950,  2.3143,  2.4029, -1.4599],
        [-1.9802, -3.0027, -0.6885,  2.4105,  0.7818,  2.0741,  2.1364, -1.2384],
        [-1.8252, -3.5034, -0.2976,  1.6628,  0.2753,  2.0424,  0.5230, -0.8571],
        [-3.4768, -0.4249, -1.2204,  2.9995,  2.5648,  4.1245,  3.9637, -0.7435],
        [-2.6009, -0.8030, -1.3439,  2.4000,  1.9139,  3.9793,  3.7124, -1.0093],
        [-2.5233, -2.6071, -1.3619,  2.7129,  0.7814,  2.9136,  2.8273, -1.3623]])...
	Related Layers:
		- parent layers: layernorm_11_535
		- child layers: transpose_48_622
		- shares parents with layers: linear_136_536, linear_137_537, linear_138_546, linear_139_548, linear_140_549, linear_141_558, linear_142_560, linear_143_561, linear_144_570, linear_145_572, linear_146_573, linear_147_582, linear_148_584, linear_149_585, linear_150_594, linear_151_596, linear_152_597, linear_153_606, linear_154_608, linear_155_609, linear_156_618, linear_158_621, linear_159_630
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.7.key:1
	Time elapsed:  1.132E-04s
	Output of modules: blocks.5.sa.heads.7.key
	Output of bottom-level module: blocks.5.sa.heads.7.key:1
	Lookup keys: -26, 672, blocks.5.sa.heads.7.key, blocks.5.sa.heads.7.key:1, linear_157, linear_157:1, linear_157_620, linear_157_620:1
--------------------------------------------
Layer linear_158_621, operation 626/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.7345,  0.1540,  1.6694, -2.0722, -0.5396, -0.6058, -0.4367,  0.3801],
        [ 0.5018, -0.4783,  1.4742, -0.8418, -1.9561,  0.7624,  0.7354, -0.3377],
        [ 1.4048,  0.4943,  1.2301, -1.9146, -0.4562, -0.4772, -0.8047, -0.3037],
        [ 1.0798,  0.4302,  1.1075, -1.9184, -0.5954, -0.5450, -0.6952, -0.2393],
        [ 1.1983,  0.4642,  1.2200, -2.1831,  0.2870, -0.9272, -0.5212,  0.3638],
        [ 0.5519, -1.8539,  0.6763, -0.6879, -0.3766, -0.0542, -1.4778, -0.0586],
        [ 0.0741, -0.3905,  1.5600, -1.7116, -1.2115,  0.3534, -0.4564, -0.0180],
        [ 1.1740,  0.3886,  1.3492, -2.2677, -1.4747, -0.1182, -0.8061, -0.0702]])...
	Related Layers:
		- parent layers: layernorm_11_535
		- child layers: matmul_95_623
		- shares parents with layers: linear_136_536, linear_137_537, linear_138_546, linear_139_548, linear_140_549, linear_141_558, linear_142_560, linear_143_561, linear_144_570, linear_145_572, linear_146_573, linear_147_582, linear_148_584, linear_149_585, linear_150_594, linear_151_596, linear_152_597, linear_153_606, linear_154_608, linear_155_609, linear_156_618, linear_157_620, linear_159_630
		- shares children with layers: transpose_48_622
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.7.query:1
	Time elapsed:  1.121E-04s
	Output of modules: blocks.5.sa.heads.7.query
	Output of bottom-level module: blocks.5.sa.heads.7.query:1
	Lookup keys: -25, 673, blocks.5.sa.heads.7.query, blocks.5.sa.heads.7.query:1, linear_158, linear_158:1, linear_158_621, linear_158_621:1
--------------------------------------------
Layer transpose_48_622, operation 627/648:
	Output tensor: shape=1x48x11(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[-2.8858, -2.4122, -2.0936, -1.9802, -1.8252, -3.4768, -2.6009, -2.5233],
        [-3.0312, -1.1106, -3.0242, -3.0027, -3.5034, -0.4249, -0.8030, -2.6071],
        [-1.0458, -0.1266, -0.8606, -0.6885, -0.2976, -1.2204, -1.3439, -1.3619],
        [ 3.0655,  2.1714,  2.3653,  2.4105,  1.6628,  2.9995,  2.4000,  2.7129],
        [ 2.5356,  1.1910,  0.9950,  0.7818,  0.2753,  2.5648,  1.9139,  0.7814],
        [ 3.2500,  2.7448,  2.3143,  2.0741,  2.0424,  4.1245,  3.9793,  2.9136],
        [ 2.8367,  3.5207,  2.4029,  2.1364,  0.5230,  3.9637,  3.7124,  2.8273],
        [-1.1823, -1.2983, -1.4599, -1.2384, -0.8571, -0.7435, -1.0093, -1.3623]])...
	Related Layers:
		- parent layers: linear_157_620
		- child layers: matmul_95_623
		- shares parents with no other layers
		- shares children with layers: linear_158_621
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: transpose (grad_fn: TransposeBackward0) 
	Computed inside module: blocks.5.sa.heads.7:1
	Time elapsed:  6.151E-05s
	Output of modules: none
	Lookup keys: -24, 674, transpose_48, transpose_48:1, transpose_48_622, transpose_48_622:1
--------------------------------------------
Layer matmul_95_623, operation 628/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[-137.9371, -101.2696,  -88.2689,  -89.6223,  -70.6490, -113.7639,
          -89.6338, -104.5970],
        [ -43.1830,  -23.5974,  -22.2141,  -21.2294,  -13.6934,  -47.9228,
          -34.0443,  -35.9667],
        [-150.0569, -103.9209, -100.0424, -101.4467,  -83.7936, -116.0600,
          -85.8964, -109.1268],
        [-159.1060, -111.1009, -104.2096, -106.1675,  -87.5449, -123.1181,
          -92.0656, -113.7036],
        [-198.2369, -141.4560, -131.0210, -133.6623, -112.6992, -147.4343,
         -112.1831, -142.6483],
        [  13.8746,  -19.5442,   23.3895,   27.6338,   40.4041,  -51.6387,
          -55.9960,   -1.2171],
        [   2.7586,   -7.7053,   10.3562,   13.2291,   22.7985,  -34.6163,
          -33.9733,  -10.8588],
        [-136.8910,  -98.8405,  -86.8350,  -86.7904,  -68.1273, -118.3853,
          -91.0338, -102.4089]])...
	Related Layers:
		- parent layers: linear_158_621, transpose_48_622
		- child layers: mul_48_624
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.7:1
	Time elapsed:  8.631E-05s
	Output of modules: none
	Lookup keys: -23, 675, matmul_95, matmul_95:1, matmul_95_623, matmul_95_623:1
--------------------------------------------
Layer mul_48_624, operation 629/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -7.0391,  -5.1679,  -4.5045,  -4.5735,  -3.6053,  -5.8055,  -4.5741,
          -5.3377],
        [ -2.2037,  -1.2042,  -1.1336,  -1.0834,  -0.6988,  -2.4456,  -1.7373,
          -1.8354],
        [ -7.6576,  -5.3032,  -5.1053,  -5.1769,  -4.2761,  -5.9227,  -4.3834,
          -5.5689],
        [ -8.1193,  -5.6696,  -5.3179,  -5.4178,  -4.4675,  -6.2828,  -4.6982,
          -5.8024],
        [-10.1162,  -7.2186,  -6.6861,  -6.8209,  -5.7512,  -7.5237,  -5.7248,
          -7.2795],
        [  0.7080,  -0.9974,   1.1936,   1.4102,   2.0619,  -2.6352,  -2.8575,
          -0.0621],
        [  0.1408,  -0.3932,   0.5285,   0.6751,   1.1634,  -1.7665,  -1.7337,
          -0.5541],
        [ -6.9857,  -5.0439,  -4.4313,  -4.4290,  -3.4766,  -6.0413,  -4.6455,
          -5.2260]])...
	Related Layers:
		- parent layers: matmul_95_623
		- child layers: maskedfill_48_627
		- shares parents with no other layers
		- shares children with layers: eq_48_626
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __mul__ (grad_fn: MulBackward0) 
	Computed inside module: blocks.5.sa.heads.7:1
	Time elapsed:  6.866E-05s
	Output of modules: none
	Lookup keys: -22, 676, mul_48, mul_48:1, mul_48_624, mul_48_624:1
--------------------------------------------
Layer getitem_48_625, operation 630/648:
	Output tensor: shape=11x11(484 bytes), dype=torch.float32, Tensor size=612 bytes, Tensor storage size=262208
		tensor([[1., 0., 0., 0., 0., 0., 0., 0.],
        [1., 1., 0., 0., 0., 0., 0., 0.],
        [1., 1., 1., 0., 0., 0., 0., 0.],
        [1., 1., 1., 1., 0., 0., 0., 0.],
        [1., 1., 1., 1., 1., 0., 0., 0.],
        [1., 1., 1., 1., 1., 1., 0., 0.],
        [1., 1., 1., 1., 1., 1., 1., 0.],
        [1., 1., 1., 1., 1., 1., 1., 1.]])...
	Related Layers:
		- parent layers: buffer_48
		- child layers: eq_48_626
		- shares parents with no other layers
		- shares children with no other layers
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __getitem__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  9.632E-05s
	Output of modules: none
	Lookup keys: -20, 678, getitem_48, getitem_48:1, getitem_48_625, getitem_48_625:1
--------------------------------------------
Layer eq_48_626, operation 631/648:
	Output tensor: shape=11x11(121 bytes), dype=torch.bool, Tensor size=249 bytes, Tensor storage size=185
		tensor([[False,  True,  True,  True,  True,  True,  True,  True],
        [False, False,  True,  True,  True,  True,  True,  True],
        [False, False, False,  True,  True,  True,  True,  True],
        [False, False, False, False,  True,  True,  True,  True],
        [False, False, False, False, False,  True,  True,  True],
        [False, False, False, False, False, False,  True,  True],
        [False, False, False, False, False, False, False,  True],
        [False, False, False, False, False, False, False, False]])...
	Related Layers:
		- parent layers: getitem_48_625
		- child layers: maskedfill_48_627
		- shares parents with no other layers
		- shares children with layers: mul_48_624
		- tensor was created de novo inside the model (not computed from input)
		- ancestor of output layers: output_1
	Params: no params used
	Function: __eq__ (grad_fn: NoneType) 
	Computed inside module: not computed inside a module
	Time elapsed:  7.820E-05s
	Output of modules: none
	Lookup keys: -19, 679, eq_48, eq_48:1, eq_48_626, eq_48_626:1
--------------------------------------------
Layer maskedfill_48_627, operation 632/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[ -7.0391,     -inf,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -2.2037,  -1.2042,     -inf,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -7.6576,  -5.3032,  -5.1053,     -inf,     -inf,     -inf,     -inf,
             -inf],
        [ -8.1193,  -5.6696,  -5.3179,  -5.4178,     -inf,     -inf,     -inf,
             -inf],
        [-10.1162,  -7.2186,  -6.6861,  -6.8209,  -5.7512,     -inf,     -inf,
             -inf],
        [  0.7080,  -0.9974,   1.1936,   1.4102,   2.0619,  -2.6352,     -inf,
             -inf],
        [  0.1408,  -0.3932,   0.5285,   0.6751,   1.1634,  -1.7665,  -1.7337,
             -inf],
        [ -6.9857,  -5.0439,  -4.4313,  -4.4290,  -3.4766,  -6.0413,  -4.6455,
          -5.2260]])...
	Related Layers:
		- parent layers: mul_48_624, eq_48_626
		- child layers: softmax_48_628
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: masked_fill (grad_fn: MaskedFillBackward0) 
	Computed inside module: blocks.5.sa.heads.7:1
	Time elapsed:  8.297E-05s
	Output of modules: none
	Lookup keys: -18, 680, maskedfill_48, maskedfill_48:1, maskedfill_48_627, maskedfill_48_627:1
--------------------------------------------
Layer softmax_48_628, operation 633/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2690, 0.7310, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0410, 0.4322, 0.5268, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0228, 0.2636, 0.3747, 0.3390, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0064, 0.1165, 0.1984, 0.1734, 0.5053, 0.0000, 0.0000, 0.0000],
        [0.1145, 0.0208, 0.1861, 0.2311, 0.4434, 0.0040, 0.0000, 0.0000],
        [0.1274, 0.0747, 0.1878, 0.2174, 0.3543, 0.0189, 0.0195, 0.0000],
        [0.0116, 0.0811, 0.1497, 0.1501, 0.3890, 0.0299, 0.1209, 0.0676]])...
	Related Layers:
		- parent layers: maskedfill_48_627
		- child layers: dropout_58_629
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: softmax (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.5.sa.heads.7:1
	Time elapsed:  7.081E-05s
	Output of modules: none
	Lookup keys: -17, 681, softmax_48, softmax_48:1, softmax_48_628, softmax_48_628:1
--------------------------------------------
Layer dropout_58_629, operation 634/648:
	Output tensor: shape=1x11x11(484 bytes), dype=torch.float32, Tensor size=628 bytes, Tensor storage size=548
		tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2690, 0.7310, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0410, 0.4322, 0.5268, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0228, 0.2636, 0.3747, 0.3390, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0064, 0.1165, 0.1984, 0.1734, 0.5053, 0.0000, 0.0000, 0.0000],
        [0.1145, 0.0208, 0.1861, 0.2311, 0.4434, 0.0040, 0.0000, 0.0000],
        [0.1274, 0.0747, 0.1878, 0.2174, 0.3543, 0.0189, 0.0195, 0.0000],
        [0.0116, 0.0811, 0.1497, 0.1501, 0.3890, 0.0299, 0.1209, 0.0676]])...
	Related Layers:
		- parent layers: softmax_48_628
		- child layers: matmul_96_631
		- shares parents with no other layers
		- shares children with layers: linear_159_630
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: SoftmaxBackward0) 
	Computed inside module: blocks.5.sa.heads.7.dropout:1
	Time elapsed:  5.364E-05s
	Output of modules: blocks.5.sa.heads.7.dropout
	Output of bottom-level module: blocks.5.sa.heads.7.dropout:1
	Lookup keys: -16, 682, blocks.5.sa.heads.7.dropout, blocks.5.sa.heads.7.dropout:1, dropout_58, dropout_58:1, dropout_58_629, dropout_58_629:1
--------------------------------------------
Layer linear_159_630, operation 635/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.0819, -0.2568, -0.2967,  0.6642, -1.1498, -1.4253, -0.8242, -1.0108],
        [ 0.5844,  1.2709, -0.8278,  0.1333, -0.6750, -1.0052, -0.7949, -1.6212],
        [ 0.4443,  0.3746,  0.1545,  1.0083, -0.9391, -0.5201, -0.6688, -0.9631],
        [ 0.2020,  0.3855, -0.1681,  1.1276, -0.9008, -0.7117, -0.7817, -1.1654],
        [ 0.0591, -0.2841, -0.1802,  1.1968, -0.5588, -1.0328, -0.3660, -1.6664],
        [-0.4699,  1.3138,  0.5908,  0.0143, -0.2443, -0.9936,  0.6778,  0.8084],
        [ 0.9612,  1.1250, -0.4328,  0.1080, -0.9277, -1.8719,  0.0662, -0.1780],
        [ 0.9910,  0.5249, -1.3711,  0.6516, -1.6388, -1.1774, -1.1599, -1.1760]])...
	Related Layers:
		- parent layers: layernorm_11_535
		- child layers: matmul_96_631
		- shares parents with layers: linear_136_536, linear_137_537, linear_138_546, linear_139_548, linear_140_549, linear_141_558, linear_142_560, linear_143_561, linear_144_570, linear_145_572, linear_146_573, linear_147_582, linear_148_584, linear_149_585, linear_150_594, linear_151_596, linear_152_597, linear_153_606, linear_154_608, linear_155_609, linear_156_618, linear_157_620, linear_158_621
		- shares children with layers: dropout_58_629
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (48, 384); 18432 params total (72.1 KB)
	Function: linear (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.7.value:1
	Time elapsed:  1.187E-04s
	Output of modules: blocks.5.sa.heads.7.value
	Output of bottom-level module: blocks.5.sa.heads.7.value:1
	Lookup keys: -15, 683, blocks.5.sa.heads.7.value, blocks.5.sa.heads.7.value:1, linear_159, linear_159:1, linear_159_630, linear_159_630:1
--------------------------------------------
Layer matmul_96_631, operation 636/648:
	Output tensor: shape=1x11x48(2112 bytes), dype=torch.float32, Tensor size=2256 bytes, Tensor storage size=2176
		tensor([[ 0.0819, -0.2568, -0.2967,  0.6642, -1.1498, -1.4253, -0.8242, -1.0108],
        [ 0.4492,  0.8599, -0.6849,  0.2761, -0.8027, -1.1182, -0.8028, -1.4570],
        [ 0.4900,  0.7361, -0.2886,  0.6160, -0.8336, -0.7669, -0.7297, -1.2495],
        [ 0.3908,  0.6002, -0.2241,  0.8103, -0.8613, -0.7335, -0.7439, -1.2063],
        [ 0.2216,  0.1439, -0.1879,  1.0201, -0.7109, -0.8747, -0.5511, -1.4306],
        [ 0.1752,  0.0352, -0.1388,  1.0578, -0.7774, -0.9074, -0.5756, -1.3337],
        [ 0.2123,  0.1625, -0.1683,  0.9554, -0.7898, -0.9303, -0.5755, -1.2626],
        [ 0.3373,  0.3143, -0.2702,  0.8618, -0.7916, -1.0202, -0.4841, -1.1875]])...
	Related Layers:
		- parent layers: dropout_58_629, linear_159_630
		- child layers: cat_6_632
		- shares parents with no other layers
		- shares children with layers: matmul_82_547, matmul_84_559, matmul_86_571, matmul_88_583, matmul_90_595, matmul_92_607, matmul_94_619
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __matmul__ (grad_fn: UnsafeViewBackward0) 
	Computed inside module: blocks.5.sa.heads.7:1
	Time elapsed:  8.273E-05s
	Output of modules: blocks.5.sa.heads.7
	Lookup keys: -14, 684, blocks.5.sa.heads.7, blocks.5.sa.heads.7:1, matmul_96, matmul_96:1, matmul_96_631, matmul_96_631:1
--------------------------------------------
Layer cat_6_632, operation 637/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[ 0.8003,  1.2568, -0.8669, -0.9908, -1.2398,  0.5950, -0.5027,  0.8934],
        [ 0.4746,  0.4604, -0.7734, -0.5898,  0.1642,  0.8225,  0.1851,  0.0749],
        [ 0.6246,  0.1775, -1.1107, -0.4622, -0.1771,  1.0850, -0.0607,  0.3630],
        [ 0.7332,  0.2099, -1.1409, -0.4159, -0.3959,  1.1421, -0.2741,  0.5037],
        [ 1.1339,  0.6348, -1.3424, -0.3482, -0.7367,  0.9230, -0.5450,  0.5957],
        [ 0.3271,  0.4258, -0.8631, -0.4258, -0.2256,  0.8920, -0.3646,  0.2873],
        [ 0.1153,  0.2232, -0.7510, -0.4397, -0.1217,  0.9577, -0.2944,  0.2655],
        [ 0.7298,  0.3636, -1.2010, -0.5288, -0.6378,  0.9769, -0.3025,  0.3710]])...
	Related Layers:
		- parent layers: matmul_82_547, matmul_84_559, matmul_86_571, matmul_88_583, matmul_90_595, matmul_92_607, matmul_94_619, matmul_96_631
		- child layers: linear_160_633
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: cat (grad_fn: CatBackward0) 
	Computed inside module: blocks.5.sa:1
	Time elapsed:  8.583E-05s
	Output of modules: none
	Lookup keys: -13, 685, cat_6, cat_6:1, cat_6_632, cat_6_632:1
--------------------------------------------
Layer linear_160_633, operation 638/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[ 5.3686e-01,  5.4806e-01, -2.4502e-01, -1.5462e-01, -7.8805e-02,
         -8.4949e-02, -4.5442e-01, -1.7063e+00],
        [-3.3756e-01,  5.7908e-01, -3.1094e-01, -1.9116e-01, -3.8597e-01,
         -1.3993e-01, -1.6212e-01, -1.3099e+00],
        [-2.5440e-01,  7.6965e-01, -3.1684e-01, -6.2696e-02, -4.2503e-01,
          1.0855e-03,  1.3028e-02, -1.2961e+00],
        [-1.4200e-01,  8.7421e-01, -3.1735e-01,  2.1611e-02, -3.3196e-01,
          3.9479e-02,  6.9101e-02, -1.3839e+00],
        [ 1.4830e-01,  9.5012e-01, -4.8464e-01,  6.8864e-02, -2.7391e-01,
          5.3687e-02, -5.1235e-02, -1.7861e+00],
        [ 5.8961e-02,  8.8118e-01,  1.3451e-01, -3.6019e-01,  1.2605e-01,
          3.6658e-01,  4.4092e-01, -1.4045e+00],
        [ 6.9889e-02,  6.1902e-01, -1.6388e-01, -5.0268e-01, -4.6146e-01,
          3.1289e-01, -1.4117e-01, -1.3573e+00],
        [ 4.4608e-02,  3.0132e-01, -5.1676e-01, -3.6780e-01, -1.5477e+00,
         -4.6653e-01, -5.9240e-01, -1.6203e+00]])...
	Related Layers:
		- parent layers: cat_6_632
		- child layers: dropout_59_634
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (384, 384), (384,); 147840 params total (577.7 KB)
	Function: linear (grad_fn: ViewBackward0) 
	Computed inside module: blocks.5.sa.proj:1
	Time elapsed:  6.878E-04s
	Output of modules: blocks.5.sa.proj
	Output of bottom-level module: blocks.5.sa.proj:1
	Lookup keys: -12, 686, blocks.5.sa.proj, blocks.5.sa.proj:1, linear_160, linear_160:1, linear_160_633, linear_160_633:1
--------------------------------------------
Layer dropout_59_634, operation 639/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[ 5.3686e-01,  5.4806e-01, -2.4502e-01, -1.5462e-01, -7.8805e-02,
         -8.4949e-02, -4.5442e-01, -1.7063e+00],
        [-3.3756e-01,  5.7908e-01, -3.1094e-01, -1.9116e-01, -3.8597e-01,
         -1.3993e-01, -1.6212e-01, -1.3099e+00],
        [-2.5440e-01,  7.6965e-01, -3.1684e-01, -6.2696e-02, -4.2503e-01,
          1.0855e-03,  1.3028e-02, -1.2961e+00],
        [-1.4200e-01,  8.7421e-01, -3.1735e-01,  2.1611e-02, -3.3196e-01,
          3.9479e-02,  6.9101e-02, -1.3839e+00],
        [ 1.4830e-01,  9.5012e-01, -4.8464e-01,  6.8864e-02, -2.7391e-01,
          5.3687e-02, -5.1235e-02, -1.7861e+00],
        [ 5.8961e-02,  8.8118e-01,  1.3451e-01, -3.6019e-01,  1.2605e-01,
          3.6658e-01,  4.4092e-01, -1.4045e+00],
        [ 6.9889e-02,  6.1902e-01, -1.6388e-01, -5.0268e-01, -4.6146e-01,
          3.1289e-01, -1.4117e-01, -1.3573e+00],
        [ 4.4608e-02,  3.0132e-01, -5.1676e-01, -3.6780e-01, -1.5477e+00,
         -4.6653e-01, -5.9240e-01, -1.6203e+00]])...
	Related Layers:
		- parent layers: linear_160_633
		- child layers: add_7_635:1
		- shares parents with no other layers
		- shares children with layers: add_6_529:2
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: ViewBackward0) 
	Computed inside module: blocks.5.sa.dropout:1
	Time elapsed:  7.057E-05s
	Output of modules: blocks.5.sa.dropout, blocks.5.sa
	Output of bottom-level module: blocks.5.sa.dropout:1
	Lookup keys: -11, 687, blocks.5.sa, blocks.5.sa.dropout, blocks.5.sa.dropout:1, blocks.5.sa:1, dropout_59, dropout_59:1, dropout_59_634, dropout_59_634:1
--------------------------------------------
Layer add_7_635 (pass 1/2), operation 640/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[ -6.8352,  -3.6917,   3.0828,  -2.6639,   1.2965,   5.1685,   1.0222,
           3.4045],
        [ -4.6317,  -5.0179,   6.1758,  -1.4324,   0.9286,   3.3046,   2.0182,
           3.7684],
        [ -5.8079,  -6.8440,   5.2722,  -3.0378,  -3.3051,  -1.0662,  -0.0713,
           0.8884],
        [ -5.8433,  -6.7731,   7.8066,  -4.2363,  -1.5192,  -1.5029,   0.3175,
           2.7248],
        [ -3.5609,  -5.2557,   5.4819,  -4.1747,   4.1694,  -2.0422,   0.2757,
           3.1835],
        [ -0.9994,   0.6615,   6.3525,   0.5586,   2.5247,   7.9004,   3.1850,
           6.0968],
        [ -5.3878,   1.5067,   6.8775,  -1.4079,   3.6210,  10.1334,   2.5271,
          -0.1257],
        [-10.0179,  -3.3066,   6.0602,  -5.6387,   1.8239,  -2.2750,  -4.9939,
           2.5990]])...
	Related Layers:
		- parent layers: add_6_529:2, dropout_59_634
		- child layers: layernorm_12_636, add_7_635:2
		- shares parents with layers: layernorm_11_535
		- shares children with layers: dropout_60_640
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __add__ (grad_fn: AddBackward0) 
	Computed inside module: blocks.5:1
	Time elapsed:  7.844E-05s
	Output of modules: none
	Lookup keys: -10, 688, add_7:1, add_7_635:1
--------------------------------------------
Layer layernorm_12_636, operation 641/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-1.7620, -0.9640,  0.7239, -0.7354,  0.2828,  1.2771,  0.2047,  0.8302],
        [-1.0734, -1.1585,  1.4400, -0.3471,  0.2158,  0.7833,  0.4673,  0.8977],
        [-1.5170, -1.7813,  1.3802, -0.8172, -0.8993, -0.2830, -0.0312,  0.2366],
        [-1.4712, -1.6969,  1.9433, -1.0998, -0.4139, -0.3956,  0.0539,  0.6807],
        [-0.8749, -1.3007,  1.4158, -1.0621,  1.1071, -0.5020,  0.0832,  0.8499],
        [-0.1550,  0.0928,  0.9260,  0.0617,  0.3649,  1.1807,  0.4562,  0.9127],
        [-0.9262,  0.2830,  1.2127, -0.2476,  0.6566,  1.8132,  0.4496, -0.0124],
        [-2.5763, -0.8375,  1.5696, -1.4807,  0.4821, -0.5895, -1.3174,  0.6897]])...
	Related Layers:
		- parent layers: add_7_635:1
		- child layers: linear_161_637
		- shares parents with layers: add_7_635:2
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (384,), (384,); 768 params total (3.2 KB)
	Function: layer_norm (grad_fn: NativeLayerNormBackward0) 
	Computed inside module: blocks.5.ln2:1
	Time elapsed:  2.515E-04s
	Output of modules: blocks.5.ln2
	Output of bottom-level module: blocks.5.ln2:1
	Lookup keys: -9, 689, blocks.5.ln2, blocks.5.ln2:1, layernorm_12, layernorm_12:1, layernorm_12_636, layernorm_12_636:1
--------------------------------------------
Layer linear_161_637, operation 642/648:
	Output tensor: shape=1x11x1536(67584 bytes), dype=torch.float32, Tensor size=67728 bytes, Tensor storage size=67648
		tensor([[ 0.5345,  0.1362,  0.2022, -0.9447,  1.7178, -0.7901, -0.6492, -0.1461],
        [ 0.9410, -1.0513, -0.4842, -0.4352,  0.2386, -1.0316, -0.0731, -0.2152],
        [ 0.5688, -0.5450, -0.3539, -0.5802,  0.1941, -0.8777, -0.4092, -0.0920],
        [ 0.5906, -0.6489, -0.1402, -0.5920,  0.3160, -0.5612, -0.4683, -0.1200],
        [ 0.3342, -0.1198, -0.4313, -0.6340,  1.1829, -0.8643, -0.4037,  0.0602],
        [-0.4605,  0.0702, -0.7592, -1.4552, -3.3036, -0.2896, -0.3613, -0.3263],
        [-0.1701, -0.1275, -0.9048, -0.6219, -3.8940, -0.3761,  0.3559, -0.2406],
        [-0.4654, -0.6972, -0.8349, -0.1568, -0.3422, -1.2009,  0.1549, -0.2974]])...
	Related Layers:
		- parent layers: layernorm_12_636
		- child layers: relu_6_638
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (1536, 384), (1536,); 591360 params total (2.3 MB)
	Function: linear (grad_fn: ViewBackward0) 
	Computed inside module: blocks.5.ffwd.net.0:1
	Time elapsed:  7.100E-04s
	Output of modules: blocks.5.ffwd.net.0
	Output of bottom-level module: blocks.5.ffwd.net.0:1
	Lookup keys: -8, 690, blocks.5.ffwd.net.0, blocks.5.ffwd.net.0:1, linear_161, linear_161:1, linear_161_637, linear_161_637:1
--------------------------------------------
Layer relu_6_638, operation 643/648:
	Output tensor: shape=1x11x1536(67584 bytes), dype=torch.float32, Tensor size=67728 bytes, Tensor storage size=67648
		tensor([[0.5345, 0.1362, 0.2022, 0.0000, 1.7178, 0.0000, 0.0000, 0.0000],
        [0.9410, 0.0000, 0.0000, 0.0000, 0.2386, 0.0000, 0.0000, 0.0000],
        [0.5688, 0.0000, 0.0000, 0.0000, 0.1941, 0.0000, 0.0000, 0.0000],
        [0.5906, 0.0000, 0.0000, 0.0000, 0.3160, 0.0000, 0.0000, 0.0000],
        [0.3342, 0.0000, 0.0000, 0.0000, 1.1829, 0.0000, 0.0000, 0.0602],
        [0.0000, 0.0702, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3559, 0.0000],
        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1549, 0.0000]])...
	Related Layers:
		- parent layers: linear_161_637
		- child layers: linear_162_639
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: relu (grad_fn: ReluBackward0) 
	Computed inside module: blocks.5.ffwd.net.1:1
	Time elapsed:  9.298E-05s
	Output of modules: blocks.5.ffwd.net.1
	Output of bottom-level module: blocks.5.ffwd.net.1:1
	Lookup keys: -7, 691, blocks.5.ffwd.net.1, blocks.5.ffwd.net.1:1, relu_6, relu_6:1, relu_6_638, relu_6_638:1
--------------------------------------------
Layer linear_162_639, operation 644/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-0.9602, -0.0314,  0.8935,  0.6769,  0.4433, -0.7533,  0.5308, -0.0595],
        [-0.7673,  0.0335,  1.1506,  0.4478,  0.5636, -0.7049,  0.9278,  0.3228],
        [-0.6544, -0.1218,  0.9223,  0.5390,  0.1724, -0.6369,  0.5689,  0.0928],
        [-0.8070, -0.1667,  0.9551,  0.5028,  0.1266, -0.6572,  0.5650, -0.0506],
        [-0.8565, -0.3213,  0.8545,  0.7262,  0.2704, -0.7362,  0.5646, -0.0202],
        [-1.2138,  0.5487,  2.3251,  1.4979,  0.9105, -0.2990,  1.3125,  0.2326],
        [-1.0235,  0.5288,  1.9048,  1.4590,  0.8488, -0.8028,  1.0989, -0.1862],
        [-1.1081,  0.0202,  1.0018,  0.4137,  0.3581, -0.7266,  0.5844,  0.1427]])...
	Related Layers:
		- parent layers: relu_6_638
		- child layers: dropout_60_640
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (384, 1536), (384,); 590208 params total (2.3 MB)
	Function: linear (grad_fn: ViewBackward0) 
	Computed inside module: blocks.5.ffwd.net.2:1
	Time elapsed:  8.519E-04s
	Output of modules: blocks.5.ffwd.net.2
	Output of bottom-level module: blocks.5.ffwd.net.2:1
	Lookup keys: -6, 692, blocks.5.ffwd.net.2, blocks.5.ffwd.net.2:1, linear_162, linear_162:1, linear_162_639, linear_162_639:1
--------------------------------------------
Layer dropout_60_640, operation 645/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-0.9602, -0.0314,  0.8935,  0.6769,  0.4433, -0.7533,  0.5308, -0.0595],
        [-0.7673,  0.0335,  1.1506,  0.4478,  0.5636, -0.7049,  0.9278,  0.3228],
        [-0.6544, -0.1218,  0.9223,  0.5390,  0.1724, -0.6369,  0.5689,  0.0928],
        [-0.8070, -0.1667,  0.9551,  0.5028,  0.1266, -0.6572,  0.5650, -0.0506],
        [-0.8565, -0.3213,  0.8545,  0.7262,  0.2704, -0.7362,  0.5646, -0.0202],
        [-1.2138,  0.5487,  2.3251,  1.4979,  0.9105, -0.2990,  1.3125,  0.2326],
        [-1.0235,  0.5288,  1.9048,  1.4590,  0.8488, -0.8028,  1.0989, -0.1862],
        [-1.1081,  0.0202,  1.0018,  0.4137,  0.3581, -0.7266,  0.5844,  0.1427]])...
	Related Layers:
		- parent layers: linear_162_639
		- child layers: add_7_635:2
		- shares parents with no other layers
		- shares children with layers: add_7_635:1
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: dropout (grad_fn: ViewBackward0) 
	Computed inside module: blocks.5.ffwd.net.3:1
	Time elapsed:  6.747E-05s
	Output of modules: blocks.5.ffwd.net.3, blocks.5.ffwd.net, blocks.5.ffwd
	Output of bottom-level module: blocks.5.ffwd.net.3:1
	Lookup keys: -5, 693, blocks.5.ffwd, blocks.5.ffwd.net, blocks.5.ffwd.net.3, blocks.5.ffwd.net.3:1, blocks.5.ffwd.net:1, blocks.5.ffwd:1, dropout_60, dropout_60:1, dropout_60_640, dropout_60_640:1
--------------------------------------------
Layer add_7_635 (pass 2/2), operation 646/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[ -7.7954,  -3.7232,   3.9763,  -1.9870,   1.7398,   4.4151,   1.5529,
           3.3450],
        [ -5.3991,  -4.9843,   7.3264,  -0.9846,   1.4922,   2.5997,   2.9461,
           4.0912],
        [ -6.4623,  -6.9658,   6.1945,  -2.4988,  -3.1327,  -1.7031,   0.4976,
           0.9811],
        [ -6.6504,  -6.9398,   8.7617,  -3.7335,  -1.3927,  -2.1601,   0.8825,
           2.6742],
        [ -4.4175,  -5.5770,   6.3364,  -3.4485,   4.4398,  -2.7784,   0.8403,
           3.1634],
        [ -2.2133,   1.2102,   8.6776,   2.0565,   3.4352,   7.6015,   4.4976,
           6.3294],
        [ -6.4113,   2.0356,   8.7824,   0.0510,   4.4698,   9.3306,   3.6260,
          -0.3119],
        [-11.1260,  -3.2864,   7.0620,  -5.2250,   2.1820,  -3.0015,  -4.4094,
           2.7417]])...
	Related Layers:
		- parent layers: add_7_635:1, dropout_60_640
		- child layers: layernorm_13_641
		- shares parents with layers: layernorm_12_636
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: __add__ (grad_fn: AddBackward0) 
	Computed inside module: blocks.5:1
	Time elapsed:  7.224E-05s
	Output of modules: blocks.5, blocks
	Lookup keys: -4, 694, add_7:2, add_7_635:2, blocks, blocks.5, blocks.5:1, blocks:1
--------------------------------------------
Layer layernorm_13_641, operation 647/648:
	Output tensor: shape=1x11x384(16896 bytes), dype=torch.float32, Tensor size=17040 bytes, Tensor storage size=16960
		tensor([[-1.9488, -0.9487,  0.9129, -0.5258,  0.3670,  1.0052,  0.3262,  0.7505],
        [-1.1964, -1.0912,  1.6026, -0.2171,  0.3209,  0.5578,  0.6414,  0.8844],
        [-1.6398, -1.7469,  1.5484, -0.6268, -0.7904, -0.4349,  0.1208,  0.2345],
        [-1.6134, -1.6636,  2.0634, -0.8978, -0.3513, -0.5341,  0.1917,  0.6084],
        [-1.0576, -1.3212,  1.5419, -0.8047,  1.0780, -0.6518,  0.2203,  0.7692],
        [-0.3302,  0.1452,  1.1768,  0.2609,  0.4515,  1.0153,  0.6003,  0.8442],
        [-1.0451,  0.3339,  1.4373,  0.0149,  0.7301,  1.5093,  0.5971, -0.0460],
        [-2.7315, -0.8009,  1.7257, -1.2612,  0.5291, -0.7316, -1.0660,  0.6623]])...
	Related Layers:
		- parent layers: add_7_635:2
		- child layers: linear_163_642
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (384,), (384,); 768 params total (3.2 KB)
	Function: layer_norm (grad_fn: NativeLayerNormBackward0) 
	Computed inside module: ln_f:1
	Time elapsed:  2.451E-04s
	Output of modules: ln_f
	Output of bottom-level module: ln_f:1
	Lookup keys: -3, 695, layernorm_13, layernorm_13:1, layernorm_13_641, layernorm_13_641:1, ln_f, ln_f:1
--------------------------------------------
Layer linear_163_642, operation 648/648:
	Output tensor: shape=1x11x65(2860 bytes), dype=torch.float32, Tensor size=3004 bytes, Tensor storage size=2924
		tensor([[ 2.7197e+00,  4.2629e+00,  1.2453e+00, -3.7204e+00, -3.4936e+00,
          1.1835e+00,  3.0254e+00,  6.0188e-02],
        [-1.4723e-02,  2.9527e+00, -9.3466e-01, -4.9663e+00, -4.6857e+00,
          4.1045e-03,  1.8932e+00, -1.9988e+00],
        [ 1.7833e+00,  4.1853e+00,  1.3373e+00, -4.4885e+00, -4.4758e+00,
          8.7668e-01,  3.7797e+00, -9.2070e-01],
        [ 1.7647e+00,  4.3340e+00,  1.6018e+00, -4.6313e+00, -4.4969e+00,
          8.0444e-01,  3.9790e+00, -9.7948e-01],
        [ 2.7880e+00,  5.8300e+00,  2.6504e+00, -3.6829e+00, -4.2087e+00,
          1.8276e+00,  5.0784e+00,  1.4971e-01],
        [-2.6657e+00, -2.1048e+00, -5.4452e+00, -5.6371e+00, -5.0538e+00,
         -2.3767e-01, -3.5281e+00, -3.8668e+00],
        [-3.5114e+00, -1.1015e+00, -3.6754e+00, -5.0050e+00, -4.5147e+00,
         -4.3873e-01, -2.1108e+00, -2.9207e+00],
        [ 4.5374e-01,  3.9169e+00,  2.2722e-01, -4.5160e+00, -4.5365e+00,
          8.9338e-01,  2.2544e+00, -7.0294e-01]])...
	Related Layers:
		- parent layers: layernorm_13_641
		- child layers: output_1
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: Computed from params with shape (65, 384), (65,); 25025 params total (98.0 KB)
	Function: linear (grad_fn: ViewBackward0) 
	Computed inside module: lm_head:1
	Time elapsed:  2.921E-04s
	Output of modules: lm_head
	Output of bottom-level module: lm_head:1
	Lookup keys: -2, 696, linear_163, linear_163:1, linear_163_642, linear_163_642:1, lm_head, lm_head:1
--------------------------------------------
Layer output_1, operation 648/648:
	Output tensor: shape=1x11x65(2860 bytes), dype=torch.float32, Tensor size=3004 bytes, Tensor storage size=2924
		tensor([[ 2.7197e+00,  4.2629e+00,  1.2453e+00, -3.7204e+00, -3.4936e+00,
          1.1835e+00,  3.0254e+00,  6.0188e-02],
        [-1.4723e-02,  2.9527e+00, -9.3466e-01, -4.9663e+00, -4.6857e+00,
          4.1045e-03,  1.8932e+00, -1.9988e+00],
        [ 1.7833e+00,  4.1853e+00,  1.3373e+00, -4.4885e+00, -4.4758e+00,
          8.7668e-01,  3.7797e+00, -9.2070e-01],
        [ 1.7647e+00,  4.3340e+00,  1.6018e+00, -4.6313e+00, -4.4969e+00,
          8.0444e-01,  3.9790e+00, -9.7948e-01],
        [ 2.7880e+00,  5.8300e+00,  2.6504e+00, -3.6829e+00, -4.2087e+00,
          1.8276e+00,  5.0784e+00,  1.4971e-01],
        [-2.6657e+00, -2.1048e+00, -5.4452e+00, -5.6371e+00, -5.0538e+00,
         -2.3767e-01, -3.5281e+00, -3.8668e+00],
        [-3.5114e+00, -1.1015e+00, -3.6754e+00, -5.0050e+00, -4.5147e+00,
         -4.3873e-01, -2.1108e+00, -2.9207e+00],
        [ 4.5374e-01,  3.9169e+00,  2.2722e-01, -4.5160e+00, -4.5365e+00,
          8.9338e-01,  2.2544e+00, -7.0294e-01]])...
	Related Layers:
		- parent layers: linear_163_642
		- no child layers
		- shares parents with no other layers
		- shares children with no other layers
		- descendent of input layers: input_1
		- ancestor of output layers: output_1
	Params: no params used
	Function: none (grad_fn: None) 
	Computed inside module: not computed inside a module
	Time elapsed:  0.000E+00s
	Output of modules: lm_head
	Lookup keys: -1, 697, lm_head, lm_head:1, output.0, output_1, output_1, output_1:1, output_1:1
--------------------------------------------


| node_title        | tensor_shape   |   shape size |   Tensor size |   Tensor storage size | input_dim               | node_addr                   |   CXL_RD |   CXL_WR | Input layer size details                                                                                                                                        |
|-------------------|----------------|--------------|---------------|-----------------------|-------------------------|-----------------------------|----------|----------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------|
| input_1           | 1x11           |           88 |           216 |                     0 |                         | input.idx                   |        0 |       88 |                                                                                                                                                                 |
| embedding_1_1     | 1x11x384       |        16896 |         17040 |                 16960 | params: 65x384          | token_embedding_table       |       88 |    16896 | input_1(88)                                                                                                                                                     |
| arange_1_2        | x11            |           88 |           200 |                   152 |                         |                             |        0 |       88 |                                                                                                                                                                 |
| embedding_2_3     | 11x384         |        16896 |         17024 |                 16960 | params: 256x384         | position_embedding_table    |       88 |    16896 | arange_1_2(88)                                                                                                                                                  |
| add_1_4           | 1x11x384       |        16896 |         17040 |                 16960 |                         |                             |    33792 |    16896 | embedding_1_1(16896) embedding_2_3(16896)                                                                                                                       |
| layernorm_1_5     | 1x11x384       |        16896 |         17040 |                 16960 | params: x384, x384      | blocks.0.ln1                |    16896 |    16896 | add_1_4(16896)                                                                                                                                                  |
| linear_1_6        | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.0.sa.heads.0.key     |    16896 |     2112 | layernorm_1_5(16896)                                                                                                                                            |
| linear_2_7        | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.0.sa.heads.0.query   |    16896 |     2112 | layernorm_1_5(16896)                                                                                                                                            |
| transpose_1_8     | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_1_6(2112)                                                                                                                                                |
| matmul_1_9        | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_2_7(2112) transpose_1_8(2112)                                                                                                                            |
| mul_1_10          | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_1_9(484)                                                                                                                                                 |
| getitem_1_11      | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_1(262144)                                                                                                                                                |
| eq_1_12           | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_1_11(484)                                                                                                                                               |
| maskedfill_1_13   | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_1_10(484) eq_1_12(121)                                                                                                                                      |
| softmax_1_14      | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_1_13(484)                                                                                                                                            |
| dropout_1_15      | 1x11x11        |          484 |           628 |                   548 |                         | blocks.0.sa.heads.0.dropout |      484 |      484 | softmax_1_14(484)                                                                                                                                               |
| linear_3_16       | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.0.sa.heads.0.value   |    16896 |     2112 | layernorm_1_5(16896)                                                                                                                                            |
| matmul_2_17       | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.0.sa.heads.0         |     2596 |     2112 | dropout_1_15(484) linear_3_16(2112)                                                                                                                             |
| linear_4_18       | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.0.sa.heads.1.key     |    16896 |     2112 | layernorm_1_5(16896)                                                                                                                                            |
| linear_5_19       | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.0.sa.heads.1.query   |    16896 |     2112 | layernorm_1_5(16896)                                                                                                                                            |
| transpose_2_20    | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_4_18(2112)                                                                                                                                               |
| matmul_3_21       | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_5_19(2112) transpose_2_20(2112)                                                                                                                          |
| mul_2_22          | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_3_21(484)                                                                                                                                                |
| getitem_2_23      | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_2(262144)                                                                                                                                                |
| eq_2_24           | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_2_23(484)                                                                                                                                               |
| maskedfill_2_25   | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_2_22(484) eq_2_24(121)                                                                                                                                      |
| softmax_2_26      | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_2_25(484)                                                                                                                                            |
| dropout_2_27      | 1x11x11        |          484 |           628 |                   548 |                         | blocks.0.sa.heads.1.dropout |      484 |      484 | softmax_2_26(484)                                                                                                                                               |
| linear_6_28       | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.0.sa.heads.1.value   |    16896 |     2112 | layernorm_1_5(16896)                                                                                                                                            |
| matmul_4_29       | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.0.sa.heads.1         |     2596 |     2112 | dropout_2_27(484) linear_6_28(2112)                                                                                                                             |
| linear_7_30       | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.0.sa.heads.2.key     |    16896 |     2112 | layernorm_1_5(16896)                                                                                                                                            |
| linear_8_31       | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.0.sa.heads.2.query   |    16896 |     2112 | layernorm_1_5(16896)                                                                                                                                            |
| transpose_3_32    | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_7_30(2112)                                                                                                                                               |
| matmul_5_33       | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_8_31(2112) transpose_3_32(2112)                                                                                                                          |
| mul_3_34          | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_5_33(484)                                                                                                                                                |
| getitem_3_35      | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_3(262144)                                                                                                                                                |
| eq_3_36           | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_3_35(484)                                                                                                                                               |
| maskedfill_3_37   | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_3_34(484) eq_3_36(121)                                                                                                                                      |
| softmax_3_38      | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_3_37(484)                                                                                                                                            |
| dropout_3_39      | 1x11x11        |          484 |           628 |                   548 |                         | blocks.0.sa.heads.2.dropout |      484 |      484 | softmax_3_38(484)                                                                                                                                               |
| linear_9_40       | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.0.sa.heads.2.value   |    16896 |     2112 | layernorm_1_5(16896)                                                                                                                                            |
| matmul_6_41       | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.0.sa.heads.2         |     2596 |     2112 | dropout_3_39(484) linear_9_40(2112)                                                                                                                             |
| linear_10_42      | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.0.sa.heads.3.key     |    16896 |     2112 | layernorm_1_5(16896)                                                                                                                                            |
| linear_11_43      | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.0.sa.heads.3.query   |    16896 |     2112 | layernorm_1_5(16896)                                                                                                                                            |
| transpose_4_44    | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_10_42(2112)                                                                                                                                              |
| matmul_7_45       | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_11_43(2112) transpose_4_44(2112)                                                                                                                         |
| mul_4_46          | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_7_45(484)                                                                                                                                                |
| getitem_4_47      | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_4(262144)                                                                                                                                                |
| eq_4_48           | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_4_47(484)                                                                                                                                               |
| maskedfill_4_49   | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_4_46(484) eq_4_48(121)                                                                                                                                      |
| softmax_4_50      | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_4_49(484)                                                                                                                                            |
| dropout_4_51      | 1x11x11        |          484 |           628 |                   548 |                         | blocks.0.sa.heads.3.dropout |      484 |      484 | softmax_4_50(484)                                                                                                                                               |
| linear_12_52      | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.0.sa.heads.3.value   |    16896 |     2112 | layernorm_1_5(16896)                                                                                                                                            |
| matmul_8_53       | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.0.sa.heads.3         |     2596 |     2112 | dropout_4_51(484) linear_12_52(2112)                                                                                                                            |
| linear_13_54      | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.0.sa.heads.4.key     |    16896 |     2112 | layernorm_1_5(16896)                                                                                                                                            |
| linear_14_55      | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.0.sa.heads.4.query   |    16896 |     2112 | layernorm_1_5(16896)                                                                                                                                            |
| transpose_5_56    | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_13_54(2112)                                                                                                                                              |
| matmul_9_57       | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_14_55(2112) transpose_5_56(2112)                                                                                                                         |
| mul_5_58          | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_9_57(484)                                                                                                                                                |
| getitem_5_59      | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_5(262144)                                                                                                                                                |
| eq_5_60           | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_5_59(484)                                                                                                                                               |
| maskedfill_5_61   | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_5_58(484) eq_5_60(121)                                                                                                                                      |
| softmax_5_62      | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_5_61(484)                                                                                                                                            |
| dropout_5_63      | 1x11x11        |          484 |           628 |                   548 |                         | blocks.0.sa.heads.4.dropout |      484 |      484 | softmax_5_62(484)                                                                                                                                               |
| linear_15_64      | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.0.sa.heads.4.value   |    16896 |     2112 | layernorm_1_5(16896)                                                                                                                                            |
| matmul_10_65      | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.0.sa.heads.4         |     2596 |     2112 | dropout_5_63(484) linear_15_64(2112)                                                                                                                            |
| linear_16_66      | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.0.sa.heads.5.key     |    16896 |     2112 | layernorm_1_5(16896)                                                                                                                                            |
| linear_17_67      | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.0.sa.heads.5.query   |    16896 |     2112 | layernorm_1_5(16896)                                                                                                                                            |
| transpose_6_68    | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_16_66(2112)                                                                                                                                              |
| matmul_11_69      | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_17_67(2112) transpose_6_68(2112)                                                                                                                         |
| mul_6_70          | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_11_69(484)                                                                                                                                               |
| getitem_6_71      | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_6(262144)                                                                                                                                                |
| eq_6_72           | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_6_71(484)                                                                                                                                               |
| maskedfill_6_73   | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_6_70(484) eq_6_72(121)                                                                                                                                      |
| softmax_6_74      | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_6_73(484)                                                                                                                                            |
| dropout_6_75      | 1x11x11        |          484 |           628 |                   548 |                         | blocks.0.sa.heads.5.dropout |      484 |      484 | softmax_6_74(484)                                                                                                                                               |
| linear_18_76      | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.0.sa.heads.5.value   |    16896 |     2112 | layernorm_1_5(16896)                                                                                                                                            |
| matmul_12_77      | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.0.sa.heads.5         |     2596 |     2112 | dropout_6_75(484) linear_18_76(2112)                                                                                                                            |
| linear_19_78      | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.0.sa.heads.6.key     |    16896 |     2112 | layernorm_1_5(16896)                                                                                                                                            |
| linear_20_79      | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.0.sa.heads.6.query   |    16896 |     2112 | layernorm_1_5(16896)                                                                                                                                            |
| transpose_7_80    | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_19_78(2112)                                                                                                                                              |
| matmul_13_81      | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_20_79(2112) transpose_7_80(2112)                                                                                                                         |
| mul_7_82          | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_13_81(484)                                                                                                                                               |
| getitem_7_83      | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_7(262144)                                                                                                                                                |
| eq_7_84           | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_7_83(484)                                                                                                                                               |
| maskedfill_7_85   | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_7_82(484) eq_7_84(121)                                                                                                                                      |
| softmax_7_86      | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_7_85(484)                                                                                                                                            |
| dropout_7_87      | 1x11x11        |          484 |           628 |                   548 |                         | blocks.0.sa.heads.6.dropout |      484 |      484 | softmax_7_86(484)                                                                                                                                               |
| linear_21_88      | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.0.sa.heads.6.value   |    16896 |     2112 | layernorm_1_5(16896)                                                                                                                                            |
| matmul_14_89      | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.0.sa.heads.6         |     2596 |     2112 | dropout_7_87(484) linear_21_88(2112)                                                                                                                            |
| linear_22_90      | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.0.sa.heads.7.key     |    16896 |     2112 | layernorm_1_5(16896)                                                                                                                                            |
| linear_23_91      | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.0.sa.heads.7.query   |    16896 |     2112 | layernorm_1_5(16896)                                                                                                                                            |
| transpose_8_92    | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_22_90(2112)                                                                                                                                              |
| matmul_15_93      | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_23_91(2112) transpose_8_92(2112)                                                                                                                         |
| mul_8_94          | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_15_93(484)                                                                                                                                               |
| getitem_8_95      | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_8(262144)                                                                                                                                                |
| eq_8_96           | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_8_95(484)                                                                                                                                               |
| maskedfill_8_97   | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_8_94(484) eq_8_96(121)                                                                                                                                      |
| softmax_8_98      | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_8_97(484)                                                                                                                                            |
| dropout_8_99      | 1x11x11        |          484 |           628 |                   548 |                         | blocks.0.sa.heads.7.dropout |      484 |      484 | softmax_8_98(484)                                                                                                                                               |
| linear_24_100     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.0.sa.heads.7.value   |    16896 |     2112 | layernorm_1_5(16896)                                                                                                                                            |
| matmul_16_101     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.0.sa.heads.7         |     2596 |     2112 | dropout_8_99(484) linear_24_100(2112)                                                                                                                           |
| cat_1_102         | 1x11x384       |        16896 |         17040 |                 16960 |                         |                             |    16896 |    16896 | matmul_2_17(2112) matmul_4_29(2112) matmul_6_41(2112) matmul_8_53(2112) matmul_10_65(2112) matmul_12_77(2112) matmul_14_89(2112) matmul_16_101(2112)            |
| linear_25_103     | 1x11x384       |        16896 |         17040 |                 16960 | params: 384x384, x384   | blocks.0.sa.proj            |    16896 |    16896 | cat_1_102(16896)                                                                                                                                                |
| dropout_9_104     | 1x11x384       |        16896 |         17040 |                 16960 |                         | blocks.0.sa.dropout         |    16896 |    16896 | linear_25_103(16896)                                                                                                                                            |
| add_2_105         | 1x11x384       |        16896 |         17040 |                 16960 |                         |                             |    33792 |    16896 | add_1_4(16896) dropout_9_104(16896)                                                                                                                             |
| layernorm_2_106   | 1x11x384       |        16896 |         17040 |                 16960 | params: x384, x384      | blocks.0.ln2                |    16896 |    16896 | add_2_105:1(16896)                                                                                                                                              |
| linear_26_107     | 1x11x1536      |        67584 |         67728 |                 67648 | params: 1536x384, x1536 | blocks.0.ffwd.net.0         |    16896 |    67584 | layernorm_2_106(16896)                                                                                                                                          |
| relu_1_108        | 1x11x1536      |        67584 |         67728 |                 67648 |                         | blocks.0.ffwd.net.1         |    67584 |    67584 | linear_26_107(67584)                                                                                                                                            |
| linear_27_109     | 1x11x384       |        16896 |         17040 |                 16960 | params: 384x1536, x384  | blocks.0.ffwd.net.2         |    67584 |    16896 | relu_1_108(67584)                                                                                                                                               |
| dropout_10_110    | 1x11x384       |        16896 |         17040 |                 16960 |                         | blocks.0.ffwd.net.3         |    16896 |    16896 | linear_27_109(16896)                                                                                                                                            |
| add_2_105         | 1x11x384       |        16896 |         17040 |                 16960 |                         |                             |    33792 |    16896 | add_2_105:1(16896) dropout_10_110(16896)                                                                                                                        |
| layernorm_3_111   | 1x11x384       |        16896 |         17040 |                 16960 | params: x384, x384      | blocks.1.ln1                |    16896 |    16896 | add_2_105:2(16896)                                                                                                                                              |
| linear_28_112     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.1.sa.heads.0.key     |    16896 |     2112 | layernorm_3_111(16896)                                                                                                                                          |
| linear_29_113     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.1.sa.heads.0.query   |    16896 |     2112 | layernorm_3_111(16896)                                                                                                                                          |
| transpose_9_114   | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_28_112(2112)                                                                                                                                             |
| matmul_17_115     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_29_113(2112) transpose_9_114(2112)                                                                                                                       |
| mul_9_116         | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_17_115(484)                                                                                                                                              |
| getitem_9_117     | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_9(262144)                                                                                                                                                |
| eq_9_118          | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_9_117(484)                                                                                                                                              |
| maskedfill_9_119  | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_9_116(484) eq_9_118(121)                                                                                                                                    |
| softmax_9_120     | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_9_119(484)                                                                                                                                           |
| dropout_11_121    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.1.sa.heads.0.dropout |      484 |      484 | softmax_9_120(484)                                                                                                                                              |
| linear_30_122     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.1.sa.heads.0.value   |    16896 |     2112 | layernorm_3_111(16896)                                                                                                                                          |
| matmul_18_123     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.1.sa.heads.0         |     2596 |     2112 | dropout_11_121(484) linear_30_122(2112)                                                                                                                         |
| linear_31_124     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.1.sa.heads.1.key     |    16896 |     2112 | layernorm_3_111(16896)                                                                                                                                          |
| linear_32_125     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.1.sa.heads.1.query   |    16896 |     2112 | layernorm_3_111(16896)                                                                                                                                          |
| transpose_10_126  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_31_124(2112)                                                                                                                                             |
| matmul_19_127     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_32_125(2112) transpose_10_126(2112)                                                                                                                      |
| mul_10_128        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_19_127(484)                                                                                                                                              |
| getitem_10_129    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_10(262144)                                                                                                                                               |
| eq_10_130         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_10_129(484)                                                                                                                                             |
| maskedfill_10_131 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_10_128(484) eq_10_130(121)                                                                                                                                  |
| softmax_10_132    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_10_131(484)                                                                                                                                          |
| dropout_12_133    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.1.sa.heads.1.dropout |      484 |      484 | softmax_10_132(484)                                                                                                                                             |
| linear_33_134     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.1.sa.heads.1.value   |    16896 |     2112 | layernorm_3_111(16896)                                                                                                                                          |
| matmul_20_135     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.1.sa.heads.1         |     2596 |     2112 | dropout_12_133(484) linear_33_134(2112)                                                                                                                         |
| linear_34_136     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.1.sa.heads.2.key     |    16896 |     2112 | layernorm_3_111(16896)                                                                                                                                          |
| linear_35_137     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.1.sa.heads.2.query   |    16896 |     2112 | layernorm_3_111(16896)                                                                                                                                          |
| transpose_11_138  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_34_136(2112)                                                                                                                                             |
| matmul_21_139     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_35_137(2112) transpose_11_138(2112)                                                                                                                      |
| mul_11_140        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_21_139(484)                                                                                                                                              |
| getitem_11_141    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_11(262144)                                                                                                                                               |
| eq_11_142         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_11_141(484)                                                                                                                                             |
| maskedfill_11_143 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_11_140(484) eq_11_142(121)                                                                                                                                  |
| softmax_11_144    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_11_143(484)                                                                                                                                          |
| dropout_13_145    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.1.sa.heads.2.dropout |      484 |      484 | softmax_11_144(484)                                                                                                                                             |
| linear_36_146     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.1.sa.heads.2.value   |    16896 |     2112 | layernorm_3_111(16896)                                                                                                                                          |
| matmul_22_147     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.1.sa.heads.2         |     2596 |     2112 | dropout_13_145(484) linear_36_146(2112)                                                                                                                         |
| linear_37_148     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.1.sa.heads.3.key     |    16896 |     2112 | layernorm_3_111(16896)                                                                                                                                          |
| linear_38_149     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.1.sa.heads.3.query   |    16896 |     2112 | layernorm_3_111(16896)                                                                                                                                          |
| transpose_12_150  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_37_148(2112)                                                                                                                                             |
| matmul_23_151     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_38_149(2112) transpose_12_150(2112)                                                                                                                      |
| mul_12_152        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_23_151(484)                                                                                                                                              |
| getitem_12_153    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_12(262144)                                                                                                                                               |
| eq_12_154         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_12_153(484)                                                                                                                                             |
| maskedfill_12_155 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_12_152(484) eq_12_154(121)                                                                                                                                  |
| softmax_12_156    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_12_155(484)                                                                                                                                          |
| dropout_14_157    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.1.sa.heads.3.dropout |      484 |      484 | softmax_12_156(484)                                                                                                                                             |
| linear_39_158     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.1.sa.heads.3.value   |    16896 |     2112 | layernorm_3_111(16896)                                                                                                                                          |
| matmul_24_159     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.1.sa.heads.3         |     2596 |     2112 | dropout_14_157(484) linear_39_158(2112)                                                                                                                         |
| linear_40_160     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.1.sa.heads.4.key     |    16896 |     2112 | layernorm_3_111(16896)                                                                                                                                          |
| linear_41_161     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.1.sa.heads.4.query   |    16896 |     2112 | layernorm_3_111(16896)                                                                                                                                          |
| transpose_13_162  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_40_160(2112)                                                                                                                                             |
| matmul_25_163     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_41_161(2112) transpose_13_162(2112)                                                                                                                      |
| mul_13_164        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_25_163(484)                                                                                                                                              |
| getitem_13_165    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_13(262144)                                                                                                                                               |
| eq_13_166         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_13_165(484)                                                                                                                                             |
| maskedfill_13_167 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_13_164(484) eq_13_166(121)                                                                                                                                  |
| softmax_13_168    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_13_167(484)                                                                                                                                          |
| dropout_15_169    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.1.sa.heads.4.dropout |      484 |      484 | softmax_13_168(484)                                                                                                                                             |
| linear_42_170     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.1.sa.heads.4.value   |    16896 |     2112 | layernorm_3_111(16896)                                                                                                                                          |
| matmul_26_171     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.1.sa.heads.4         |     2596 |     2112 | dropout_15_169(484) linear_42_170(2112)                                                                                                                         |
| linear_43_172     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.1.sa.heads.5.key     |    16896 |     2112 | layernorm_3_111(16896)                                                                                                                                          |
| linear_44_173     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.1.sa.heads.5.query   |    16896 |     2112 | layernorm_3_111(16896)                                                                                                                                          |
| transpose_14_174  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_43_172(2112)                                                                                                                                             |
| matmul_27_175     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_44_173(2112) transpose_14_174(2112)                                                                                                                      |
| mul_14_176        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_27_175(484)                                                                                                                                              |
| getitem_14_177    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_14(262144)                                                                                                                                               |
| eq_14_178         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_14_177(484)                                                                                                                                             |
| maskedfill_14_179 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_14_176(484) eq_14_178(121)                                                                                                                                  |
| softmax_14_180    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_14_179(484)                                                                                                                                          |
| dropout_16_181    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.1.sa.heads.5.dropout |      484 |      484 | softmax_14_180(484)                                                                                                                                             |
| linear_45_182     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.1.sa.heads.5.value   |    16896 |     2112 | layernorm_3_111(16896)                                                                                                                                          |
| matmul_28_183     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.1.sa.heads.5         |     2596 |     2112 | dropout_16_181(484) linear_45_182(2112)                                                                                                                         |
| linear_46_184     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.1.sa.heads.6.key     |    16896 |     2112 | layernorm_3_111(16896)                                                                                                                                          |
| linear_47_185     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.1.sa.heads.6.query   |    16896 |     2112 | layernorm_3_111(16896)                                                                                                                                          |
| transpose_15_186  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_46_184(2112)                                                                                                                                             |
| matmul_29_187     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_47_185(2112) transpose_15_186(2112)                                                                                                                      |
| mul_15_188        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_29_187(484)                                                                                                                                              |
| getitem_15_189    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_15(262144)                                                                                                                                               |
| eq_15_190         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_15_189(484)                                                                                                                                             |
| maskedfill_15_191 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_15_188(484) eq_15_190(121)                                                                                                                                  |
| softmax_15_192    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_15_191(484)                                                                                                                                          |
| dropout_17_193    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.1.sa.heads.6.dropout |      484 |      484 | softmax_15_192(484)                                                                                                                                             |
| linear_48_194     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.1.sa.heads.6.value   |    16896 |     2112 | layernorm_3_111(16896)                                                                                                                                          |
| matmul_30_195     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.1.sa.heads.6         |     2596 |     2112 | dropout_17_193(484) linear_48_194(2112)                                                                                                                         |
| linear_49_196     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.1.sa.heads.7.key     |    16896 |     2112 | layernorm_3_111(16896)                                                                                                                                          |
| linear_50_197     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.1.sa.heads.7.query   |    16896 |     2112 | layernorm_3_111(16896)                                                                                                                                          |
| transpose_16_198  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_49_196(2112)                                                                                                                                             |
| matmul_31_199     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_50_197(2112) transpose_16_198(2112)                                                                                                                      |
| mul_16_200        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_31_199(484)                                                                                                                                              |
| getitem_16_201    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_16(262144)                                                                                                                                               |
| eq_16_202         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_16_201(484)                                                                                                                                             |
| maskedfill_16_203 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_16_200(484) eq_16_202(121)                                                                                                                                  |
| softmax_16_204    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_16_203(484)                                                                                                                                          |
| dropout_18_205    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.1.sa.heads.7.dropout |      484 |      484 | softmax_16_204(484)                                                                                                                                             |
| linear_51_206     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.1.sa.heads.7.value   |    16896 |     2112 | layernorm_3_111(16896)                                                                                                                                          |
| matmul_32_207     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.1.sa.heads.7         |     2596 |     2112 | dropout_18_205(484) linear_51_206(2112)                                                                                                                         |
| cat_2_208         | 1x11x384       |        16896 |         17040 |                 16960 |                         |                             |    16896 |    16896 | matmul_18_123(2112) matmul_20_135(2112) matmul_22_147(2112) matmul_24_159(2112) matmul_26_171(2112) matmul_28_183(2112) matmul_30_195(2112) matmul_32_207(2112) |
| linear_52_209     | 1x11x384       |        16896 |         17040 |                 16960 | params: 384x384, x384   | blocks.1.sa.proj            |    16896 |    16896 | cat_2_208(16896)                                                                                                                                                |
| dropout_19_210    | 1x11x384       |        16896 |         17040 |                 16960 |                         | blocks.1.sa.dropout         |    16896 |    16896 | linear_52_209(16896)                                                                                                                                            |
| add_3_211         | 1x11x384       |        16896 |         17040 |                 16960 |                         |                             |    33792 |    16896 | add_2_105:2(16896) dropout_19_210(16896)                                                                                                                        |
| layernorm_4_212   | 1x11x384       |        16896 |         17040 |                 16960 | params: x384, x384      | blocks.1.ln2                |    16896 |    16896 | add_3_211:1(16896)                                                                                                                                              |
| linear_53_213     | 1x11x1536      |        67584 |         67728 |                 67648 | params: 1536x384, x1536 | blocks.1.ffwd.net.0         |    16896 |    67584 | layernorm_4_212(16896)                                                                                                                                          |
| relu_2_214        | 1x11x1536      |        67584 |         67728 |                 67648 |                         | blocks.1.ffwd.net.1         |    67584 |    67584 | linear_53_213(67584)                                                                                                                                            |
| linear_54_215     | 1x11x384       |        16896 |         17040 |                 16960 | params: 384x1536, x384  | blocks.1.ffwd.net.2         |    67584 |    16896 | relu_2_214(67584)                                                                                                                                               |
| dropout_20_216    | 1x11x384       |        16896 |         17040 |                 16960 |                         | blocks.1.ffwd.net.3         |    16896 |    16896 | linear_54_215(16896)                                                                                                                                            |
| add_3_211         | 1x11x384       |        16896 |         17040 |                 16960 |                         |                             |    33792 |    16896 | add_3_211:1(16896) dropout_20_216(16896)                                                                                                                        |
| layernorm_5_217   | 1x11x384       |        16896 |         17040 |                 16960 | params: x384, x384      | blocks.2.ln1                |    16896 |    16896 | add_3_211:2(16896)                                                                                                                                              |
| linear_55_218     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.2.sa.heads.0.key     |    16896 |     2112 | layernorm_5_217(16896)                                                                                                                                          |
| linear_56_219     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.2.sa.heads.0.query   |    16896 |     2112 | layernorm_5_217(16896)                                                                                                                                          |
| transpose_17_220  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_55_218(2112)                                                                                                                                             |
| matmul_33_221     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_56_219(2112) transpose_17_220(2112)                                                                                                                      |
| mul_17_222        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_33_221(484)                                                                                                                                              |
| getitem_17_223    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_17(262144)                                                                                                                                               |
| eq_17_224         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_17_223(484)                                                                                                                                             |
| maskedfill_17_225 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_17_222(484) eq_17_224(121)                                                                                                                                  |
| softmax_17_226    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_17_225(484)                                                                                                                                          |
| dropout_21_227    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.2.sa.heads.0.dropout |      484 |      484 | softmax_17_226(484)                                                                                                                                             |
| linear_57_228     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.2.sa.heads.0.value   |    16896 |     2112 | layernorm_5_217(16896)                                                                                                                                          |
| matmul_34_229     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.2.sa.heads.0         |     2596 |     2112 | dropout_21_227(484) linear_57_228(2112)                                                                                                                         |
| linear_58_230     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.2.sa.heads.1.key     |    16896 |     2112 | layernorm_5_217(16896)                                                                                                                                          |
| linear_59_231     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.2.sa.heads.1.query   |    16896 |     2112 | layernorm_5_217(16896)                                                                                                                                          |
| transpose_18_232  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_58_230(2112)                                                                                                                                             |
| matmul_35_233     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_59_231(2112) transpose_18_232(2112)                                                                                                                      |
| mul_18_234        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_35_233(484)                                                                                                                                              |
| getitem_18_235    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_18(262144)                                                                                                                                               |
| eq_18_236         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_18_235(484)                                                                                                                                             |
| maskedfill_18_237 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_18_234(484) eq_18_236(121)                                                                                                                                  |
| softmax_18_238    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_18_237(484)                                                                                                                                          |
| dropout_22_239    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.2.sa.heads.1.dropout |      484 |      484 | softmax_18_238(484)                                                                                                                                             |
| linear_60_240     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.2.sa.heads.1.value   |    16896 |     2112 | layernorm_5_217(16896)                                                                                                                                          |
| matmul_36_241     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.2.sa.heads.1         |     2596 |     2112 | dropout_22_239(484) linear_60_240(2112)                                                                                                                         |
| linear_61_242     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.2.sa.heads.2.key     |    16896 |     2112 | layernorm_5_217(16896)                                                                                                                                          |
| linear_62_243     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.2.sa.heads.2.query   |    16896 |     2112 | layernorm_5_217(16896)                                                                                                                                          |
| transpose_19_244  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_61_242(2112)                                                                                                                                             |
| matmul_37_245     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_62_243(2112) transpose_19_244(2112)                                                                                                                      |
| mul_19_246        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_37_245(484)                                                                                                                                              |
| getitem_19_247    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_19(262144)                                                                                                                                               |
| eq_19_248         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_19_247(484)                                                                                                                                             |
| maskedfill_19_249 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_19_246(484) eq_19_248(121)                                                                                                                                  |
| softmax_19_250    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_19_249(484)                                                                                                                                          |
| dropout_23_251    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.2.sa.heads.2.dropout |      484 |      484 | softmax_19_250(484)                                                                                                                                             |
| linear_63_252     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.2.sa.heads.2.value   |    16896 |     2112 | layernorm_5_217(16896)                                                                                                                                          |
| matmul_38_253     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.2.sa.heads.2         |     2596 |     2112 | dropout_23_251(484) linear_63_252(2112)                                                                                                                         |
| linear_64_254     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.2.sa.heads.3.key     |    16896 |     2112 | layernorm_5_217(16896)                                                                                                                                          |
| linear_65_255     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.2.sa.heads.3.query   |    16896 |     2112 | layernorm_5_217(16896)                                                                                                                                          |
| transpose_20_256  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_64_254(2112)                                                                                                                                             |
| matmul_39_257     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_65_255(2112) transpose_20_256(2112)                                                                                                                      |
| mul_20_258        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_39_257(484)                                                                                                                                              |
| getitem_20_259    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_20(262144)                                                                                                                                               |
| eq_20_260         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_20_259(484)                                                                                                                                             |
| maskedfill_20_261 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_20_258(484) eq_20_260(121)                                                                                                                                  |
| softmax_20_262    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_20_261(484)                                                                                                                                          |
| dropout_24_263    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.2.sa.heads.3.dropout |      484 |      484 | softmax_20_262(484)                                                                                                                                             |
| linear_66_264     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.2.sa.heads.3.value   |    16896 |     2112 | layernorm_5_217(16896)                                                                                                                                          |
| matmul_40_265     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.2.sa.heads.3         |     2596 |     2112 | dropout_24_263(484) linear_66_264(2112)                                                                                                                         |
| linear_67_266     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.2.sa.heads.4.key     |    16896 |     2112 | layernorm_5_217(16896)                                                                                                                                          |
| linear_68_267     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.2.sa.heads.4.query   |    16896 |     2112 | layernorm_5_217(16896)                                                                                                                                          |
| transpose_21_268  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_67_266(2112)                                                                                                                                             |
| matmul_41_269     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_68_267(2112) transpose_21_268(2112)                                                                                                                      |
| mul_21_270        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_41_269(484)                                                                                                                                              |
| getitem_21_271    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_21(262144)                                                                                                                                               |
| eq_21_272         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_21_271(484)                                                                                                                                             |
| maskedfill_21_273 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_21_270(484) eq_21_272(121)                                                                                                                                  |
| softmax_21_274    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_21_273(484)                                                                                                                                          |
| dropout_25_275    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.2.sa.heads.4.dropout |      484 |      484 | softmax_21_274(484)                                                                                                                                             |
| linear_69_276     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.2.sa.heads.4.value   |    16896 |     2112 | layernorm_5_217(16896)                                                                                                                                          |
| matmul_42_277     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.2.sa.heads.4         |     2596 |     2112 | dropout_25_275(484) linear_69_276(2112)                                                                                                                         |
| linear_70_278     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.2.sa.heads.5.key     |    16896 |     2112 | layernorm_5_217(16896)                                                                                                                                          |
| linear_71_279     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.2.sa.heads.5.query   |    16896 |     2112 | layernorm_5_217(16896)                                                                                                                                          |
| transpose_22_280  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_70_278(2112)                                                                                                                                             |
| matmul_43_281     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_71_279(2112) transpose_22_280(2112)                                                                                                                      |
| mul_22_282        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_43_281(484)                                                                                                                                              |
| getitem_22_283    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_22(262144)                                                                                                                                               |
| eq_22_284         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_22_283(484)                                                                                                                                             |
| maskedfill_22_285 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_22_282(484) eq_22_284(121)                                                                                                                                  |
| softmax_22_286    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_22_285(484)                                                                                                                                          |
| dropout_26_287    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.2.sa.heads.5.dropout |      484 |      484 | softmax_22_286(484)                                                                                                                                             |
| linear_72_288     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.2.sa.heads.5.value   |    16896 |     2112 | layernorm_5_217(16896)                                                                                                                                          |
| matmul_44_289     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.2.sa.heads.5         |     2596 |     2112 | dropout_26_287(484) linear_72_288(2112)                                                                                                                         |
| linear_73_290     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.2.sa.heads.6.key     |    16896 |     2112 | layernorm_5_217(16896)                                                                                                                                          |
| linear_74_291     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.2.sa.heads.6.query   |    16896 |     2112 | layernorm_5_217(16896)                                                                                                                                          |
| transpose_23_292  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_73_290(2112)                                                                                                                                             |
| matmul_45_293     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_74_291(2112) transpose_23_292(2112)                                                                                                                      |
| mul_23_294        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_45_293(484)                                                                                                                                              |
| getitem_23_295    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_23(262144)                                                                                                                                               |
| eq_23_296         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_23_295(484)                                                                                                                                             |
| maskedfill_23_297 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_23_294(484) eq_23_296(121)                                                                                                                                  |
| softmax_23_298    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_23_297(484)                                                                                                                                          |
| dropout_27_299    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.2.sa.heads.6.dropout |      484 |      484 | softmax_23_298(484)                                                                                                                                             |
| linear_75_300     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.2.sa.heads.6.value   |    16896 |     2112 | layernorm_5_217(16896)                                                                                                                                          |
| matmul_46_301     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.2.sa.heads.6         |     2596 |     2112 | dropout_27_299(484) linear_75_300(2112)                                                                                                                         |
| linear_76_302     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.2.sa.heads.7.key     |    16896 |     2112 | layernorm_5_217(16896)                                                                                                                                          |
| linear_77_303     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.2.sa.heads.7.query   |    16896 |     2112 | layernorm_5_217(16896)                                                                                                                                          |
| transpose_24_304  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_76_302(2112)                                                                                                                                             |
| matmul_47_305     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_77_303(2112) transpose_24_304(2112)                                                                                                                      |
| mul_24_306        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_47_305(484)                                                                                                                                              |
| getitem_24_307    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_24(262144)                                                                                                                                               |
| eq_24_308         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_24_307(484)                                                                                                                                             |
| maskedfill_24_309 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_24_306(484) eq_24_308(121)                                                                                                                                  |
| softmax_24_310    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_24_309(484)                                                                                                                                          |
| dropout_28_311    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.2.sa.heads.7.dropout |      484 |      484 | softmax_24_310(484)                                                                                                                                             |
| linear_78_312     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.2.sa.heads.7.value   |    16896 |     2112 | layernorm_5_217(16896)                                                                                                                                          |
| matmul_48_313     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.2.sa.heads.7         |     2596 |     2112 | dropout_28_311(484) linear_78_312(2112)                                                                                                                         |
| cat_3_314         | 1x11x384       |        16896 |         17040 |                 16960 |                         |                             |    16896 |    16896 | matmul_34_229(2112) matmul_36_241(2112) matmul_38_253(2112) matmul_40_265(2112) matmul_42_277(2112) matmul_44_289(2112) matmul_46_301(2112) matmul_48_313(2112) |
| linear_79_315     | 1x11x384       |        16896 |         17040 |                 16960 | params: 384x384, x384   | blocks.2.sa.proj            |    16896 |    16896 | cat_3_314(16896)                                                                                                                                                |
| dropout_29_316    | 1x11x384       |        16896 |         17040 |                 16960 |                         | blocks.2.sa.dropout         |    16896 |    16896 | linear_79_315(16896)                                                                                                                                            |
| add_4_317         | 1x11x384       |        16896 |         17040 |                 16960 |                         |                             |    33792 |    16896 | add_3_211:2(16896) dropout_29_316(16896)                                                                                                                        |
| layernorm_6_318   | 1x11x384       |        16896 |         17040 |                 16960 | params: x384, x384      | blocks.2.ln2                |    16896 |    16896 | add_4_317:1(16896)                                                                                                                                              |
| linear_80_319     | 1x11x1536      |        67584 |         67728 |                 67648 | params: 1536x384, x1536 | blocks.2.ffwd.net.0         |    16896 |    67584 | layernorm_6_318(16896)                                                                                                                                          |
| relu_3_320        | 1x11x1536      |        67584 |         67728 |                 67648 |                         | blocks.2.ffwd.net.1         |    67584 |    67584 | linear_80_319(67584)                                                                                                                                            |
| linear_81_321     | 1x11x384       |        16896 |         17040 |                 16960 | params: 384x1536, x384  | blocks.2.ffwd.net.2         |    67584 |    16896 | relu_3_320(67584)                                                                                                                                               |
| dropout_30_322    | 1x11x384       |        16896 |         17040 |                 16960 |                         | blocks.2.ffwd.net.3         |    16896 |    16896 | linear_81_321(16896)                                                                                                                                            |
| add_4_317         | 1x11x384       |        16896 |         17040 |                 16960 |                         |                             |    33792 |    16896 | add_4_317:1(16896) dropout_30_322(16896)                                                                                                                        |
| layernorm_7_323   | 1x11x384       |        16896 |         17040 |                 16960 | params: x384, x384      | blocks.3.ln1                |    16896 |    16896 | add_4_317:2(16896)                                                                                                                                              |
| linear_82_324     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.3.sa.heads.0.key     |    16896 |     2112 | layernorm_7_323(16896)                                                                                                                                          |
| linear_83_325     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.3.sa.heads.0.query   |    16896 |     2112 | layernorm_7_323(16896)                                                                                                                                          |
| transpose_25_326  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_82_324(2112)                                                                                                                                             |
| matmul_49_327     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_83_325(2112) transpose_25_326(2112)                                                                                                                      |
| mul_25_328        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_49_327(484)                                                                                                                                              |
| getitem_25_329    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_25(262144)                                                                                                                                               |
| eq_25_330         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_25_329(484)                                                                                                                                             |
| maskedfill_25_331 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_25_328(484) eq_25_330(121)                                                                                                                                  |
| softmax_25_332    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_25_331(484)                                                                                                                                          |
| dropout_31_333    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.3.sa.heads.0.dropout |      484 |      484 | softmax_25_332(484)                                                                                                                                             |
| linear_84_334     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.3.sa.heads.0.value   |    16896 |     2112 | layernorm_7_323(16896)                                                                                                                                          |
| matmul_50_335     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.3.sa.heads.0         |     2596 |     2112 | dropout_31_333(484) linear_84_334(2112)                                                                                                                         |
| linear_85_336     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.3.sa.heads.1.key     |    16896 |     2112 | layernorm_7_323(16896)                                                                                                                                          |
| linear_86_337     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.3.sa.heads.1.query   |    16896 |     2112 | layernorm_7_323(16896)                                                                                                                                          |
| transpose_26_338  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_85_336(2112)                                                                                                                                             |
| matmul_51_339     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_86_337(2112) transpose_26_338(2112)                                                                                                                      |
| mul_26_340        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_51_339(484)                                                                                                                                              |
| getitem_26_341    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_26(262144)                                                                                                                                               |
| eq_26_342         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_26_341(484)                                                                                                                                             |
| maskedfill_26_343 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_26_340(484) eq_26_342(121)                                                                                                                                  |
| softmax_26_344    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_26_343(484)                                                                                                                                          |
| dropout_32_345    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.3.sa.heads.1.dropout |      484 |      484 | softmax_26_344(484)                                                                                                                                             |
| linear_87_346     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.3.sa.heads.1.value   |    16896 |     2112 | layernorm_7_323(16896)                                                                                                                                          |
| matmul_52_347     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.3.sa.heads.1         |     2596 |     2112 | dropout_32_345(484) linear_87_346(2112)                                                                                                                         |
| linear_88_348     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.3.sa.heads.2.key     |    16896 |     2112 | layernorm_7_323(16896)                                                                                                                                          |
| linear_89_349     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.3.sa.heads.2.query   |    16896 |     2112 | layernorm_7_323(16896)                                                                                                                                          |
| transpose_27_350  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_88_348(2112)                                                                                                                                             |
| matmul_53_351     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_89_349(2112) transpose_27_350(2112)                                                                                                                      |
| mul_27_352        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_53_351(484)                                                                                                                                              |
| getitem_27_353    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_27(262144)                                                                                                                                               |
| eq_27_354         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_27_353(484)                                                                                                                                             |
| maskedfill_27_355 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_27_352(484) eq_27_354(121)                                                                                                                                  |
| softmax_27_356    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_27_355(484)                                                                                                                                          |
| dropout_33_357    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.3.sa.heads.2.dropout |      484 |      484 | softmax_27_356(484)                                                                                                                                             |
| linear_90_358     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.3.sa.heads.2.value   |    16896 |     2112 | layernorm_7_323(16896)                                                                                                                                          |
| matmul_54_359     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.3.sa.heads.2         |     2596 |     2112 | dropout_33_357(484) linear_90_358(2112)                                                                                                                         |
| linear_91_360     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.3.sa.heads.3.key     |    16896 |     2112 | layernorm_7_323(16896)                                                                                                                                          |
| linear_92_361     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.3.sa.heads.3.query   |    16896 |     2112 | layernorm_7_323(16896)                                                                                                                                          |
| transpose_28_362  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_91_360(2112)                                                                                                                                             |
| matmul_55_363     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_92_361(2112) transpose_28_362(2112)                                                                                                                      |
| mul_28_364        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_55_363(484)                                                                                                                                              |
| getitem_28_365    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_28(262144)                                                                                                                                               |
| eq_28_366         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_28_365(484)                                                                                                                                             |
| maskedfill_28_367 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_28_364(484) eq_28_366(121)                                                                                                                                  |
| softmax_28_368    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_28_367(484)                                                                                                                                          |
| dropout_34_369    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.3.sa.heads.3.dropout |      484 |      484 | softmax_28_368(484)                                                                                                                                             |
| linear_93_370     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.3.sa.heads.3.value   |    16896 |     2112 | layernorm_7_323(16896)                                                                                                                                          |
| matmul_56_371     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.3.sa.heads.3         |     2596 |     2112 | dropout_34_369(484) linear_93_370(2112)                                                                                                                         |
| linear_94_372     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.3.sa.heads.4.key     |    16896 |     2112 | layernorm_7_323(16896)                                                                                                                                          |
| linear_95_373     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.3.sa.heads.4.query   |    16896 |     2112 | layernorm_7_323(16896)                                                                                                                                          |
| transpose_29_374  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_94_372(2112)                                                                                                                                             |
| matmul_57_375     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_95_373(2112) transpose_29_374(2112)                                                                                                                      |
| mul_29_376        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_57_375(484)                                                                                                                                              |
| getitem_29_377    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_29(262144)                                                                                                                                               |
| eq_29_378         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_29_377(484)                                                                                                                                             |
| maskedfill_29_379 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_29_376(484) eq_29_378(121)                                                                                                                                  |
| softmax_29_380    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_29_379(484)                                                                                                                                          |
| dropout_35_381    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.3.sa.heads.4.dropout |      484 |      484 | softmax_29_380(484)                                                                                                                                             |
| linear_96_382     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.3.sa.heads.4.value   |    16896 |     2112 | layernorm_7_323(16896)                                                                                                                                          |
| matmul_58_383     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.3.sa.heads.4         |     2596 |     2112 | dropout_35_381(484) linear_96_382(2112)                                                                                                                         |
| linear_97_384     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.3.sa.heads.5.key     |    16896 |     2112 | layernorm_7_323(16896)                                                                                                                                          |
| linear_98_385     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.3.sa.heads.5.query   |    16896 |     2112 | layernorm_7_323(16896)                                                                                                                                          |
| transpose_30_386  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_97_384(2112)                                                                                                                                             |
| matmul_59_387     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_98_385(2112) transpose_30_386(2112)                                                                                                                      |
| mul_30_388        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_59_387(484)                                                                                                                                              |
| getitem_30_389    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_30(262144)                                                                                                                                               |
| eq_30_390         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_30_389(484)                                                                                                                                             |
| maskedfill_30_391 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_30_388(484) eq_30_390(121)                                                                                                                                  |
| softmax_30_392    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_30_391(484)                                                                                                                                          |
| dropout_36_393    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.3.sa.heads.5.dropout |      484 |      484 | softmax_30_392(484)                                                                                                                                             |
| linear_99_394     | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.3.sa.heads.5.value   |    16896 |     2112 | layernorm_7_323(16896)                                                                                                                                          |
| matmul_60_395     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.3.sa.heads.5         |     2596 |     2112 | dropout_36_393(484) linear_99_394(2112)                                                                                                                         |
| linear_100_396    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.3.sa.heads.6.key     |    16896 |     2112 | layernorm_7_323(16896)                                                                                                                                          |
| linear_101_397    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.3.sa.heads.6.query   |    16896 |     2112 | layernorm_7_323(16896)                                                                                                                                          |
| transpose_31_398  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_100_396(2112)                                                                                                                                            |
| matmul_61_399     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_101_397(2112) transpose_31_398(2112)                                                                                                                     |
| mul_31_400        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_61_399(484)                                                                                                                                              |
| getitem_31_401    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_31(262144)                                                                                                                                               |
| eq_31_402         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_31_401(484)                                                                                                                                             |
| maskedfill_31_403 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_31_400(484) eq_31_402(121)                                                                                                                                  |
| softmax_31_404    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_31_403(484)                                                                                                                                          |
| dropout_37_405    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.3.sa.heads.6.dropout |      484 |      484 | softmax_31_404(484)                                                                                                                                             |
| linear_102_406    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.3.sa.heads.6.value   |    16896 |     2112 | layernorm_7_323(16896)                                                                                                                                          |
| matmul_62_407     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.3.sa.heads.6         |     2596 |     2112 | dropout_37_405(484) linear_102_406(2112)                                                                                                                        |
| linear_103_408    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.3.sa.heads.7.key     |    16896 |     2112 | layernorm_7_323(16896)                                                                                                                                          |
| linear_104_409    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.3.sa.heads.7.query   |    16896 |     2112 | layernorm_7_323(16896)                                                                                                                                          |
| transpose_32_410  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_103_408(2112)                                                                                                                                            |
| matmul_63_411     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_104_409(2112) transpose_32_410(2112)                                                                                                                     |
| mul_32_412        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_63_411(484)                                                                                                                                              |
| getitem_32_413    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_32(262144)                                                                                                                                               |
| eq_32_414         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_32_413(484)                                                                                                                                             |
| maskedfill_32_415 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_32_412(484) eq_32_414(121)                                                                                                                                  |
| softmax_32_416    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_32_415(484)                                                                                                                                          |
| dropout_38_417    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.3.sa.heads.7.dropout |      484 |      484 | softmax_32_416(484)                                                                                                                                             |
| linear_105_418    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.3.sa.heads.7.value   |    16896 |     2112 | layernorm_7_323(16896)                                                                                                                                          |
| matmul_64_419     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.3.sa.heads.7         |     2596 |     2112 | dropout_38_417(484) linear_105_418(2112)                                                                                                                        |
| cat_4_420         | 1x11x384       |        16896 |         17040 |                 16960 |                         |                             |    16896 |    16896 | matmul_50_335(2112) matmul_52_347(2112) matmul_54_359(2112) matmul_56_371(2112) matmul_58_383(2112) matmul_60_395(2112) matmul_62_407(2112) matmul_64_419(2112) |
| linear_106_421    | 1x11x384       |        16896 |         17040 |                 16960 | params: 384x384, x384   | blocks.3.sa.proj            |    16896 |    16896 | cat_4_420(16896)                                                                                                                                                |
| dropout_39_422    | 1x11x384       |        16896 |         17040 |                 16960 |                         | blocks.3.sa.dropout         |    16896 |    16896 | linear_106_421(16896)                                                                                                                                           |
| add_5_423         | 1x11x384       |        16896 |         17040 |                 16960 |                         |                             |    33792 |    16896 | add_4_317:2(16896) dropout_39_422(16896)                                                                                                                        |
| layernorm_8_424   | 1x11x384       |        16896 |         17040 |                 16960 | params: x384, x384      | blocks.3.ln2                |    16896 |    16896 | add_5_423:1(16896)                                                                                                                                              |
| linear_107_425    | 1x11x1536      |        67584 |         67728 |                 67648 | params: 1536x384, x1536 | blocks.3.ffwd.net.0         |    16896 |    67584 | layernorm_8_424(16896)                                                                                                                                          |
| relu_4_426        | 1x11x1536      |        67584 |         67728 |                 67648 |                         | blocks.3.ffwd.net.1         |    67584 |    67584 | linear_107_425(67584)                                                                                                                                           |
| linear_108_427    | 1x11x384       |        16896 |         17040 |                 16960 | params: 384x1536, x384  | blocks.3.ffwd.net.2         |    67584 |    16896 | relu_4_426(67584)                                                                                                                                               |
| dropout_40_428    | 1x11x384       |        16896 |         17040 |                 16960 |                         | blocks.3.ffwd.net.3         |    16896 |    16896 | linear_108_427(16896)                                                                                                                                           |
| add_5_423         | 1x11x384       |        16896 |         17040 |                 16960 |                         |                             |    33792 |    16896 | add_5_423:1(16896) dropout_40_428(16896)                                                                                                                        |
| layernorm_9_429   | 1x11x384       |        16896 |         17040 |                 16960 | params: x384, x384      | blocks.4.ln1                |    16896 |    16896 | add_5_423:2(16896)                                                                                                                                              |
| linear_109_430    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.4.sa.heads.0.key     |    16896 |     2112 | layernorm_9_429(16896)                                                                                                                                          |
| linear_110_431    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.4.sa.heads.0.query   |    16896 |     2112 | layernorm_9_429(16896)                                                                                                                                          |
| transpose_33_432  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_109_430(2112)                                                                                                                                            |
| matmul_65_433     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_110_431(2112) transpose_33_432(2112)                                                                                                                     |
| mul_33_434        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_65_433(484)                                                                                                                                              |
| getitem_33_435    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_33(262144)                                                                                                                                               |
| eq_33_436         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_33_435(484)                                                                                                                                             |
| maskedfill_33_437 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_33_434(484) eq_33_436(121)                                                                                                                                  |
| softmax_33_438    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_33_437(484)                                                                                                                                          |
| dropout_41_439    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.4.sa.heads.0.dropout |      484 |      484 | softmax_33_438(484)                                                                                                                                             |
| linear_111_440    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.4.sa.heads.0.value   |    16896 |     2112 | layernorm_9_429(16896)                                                                                                                                          |
| matmul_66_441     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.4.sa.heads.0         |     2596 |     2112 | dropout_41_439(484) linear_111_440(2112)                                                                                                                        |
| linear_112_442    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.4.sa.heads.1.key     |    16896 |     2112 | layernorm_9_429(16896)                                                                                                                                          |
| linear_113_443    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.4.sa.heads.1.query   |    16896 |     2112 | layernorm_9_429(16896)                                                                                                                                          |
| transpose_34_444  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_112_442(2112)                                                                                                                                            |
| matmul_67_445     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_113_443(2112) transpose_34_444(2112)                                                                                                                     |
| mul_34_446        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_67_445(484)                                                                                                                                              |
| getitem_34_447    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_34(262144)                                                                                                                                               |
| eq_34_448         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_34_447(484)                                                                                                                                             |
| maskedfill_34_449 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_34_446(484) eq_34_448(121)                                                                                                                                  |
| softmax_34_450    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_34_449(484)                                                                                                                                          |
| dropout_42_451    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.4.sa.heads.1.dropout |      484 |      484 | softmax_34_450(484)                                                                                                                                             |
| linear_114_452    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.4.sa.heads.1.value   |    16896 |     2112 | layernorm_9_429(16896)                                                                                                                                          |
| matmul_68_453     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.4.sa.heads.1         |     2596 |     2112 | dropout_42_451(484) linear_114_452(2112)                                                                                                                        |
| linear_115_454    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.4.sa.heads.2.key     |    16896 |     2112 | layernorm_9_429(16896)                                                                                                                                          |
| linear_116_455    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.4.sa.heads.2.query   |    16896 |     2112 | layernorm_9_429(16896)                                                                                                                                          |
| transpose_35_456  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_115_454(2112)                                                                                                                                            |
| matmul_69_457     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_116_455(2112) transpose_35_456(2112)                                                                                                                     |
| mul_35_458        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_69_457(484)                                                                                                                                              |
| getitem_35_459    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_35(262144)                                                                                                                                               |
| eq_35_460         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_35_459(484)                                                                                                                                             |
| maskedfill_35_461 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_35_458(484) eq_35_460(121)                                                                                                                                  |
| softmax_35_462    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_35_461(484)                                                                                                                                          |
| dropout_43_463    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.4.sa.heads.2.dropout |      484 |      484 | softmax_35_462(484)                                                                                                                                             |
| linear_117_464    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.4.sa.heads.2.value   |    16896 |     2112 | layernorm_9_429(16896)                                                                                                                                          |
| matmul_70_465     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.4.sa.heads.2         |     2596 |     2112 | dropout_43_463(484) linear_117_464(2112)                                                                                                                        |
| linear_118_466    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.4.sa.heads.3.key     |    16896 |     2112 | layernorm_9_429(16896)                                                                                                                                          |
| linear_119_467    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.4.sa.heads.3.query   |    16896 |     2112 | layernorm_9_429(16896)                                                                                                                                          |
| transpose_36_468  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_118_466(2112)                                                                                                                                            |
| matmul_71_469     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_119_467(2112) transpose_36_468(2112)                                                                                                                     |
| mul_36_470        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_71_469(484)                                                                                                                                              |
| getitem_36_471    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_36(262144)                                                                                                                                               |
| eq_36_472         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_36_471(484)                                                                                                                                             |
| maskedfill_36_473 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_36_470(484) eq_36_472(121)                                                                                                                                  |
| softmax_36_474    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_36_473(484)                                                                                                                                          |
| dropout_44_475    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.4.sa.heads.3.dropout |      484 |      484 | softmax_36_474(484)                                                                                                                                             |
| linear_120_476    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.4.sa.heads.3.value   |    16896 |     2112 | layernorm_9_429(16896)                                                                                                                                          |
| matmul_72_477     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.4.sa.heads.3         |     2596 |     2112 | dropout_44_475(484) linear_120_476(2112)                                                                                                                        |
| linear_121_478    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.4.sa.heads.4.key     |    16896 |     2112 | layernorm_9_429(16896)                                                                                                                                          |
| linear_122_479    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.4.sa.heads.4.query   |    16896 |     2112 | layernorm_9_429(16896)                                                                                                                                          |
| transpose_37_480  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_121_478(2112)                                                                                                                                            |
| matmul_73_481     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_122_479(2112) transpose_37_480(2112)                                                                                                                     |
| mul_37_482        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_73_481(484)                                                                                                                                              |
| getitem_37_483    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_37(262144)                                                                                                                                               |
| eq_37_484         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_37_483(484)                                                                                                                                             |
| maskedfill_37_485 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_37_482(484) eq_37_484(121)                                                                                                                                  |
| softmax_37_486    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_37_485(484)                                                                                                                                          |
| dropout_45_487    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.4.sa.heads.4.dropout |      484 |      484 | softmax_37_486(484)                                                                                                                                             |
| linear_123_488    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.4.sa.heads.4.value   |    16896 |     2112 | layernorm_9_429(16896)                                                                                                                                          |
| matmul_74_489     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.4.sa.heads.4         |     2596 |     2112 | dropout_45_487(484) linear_123_488(2112)                                                                                                                        |
| linear_124_490    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.4.sa.heads.5.key     |    16896 |     2112 | layernorm_9_429(16896)                                                                                                                                          |
| linear_125_491    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.4.sa.heads.5.query   |    16896 |     2112 | layernorm_9_429(16896)                                                                                                                                          |
| transpose_38_492  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_124_490(2112)                                                                                                                                            |
| matmul_75_493     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_125_491(2112) transpose_38_492(2112)                                                                                                                     |
| mul_38_494        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_75_493(484)                                                                                                                                              |
| getitem_38_495    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_38(262144)                                                                                                                                               |
| eq_38_496         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_38_495(484)                                                                                                                                             |
| maskedfill_38_497 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_38_494(484) eq_38_496(121)                                                                                                                                  |
| softmax_38_498    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_38_497(484)                                                                                                                                          |
| dropout_46_499    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.4.sa.heads.5.dropout |      484 |      484 | softmax_38_498(484)                                                                                                                                             |
| linear_126_500    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.4.sa.heads.5.value   |    16896 |     2112 | layernorm_9_429(16896)                                                                                                                                          |
| matmul_76_501     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.4.sa.heads.5         |     2596 |     2112 | dropout_46_499(484) linear_126_500(2112)                                                                                                                        |
| linear_127_502    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.4.sa.heads.6.key     |    16896 |     2112 | layernorm_9_429(16896)                                                                                                                                          |
| linear_128_503    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.4.sa.heads.6.query   |    16896 |     2112 | layernorm_9_429(16896)                                                                                                                                          |
| transpose_39_504  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_127_502(2112)                                                                                                                                            |
| matmul_77_505     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_128_503(2112) transpose_39_504(2112)                                                                                                                     |
| mul_39_506        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_77_505(484)                                                                                                                                              |
| getitem_39_507    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_39(262144)                                                                                                                                               |
| eq_39_508         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_39_507(484)                                                                                                                                             |
| maskedfill_39_509 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_39_506(484) eq_39_508(121)                                                                                                                                  |
| softmax_39_510    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_39_509(484)                                                                                                                                          |
| dropout_47_511    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.4.sa.heads.6.dropout |      484 |      484 | softmax_39_510(484)                                                                                                                                             |
| linear_129_512    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.4.sa.heads.6.value   |    16896 |     2112 | layernorm_9_429(16896)                                                                                                                                          |
| matmul_78_513     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.4.sa.heads.6         |     2596 |     2112 | dropout_47_511(484) linear_129_512(2112)                                                                                                                        |
| linear_130_514    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.4.sa.heads.7.key     |    16896 |     2112 | layernorm_9_429(16896)                                                                                                                                          |
| linear_131_515    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.4.sa.heads.7.query   |    16896 |     2112 | layernorm_9_429(16896)                                                                                                                                          |
| transpose_40_516  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_130_514(2112)                                                                                                                                            |
| matmul_79_517     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_131_515(2112) transpose_40_516(2112)                                                                                                                     |
| mul_40_518        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_79_517(484)                                                                                                                                              |
| getitem_40_519    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_40(262144)                                                                                                                                               |
| eq_40_520         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_40_519(484)                                                                                                                                             |
| maskedfill_40_521 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_40_518(484) eq_40_520(121)                                                                                                                                  |
| softmax_40_522    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_40_521(484)                                                                                                                                          |
| dropout_48_523    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.4.sa.heads.7.dropout |      484 |      484 | softmax_40_522(484)                                                                                                                                             |
| linear_132_524    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.4.sa.heads.7.value   |    16896 |     2112 | layernorm_9_429(16896)                                                                                                                                          |
| matmul_80_525     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.4.sa.heads.7         |     2596 |     2112 | dropout_48_523(484) linear_132_524(2112)                                                                                                                        |
| cat_5_526         | 1x11x384       |        16896 |         17040 |                 16960 |                         |                             |    16896 |    16896 | matmul_66_441(2112) matmul_68_453(2112) matmul_70_465(2112) matmul_72_477(2112) matmul_74_489(2112) matmul_76_501(2112) matmul_78_513(2112) matmul_80_525(2112) |
| linear_133_527    | 1x11x384       |        16896 |         17040 |                 16960 | params: 384x384, x384   | blocks.4.sa.proj            |    16896 |    16896 | cat_5_526(16896)                                                                                                                                                |
| dropout_49_528    | 1x11x384       |        16896 |         17040 |                 16960 |                         | blocks.4.sa.dropout         |    16896 |    16896 | linear_133_527(16896)                                                                                                                                           |
| add_6_529         | 1x11x384       |        16896 |         17040 |                 16960 |                         |                             |    33792 |    16896 | add_5_423:2(16896) dropout_49_528(16896)                                                                                                                        |
| layernorm_10_530  | 1x11x384       |        16896 |         17040 |                 16960 | params: x384, x384      | blocks.4.ln2                |    16896 |    16896 | add_6_529:1(16896)                                                                                                                                              |
| linear_134_531    | 1x11x1536      |        67584 |         67728 |                 67648 | params: 1536x384, x1536 | blocks.4.ffwd.net.0         |    16896 |    67584 | layernorm_10_530(16896)                                                                                                                                         |
| relu_5_532        | 1x11x1536      |        67584 |         67728 |                 67648 |                         | blocks.4.ffwd.net.1         |    67584 |    67584 | linear_134_531(67584)                                                                                                                                           |
| linear_135_533    | 1x11x384       |        16896 |         17040 |                 16960 | params: 384x1536, x384  | blocks.4.ffwd.net.2         |    67584 |    16896 | relu_5_532(67584)                                                                                                                                               |
| dropout_50_534    | 1x11x384       |        16896 |         17040 |                 16960 |                         | blocks.4.ffwd.net.3         |    16896 |    16896 | linear_135_533(16896)                                                                                                                                           |
| add_6_529         | 1x11x384       |        16896 |         17040 |                 16960 |                         |                             |    33792 |    16896 | add_6_529:1(16896) dropout_50_534(16896)                                                                                                                        |
| layernorm_11_535  | 1x11x384       |        16896 |         17040 |                 16960 | params: x384, x384      | blocks.5.ln1                |    16896 |    16896 | add_6_529:2(16896)                                                                                                                                              |
| linear_136_536    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.5.sa.heads.0.key     |    16896 |     2112 | layernorm_11_535(16896)                                                                                                                                         |
| linear_137_537    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.5.sa.heads.0.query   |    16896 |     2112 | layernorm_11_535(16896)                                                                                                                                         |
| transpose_41_538  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_136_536(2112)                                                                                                                                            |
| matmul_81_539     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_137_537(2112) transpose_41_538(2112)                                                                                                                     |
| mul_41_540        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_81_539(484)                                                                                                                                              |
| getitem_41_541    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_41(262144)                                                                                                                                               |
| eq_41_542         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_41_541(484)                                                                                                                                             |
| maskedfill_41_543 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_41_540(484) eq_41_542(121)                                                                                                                                  |
| softmax_41_544    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_41_543(484)                                                                                                                                          |
| dropout_51_545    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.5.sa.heads.0.dropout |      484 |      484 | softmax_41_544(484)                                                                                                                                             |
| linear_138_546    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.5.sa.heads.0.value   |    16896 |     2112 | layernorm_11_535(16896)                                                                                                                                         |
| matmul_82_547     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.5.sa.heads.0         |     2596 |     2112 | dropout_51_545(484) linear_138_546(2112)                                                                                                                        |
| linear_139_548    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.5.sa.heads.1.key     |    16896 |     2112 | layernorm_11_535(16896)                                                                                                                                         |
| linear_140_549    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.5.sa.heads.1.query   |    16896 |     2112 | layernorm_11_535(16896)                                                                                                                                         |
| transpose_42_550  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_139_548(2112)                                                                                                                                            |
| matmul_83_551     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_140_549(2112) transpose_42_550(2112)                                                                                                                     |
| mul_42_552        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_83_551(484)                                                                                                                                              |
| getitem_42_553    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_42(262144)                                                                                                                                               |
| eq_42_554         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_42_553(484)                                                                                                                                             |
| maskedfill_42_555 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_42_552(484) eq_42_554(121)                                                                                                                                  |
| softmax_42_556    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_42_555(484)                                                                                                                                          |
| dropout_52_557    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.5.sa.heads.1.dropout |      484 |      484 | softmax_42_556(484)                                                                                                                                             |
| linear_141_558    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.5.sa.heads.1.value   |    16896 |     2112 | layernorm_11_535(16896)                                                                                                                                         |
| matmul_84_559     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.5.sa.heads.1         |     2596 |     2112 | dropout_52_557(484) linear_141_558(2112)                                                                                                                        |
| linear_142_560    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.5.sa.heads.2.key     |    16896 |     2112 | layernorm_11_535(16896)                                                                                                                                         |
| linear_143_561    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.5.sa.heads.2.query   |    16896 |     2112 | layernorm_11_535(16896)                                                                                                                                         |
| transpose_43_562  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_142_560(2112)                                                                                                                                            |
| matmul_85_563     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_143_561(2112) transpose_43_562(2112)                                                                                                                     |
| mul_43_564        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_85_563(484)                                                                                                                                              |
| getitem_43_565    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_43(262144)                                                                                                                                               |
| eq_43_566         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_43_565(484)                                                                                                                                             |
| maskedfill_43_567 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_43_564(484) eq_43_566(121)                                                                                                                                  |
| softmax_43_568    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_43_567(484)                                                                                                                                          |
| dropout_53_569    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.5.sa.heads.2.dropout |      484 |      484 | softmax_43_568(484)                                                                                                                                             |
| linear_144_570    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.5.sa.heads.2.value   |    16896 |     2112 | layernorm_11_535(16896)                                                                                                                                         |
| matmul_86_571     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.5.sa.heads.2         |     2596 |     2112 | dropout_53_569(484) linear_144_570(2112)                                                                                                                        |
| linear_145_572    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.5.sa.heads.3.key     |    16896 |     2112 | layernorm_11_535(16896)                                                                                                                                         |
| linear_146_573    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.5.sa.heads.3.query   |    16896 |     2112 | layernorm_11_535(16896)                                                                                                                                         |
| transpose_44_574  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_145_572(2112)                                                                                                                                            |
| matmul_87_575     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_146_573(2112) transpose_44_574(2112)                                                                                                                     |
| mul_44_576        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_87_575(484)                                                                                                                                              |
| getitem_44_577    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_44(262144)                                                                                                                                               |
| eq_44_578         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_44_577(484)                                                                                                                                             |
| maskedfill_44_579 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_44_576(484) eq_44_578(121)                                                                                                                                  |
| softmax_44_580    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_44_579(484)                                                                                                                                          |
| dropout_54_581    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.5.sa.heads.3.dropout |      484 |      484 | softmax_44_580(484)                                                                                                                                             |
| linear_147_582    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.5.sa.heads.3.value   |    16896 |     2112 | layernorm_11_535(16896)                                                                                                                                         |
| matmul_88_583     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.5.sa.heads.3         |     2596 |     2112 | dropout_54_581(484) linear_147_582(2112)                                                                                                                        |
| linear_148_584    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.5.sa.heads.4.key     |    16896 |     2112 | layernorm_11_535(16896)                                                                                                                                         |
| linear_149_585    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.5.sa.heads.4.query   |    16896 |     2112 | layernorm_11_535(16896)                                                                                                                                         |
| transpose_45_586  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_148_584(2112)                                                                                                                                            |
| matmul_89_587     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_149_585(2112) transpose_45_586(2112)                                                                                                                     |
| mul_45_588        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_89_587(484)                                                                                                                                              |
| getitem_45_589    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_45(262144)                                                                                                                                               |
| eq_45_590         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_45_589(484)                                                                                                                                             |
| maskedfill_45_591 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_45_588(484) eq_45_590(121)                                                                                                                                  |
| softmax_45_592    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_45_591(484)                                                                                                                                          |
| dropout_55_593    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.5.sa.heads.4.dropout |      484 |      484 | softmax_45_592(484)                                                                                                                                             |
| linear_150_594    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.5.sa.heads.4.value   |    16896 |     2112 | layernorm_11_535(16896)                                                                                                                                         |
| matmul_90_595     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.5.sa.heads.4         |     2596 |     2112 | dropout_55_593(484) linear_150_594(2112)                                                                                                                        |
| linear_151_596    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.5.sa.heads.5.key     |    16896 |     2112 | layernorm_11_535(16896)                                                                                                                                         |
| linear_152_597    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.5.sa.heads.5.query   |    16896 |     2112 | layernorm_11_535(16896)                                                                                                                                         |
| transpose_46_598  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_151_596(2112)                                                                                                                                            |
| matmul_91_599     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_152_597(2112) transpose_46_598(2112)                                                                                                                     |
| mul_46_600        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_91_599(484)                                                                                                                                              |
| getitem_46_601    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_46(262144)                                                                                                                                               |
| eq_46_602         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_46_601(484)                                                                                                                                             |
| maskedfill_46_603 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_46_600(484) eq_46_602(121)                                                                                                                                  |
| softmax_46_604    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_46_603(484)                                                                                                                                          |
| dropout_56_605    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.5.sa.heads.5.dropout |      484 |      484 | softmax_46_604(484)                                                                                                                                             |
| linear_153_606    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.5.sa.heads.5.value   |    16896 |     2112 | layernorm_11_535(16896)                                                                                                                                         |
| matmul_92_607     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.5.sa.heads.5         |     2596 |     2112 | dropout_56_605(484) linear_153_606(2112)                                                                                                                        |
| linear_154_608    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.5.sa.heads.6.key     |    16896 |     2112 | layernorm_11_535(16896)                                                                                                                                         |
| linear_155_609    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.5.sa.heads.6.query   |    16896 |     2112 | layernorm_11_535(16896)                                                                                                                                         |
| transpose_47_610  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_154_608(2112)                                                                                                                                            |
| matmul_93_611     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_155_609(2112) transpose_47_610(2112)                                                                                                                     |
| mul_47_612        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_93_611(484)                                                                                                                                              |
| getitem_47_613    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_47(262144)                                                                                                                                               |
| eq_47_614         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_47_613(484)                                                                                                                                             |
| maskedfill_47_615 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_47_612(484) eq_47_614(121)                                                                                                                                  |
| softmax_47_616    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_47_615(484)                                                                                                                                          |
| dropout_57_617    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.5.sa.heads.6.dropout |      484 |      484 | softmax_47_616(484)                                                                                                                                             |
| linear_156_618    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.5.sa.heads.6.value   |    16896 |     2112 | layernorm_11_535(16896)                                                                                                                                         |
| matmul_94_619     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.5.sa.heads.6         |     2596 |     2112 | dropout_57_617(484) linear_156_618(2112)                                                                                                                        |
| linear_157_620    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.5.sa.heads.7.key     |    16896 |     2112 | layernorm_11_535(16896)                                                                                                                                         |
| linear_158_621    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.5.sa.heads.7.query   |    16896 |     2112 | layernorm_11_535(16896)                                                                                                                                         |
| transpose_48_622  | 1x48x11        |         2112 |          2256 |                  2176 |                         |                             |     2112 |     2112 | linear_157_620(2112)                                                                                                                                            |
| matmul_95_623     | 1x11x11        |          484 |           628 |                   548 |                         |                             |     4224 |      484 | linear_158_621(2112) transpose_48_622(2112)                                                                                                                     |
| mul_48_624        | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | matmul_95_623(484)                                                                                                                                              |
| getitem_48_625    | 11x11          |          484 |           612 |                262208 |                         |                             |   262144 |      484 | buffer_48(262144)                                                                                                                                               |
| eq_48_626         | 11x11          |          121 |           249 |                   185 |                         |                             |      484 |      121 | getitem_48_625(484)                                                                                                                                             |
| maskedfill_48_627 | 1x11x11        |          484 |           628 |                   548 |                         |                             |      605 |      484 | mul_48_624(484) eq_48_626(121)                                                                                                                                  |
| softmax_48_628    | 1x11x11        |          484 |           628 |                   548 |                         |                             |      484 |      484 | maskedfill_48_627(484)                                                                                                                                          |
| dropout_58_629    | 1x11x11        |          484 |           628 |                   548 |                         | blocks.5.sa.heads.7.dropout |      484 |      484 | softmax_48_628(484)                                                                                                                                             |
| linear_159_630    | 1x11x48        |         2112 |          2256 |                  2176 | params: 48x384          | blocks.5.sa.heads.7.value   |    16896 |     2112 | layernorm_11_535(16896)                                                                                                                                         |
| matmul_96_631     | 1x11x48        |         2112 |          2256 |                  2176 |                         | blocks.5.sa.heads.7         |     2596 |     2112 | dropout_58_629(484) linear_159_630(2112)                                                                                                                        |
| cat_6_632         | 1x11x384       |        16896 |         17040 |                 16960 |                         |                             |    16896 |    16896 | matmul_82_547(2112) matmul_84_559(2112) matmul_86_571(2112) matmul_88_583(2112) matmul_90_595(2112) matmul_92_607(2112) matmul_94_619(2112) matmul_96_631(2112) |
| linear_160_633    | 1x11x384       |        16896 |         17040 |                 16960 | params: 384x384, x384   | blocks.5.sa.proj            |    16896 |    16896 | cat_6_632(16896)                                                                                                                                                |
| dropout_59_634    | 1x11x384       |        16896 |         17040 |                 16960 |                         | blocks.5.sa.dropout         |    16896 |    16896 | linear_160_633(16896)                                                                                                                                           |
| add_7_635         | 1x11x384       |        16896 |         17040 |                 16960 |                         |                             |    33792 |    16896 | add_6_529:2(16896) dropout_59_634(16896)                                                                                                                        |
| layernorm_12_636  | 1x11x384       |        16896 |         17040 |                 16960 | params: x384, x384      | blocks.5.ln2                |    16896 |    16896 | add_7_635:1(16896)                                                                                                                                              |
| linear_161_637    | 1x11x1536      |        67584 |         67728 |                 67648 | params: 1536x384, x1536 | blocks.5.ffwd.net.0         |    16896 |    67584 | layernorm_12_636(16896)                                                                                                                                         |
| relu_6_638        | 1x11x1536      |        67584 |         67728 |                 67648 |                         | blocks.5.ffwd.net.1         |    67584 |    67584 | linear_161_637(67584)                                                                                                                                           |
| linear_162_639    | 1x11x384       |        16896 |         17040 |                 16960 | params: 384x1536, x384  | blocks.5.ffwd.net.2         |    67584 |    16896 | relu_6_638(67584)                                                                                                                                               |
| dropout_60_640    | 1x11x384       |        16896 |         17040 |                 16960 |                         | blocks.5.ffwd.net.3         |    16896 |    16896 | linear_162_639(16896)                                                                                                                                           |
| add_7_635         | 1x11x384       |        16896 |         17040 |                 16960 |                         |                             |    33792 |    16896 | add_7_635:1(16896) dropout_60_640(16896)                                                                                                                        |
| layernorm_13_641  | 1x11x384       |        16896 |         17040 |                 16960 | params: x384, x384      | ln_f                        |    16896 |    16896 | add_7_635:2(16896)                                                                                                                                              |
| linear_163_642    | 1x11x65        |         2860 |          3004 |                  2924 | params: 65x384, x65     | lm_head                     |    16896 |     2860 | layernorm_13_641(16896)                                                                                                                                         |
| output_1          | 1x11x65        |         2860 |          3004 |                  2924 |                         | output.0                    |     2860 |     2860 | linear_163_642(2860)                                                                                                                                            |


	Module Hierarchy:
		token_embedding_table
		position_embedding_table
		blocks
		    blocks.0
		        blocks.0.ln1
		        blocks.0.sa
		            blocks.0.sa.heads.0
		                blocks.0.sa.heads.0.key, blocks.0.sa.heads.0.query, blocks.0.sa.heads.0.dropout, blocks.0.sa.heads.0.value
		            blocks.0.sa.heads.1
		                blocks.0.sa.heads.1.key, blocks.0.sa.heads.1.query, blocks.0.sa.heads.1.dropout, blocks.0.sa.heads.1.value
		            blocks.0.sa.heads.2
		                blocks.0.sa.heads.2.key, blocks.0.sa.heads.2.query, blocks.0.sa.heads.2.dropout, blocks.0.sa.heads.2.value
		            blocks.0.sa.heads.3
		                blocks.0.sa.heads.3.key, blocks.0.sa.heads.3.query, blocks.0.sa.heads.3.dropout, blocks.0.sa.heads.3.value
		            blocks.0.sa.heads.4
		                blocks.0.sa.heads.4.key, blocks.0.sa.heads.4.query, blocks.0.sa.heads.4.dropout, blocks.0.sa.heads.4.value
		            blocks.0.sa.heads.5
		                blocks.0.sa.heads.5.key, blocks.0.sa.heads.5.query, blocks.0.sa.heads.5.dropout, blocks.0.sa.heads.5.value
		            blocks.0.sa.heads.6
		                blocks.0.sa.heads.6.key, blocks.0.sa.heads.6.query, blocks.0.sa.heads.6.dropout, blocks.0.sa.heads.6.value
		            blocks.0.sa.heads.7
		                blocks.0.sa.heads.7.key, blocks.0.sa.heads.7.query, blocks.0.sa.heads.7.dropout, blocks.0.sa.heads.7.value
		            blocks.0.sa.proj
		            blocks.0.sa.dropout
		        blocks.0.ln2
		        blocks.0.ffwd
		            blocks.0.ffwd.net
		                blocks.0.ffwd.net.0, blocks.0.ffwd.net.1, blocks.0.ffwd.net.2, blocks.0.ffwd.net.3
		    blocks.1
		        blocks.1.ln1
		        blocks.1.sa
		            blocks.1.sa.heads.0
		                blocks.1.sa.heads.0.key, blocks.1.sa.heads.0.query, blocks.1.sa.heads.0.dropout, blocks.1.sa.heads.0.value
		            blocks.1.sa.heads.1
		                blocks.1.sa.heads.1.key, blocks.1.sa.heads.1.query, blocks.1.sa.heads.1.dropout, blocks.1.sa.heads.1.value
		            blocks.1.sa.heads.2
		                blocks.1.sa.heads.2.key, blocks.1.sa.heads.2.query, blocks.1.sa.heads.2.dropout, blocks.1.sa.heads.2.value
		            blocks.1.sa.heads.3
		                blocks.1.sa.heads.3.key, blocks.1.sa.heads.3.query, blocks.1.sa.heads.3.dropout, blocks.1.sa.heads.3.value
		            blocks.1.sa.heads.4
		                blocks.1.sa.heads.4.key, blocks.1.sa.heads.4.query, blocks.1.sa.heads.4.dropout, blocks.1.sa.heads.4.value
		            blocks.1.sa.heads.5
		                blocks.1.sa.heads.5.key, blocks.1.sa.heads.5.query, blocks.1.sa.heads.5.dropout, blocks.1.sa.heads.5.value
		            blocks.1.sa.heads.6
		                blocks.1.sa.heads.6.key, blocks.1.sa.heads.6.query, blocks.1.sa.heads.6.dropout, blocks.1.sa.heads.6.value
		            blocks.1.sa.heads.7
		                blocks.1.sa.heads.7.key, blocks.1.sa.heads.7.query, blocks.1.sa.heads.7.dropout, blocks.1.sa.heads.7.value
		            blocks.1.sa.proj
		            blocks.1.sa.dropout
		        blocks.1.ln2
		        blocks.1.ffwd
		            blocks.1.ffwd.net
		                blocks.1.ffwd.net.0, blocks.1.ffwd.net.1, blocks.1.ffwd.net.2, blocks.1.ffwd.net.3
		    blocks.2
		        blocks.2.ln1
		        blocks.2.sa
		            blocks.2.sa.heads.0
		                blocks.2.sa.heads.0.key, blocks.2.sa.heads.0.query, blocks.2.sa.heads.0.dropout, blocks.2.sa.heads.0.value
		            blocks.2.sa.heads.1
		                blocks.2.sa.heads.1.key, blocks.2.sa.heads.1.query, blocks.2.sa.heads.1.dropout, blocks.2.sa.heads.1.value
		            blocks.2.sa.heads.2
		                blocks.2.sa.heads.2.key, blocks.2.sa.heads.2.query, blocks.2.sa.heads.2.dropout, blocks.2.sa.heads.2.value
		            blocks.2.sa.heads.3
		                blocks.2.sa.heads.3.key, blocks.2.sa.heads.3.query, blocks.2.sa.heads.3.dropout, blocks.2.sa.heads.3.value
		            blocks.2.sa.heads.4
		                blocks.2.sa.heads.4.key, blocks.2.sa.heads.4.query, blocks.2.sa.heads.4.dropout, blocks.2.sa.heads.4.value
		            blocks.2.sa.heads.5
		                blocks.2.sa.heads.5.key, blocks.2.sa.heads.5.query, blocks.2.sa.heads.5.dropout, blocks.2.sa.heads.5.value
		            blocks.2.sa.heads.6
		                blocks.2.sa.heads.6.key, blocks.2.sa.heads.6.query, blocks.2.sa.heads.6.dropout, blocks.2.sa.heads.6.value
		            blocks.2.sa.heads.7
		                blocks.2.sa.heads.7.key, blocks.2.sa.heads.7.query, blocks.2.sa.heads.7.dropout, blocks.2.sa.heads.7.value
		            blocks.2.sa.proj
		            blocks.2.sa.dropout
		        blocks.2.ln2
		        blocks.2.ffwd
		            blocks.2.ffwd.net
		                blocks.2.ffwd.net.0, blocks.2.ffwd.net.1, blocks.2.ffwd.net.2, blocks.2.ffwd.net.3
		    blocks.3
		        blocks.3.ln1
		        blocks.3.sa
		            blocks.3.sa.heads.0
		                blocks.3.sa.heads.0.key, blocks.3.sa.heads.0.query, blocks.3.sa.heads.0.dropout, blocks.3.sa.heads.0.value
		            blocks.3.sa.heads.1
		                blocks.3.sa.heads.1.key, blocks.3.sa.heads.1.query, blocks.3.sa.heads.1.dropout, blocks.3.sa.heads.1.value
		            blocks.3.sa.heads.2
		                blocks.3.sa.heads.2.key, blocks.3.sa.heads.2.query, blocks.3.sa.heads.2.dropout, blocks.3.sa.heads.2.value
		            blocks.3.sa.heads.3
		                blocks.3.sa.heads.3.key, blocks.3.sa.heads.3.query, blocks.3.sa.heads.3.dropout, blocks.3.sa.heads.3.value
		            blocks.3.sa.heads.4
		                blocks.3.sa.heads.4.key, blocks.3.sa.heads.4.query, blocks.3.sa.heads.4.dropout, blocks.3.sa.heads.4.value
		            blocks.3.sa.heads.5
		                blocks.3.sa.heads.5.key, blocks.3.sa.heads.5.query, blocks.3.sa.heads.5.dropout, blocks.3.sa.heads.5.value
		            blocks.3.sa.heads.6
		                blocks.3.sa.heads.6.key, blocks.3.sa.heads.6.query, blocks.3.sa.heads.6.dropout, blocks.3.sa.heads.6.value
		            blocks.3.sa.heads.7
		                blocks.3.sa.heads.7.key, blocks.3.sa.heads.7.query, blocks.3.sa.heads.7.dropout, blocks.3.sa.heads.7.value
		            blocks.3.sa.proj
		            blocks.3.sa.dropout
		        blocks.3.ln2
		        blocks.3.ffwd
		            blocks.3.ffwd.net
		                blocks.3.ffwd.net.0, blocks.3.ffwd.net.1, blocks.3.ffwd.net.2, blocks.3.ffwd.net.3
		    blocks.4
		        blocks.4.ln1
		        blocks.4.sa
		            blocks.4.sa.heads.0
		                blocks.4.sa.heads.0.key, blocks.4.sa.heads.0.query, blocks.4.sa.heads.0.dropout, blocks.4.sa.heads.0.value
		            blocks.4.sa.heads.1
		                blocks.4.sa.heads.1.key, blocks.4.sa.heads.1.query, blocks.4.sa.heads.1.dropout, blocks.4.sa.heads.1.value
		            blocks.4.sa.heads.2
		                blocks.4.sa.heads.2.key, blocks.4.sa.heads.2.query, blocks.4.sa.heads.2.dropout, blocks.4.sa.heads.2.value
		            blocks.4.sa.heads.3
		                blocks.4.sa.heads.3.key, blocks.4.sa.heads.3.query, blocks.4.sa.heads.3.dropout, blocks.4.sa.heads.3.value
		            blocks.4.sa.heads.4
		                blocks.4.sa.heads.4.key, blocks.4.sa.heads.4.query, blocks.4.sa.heads.4.dropout, blocks.4.sa.heads.4.value
		            blocks.4.sa.heads.5
		                blocks.4.sa.heads.5.key, blocks.4.sa.heads.5.query, blocks.4.sa.heads.5.dropout, blocks.4.sa.heads.5.value
		            blocks.4.sa.heads.6
		                blocks.4.sa.heads.6.key, blocks.4.sa.heads.6.query, blocks.4.sa.heads.6.dropout, blocks.4.sa.heads.6.value
		            blocks.4.sa.heads.7
		                blocks.4.sa.heads.7.key, blocks.4.sa.heads.7.query, blocks.4.sa.heads.7.dropout, blocks.4.sa.heads.7.value
		            blocks.4.sa.proj
		            blocks.4.sa.dropout
		        blocks.4.ln2
		        blocks.4.ffwd
		            blocks.4.ffwd.net
		                blocks.4.ffwd.net.0, blocks.4.ffwd.net.1, blocks.4.ffwd.net.2, blocks.4.ffwd.net.3
		    blocks.5
		        blocks.5.ln1
		        blocks.5.sa
		            blocks.5.sa.heads.0
		                blocks.5.sa.heads.0.key, blocks.5.sa.heads.0.query, blocks.5.sa.heads.0.dropout, blocks.5.sa.heads.0.value
		            blocks.5.sa.heads.1
		                blocks.5.sa.heads.1.key, blocks.5.sa.heads.1.query, blocks.5.sa.heads.1.dropout, blocks.5.sa.heads.1.value
		            blocks.5.sa.heads.2
		                blocks.5.sa.heads.2.key, blocks.5.sa.heads.2.query, blocks.5.sa.heads.2.dropout, blocks.5.sa.heads.2.value
		            blocks.5.sa.heads.3
		                blocks.5.sa.heads.3.key, blocks.5.sa.heads.3.query, blocks.5.sa.heads.3.dropout, blocks.5.sa.heads.3.value
		            blocks.5.sa.heads.4
		                blocks.5.sa.heads.4.key, blocks.5.sa.heads.4.query, blocks.5.sa.heads.4.dropout, blocks.5.sa.heads.4.value
		            blocks.5.sa.heads.5
		                blocks.5.sa.heads.5.key, blocks.5.sa.heads.5.query, blocks.5.sa.heads.5.dropout, blocks.5.sa.heads.5.value
		            blocks.5.sa.heads.6
		                blocks.5.sa.heads.6.key, blocks.5.sa.heads.6.query, blocks.5.sa.heads.6.dropout, blocks.5.sa.heads.6.value
		            blocks.5.sa.heads.7
		                blocks.5.sa.heads.7.key, blocks.5.sa.heads.7.query, blocks.5.sa.heads.7.dropout, blocks.5.sa.heads.7.value
		            blocks.5.sa.proj
		            blocks.5.sa.dropout
		        blocks.5.ln2
		        blocks.5.ffwd
		            blocks.5.ffwd.net
		                blocks.5.ffwd.net.0, blocks.5.ffwd.net.1, blocks.5.ffwd.net.2, blocks.5.ffwd.net.3
		ln_f
		lm_head


	Layers (all have saved activations):
		  (0) input_1 
		  (1) embedding_1_1 
		  (2) arange_1_2 
		  (3) embedding_2_3 
		  (4) add_1_4 
		  (5) layernorm_1_5 
		  (6) linear_1_6 
		  (7) linear_2_7 
		  (8) transpose_1_8 
		  (9) matmul_1_9 
		  (10) mul_1_10 
		  (11) buffer_1 
		  (12) getitem_1_11 
		  (13) eq_1_12 
		  (14) maskedfill_1_13 
		  (15) softmax_1_14 
		  (16) dropout_1_15 
		  (17) linear_3_16 
		  (18) matmul_2_17 
		  (19) linear_4_18 
		  (20) linear_5_19 
		  (21) transpose_2_20 
		  (22) matmul_3_21 
		  (23) mul_2_22 
		  (24) buffer_2 
		  (25) getitem_2_23 
		  (26) eq_2_24 
		  (27) maskedfill_2_25 
		  (28) softmax_2_26 
		  (29) dropout_2_27 
		  (30) linear_6_28 
		  (31) matmul_4_29 
		  (32) linear_7_30 
		  (33) linear_8_31 
		  (34) transpose_3_32 
		  (35) matmul_5_33 
		  (36) mul_3_34 
		  (37) buffer_3 
		  (38) getitem_3_35 
		  (39) eq_3_36 
		  (40) maskedfill_3_37 
		  (41) softmax_3_38 
		  (42) dropout_3_39 
		  (43) linear_9_40 
		  (44) matmul_6_41 
		  (45) linear_10_42 
		  (46) linear_11_43 
		  (47) transpose_4_44 
		  (48) matmul_7_45 
		  (49) mul_4_46 
		  (50) buffer_4 
		  (51) getitem_4_47 
		  (52) eq_4_48 
		  (53) maskedfill_4_49 
		  (54) softmax_4_50 
		  (55) dropout_4_51 
		  (56) linear_12_52 
		  (57) matmul_8_53 
		  (58) linear_13_54 
		  (59) linear_14_55 
		  (60) transpose_5_56 
		  (61) matmul_9_57 
		  (62) mul_5_58 
		  (63) buffer_5 
		  (64) getitem_5_59 
		  (65) eq_5_60 
		  (66) maskedfill_5_61 
		  (67) softmax_5_62 
		  (68) dropout_5_63 
		  (69) linear_15_64 
		  (70) matmul_10_65 
		  (71) linear_16_66 
		  (72) linear_17_67 
		  (73) transpose_6_68 
		  (74) matmul_11_69 
		  (75) mul_6_70 
		  (76) buffer_6 
		  (77) getitem_6_71 
		  (78) eq_6_72 
		  (79) maskedfill_6_73 
		  (80) softmax_6_74 
		  (81) dropout_6_75 
		  (82) linear_18_76 
		  (83) matmul_12_77 
		  (84) linear_19_78 
		  (85) linear_20_79 
		  (86) transpose_7_80 
		  (87) matmul_13_81 
		  (88) mul_7_82 
		  (89) buffer_7 
		  (90) getitem_7_83 
		  (91) eq_7_84 
		  (92) maskedfill_7_85 
		  (93) softmax_7_86 
		  (94) dropout_7_87 
		  (95) linear_21_88 
		  (96) matmul_14_89 
		  (97) linear_22_90 
		  (98) linear_23_91 
		  (99) transpose_8_92 
		  (100) matmul_15_93 
		  (101) mul_8_94 
		  (102) buffer_8 
		  (103) getitem_8_95 
		  (104) eq_8_96 
		  (105) maskedfill_8_97 
		  (106) softmax_8_98 
		  (107) dropout_8_99 
		  (108) linear_24_100 
		  (109) matmul_16_101 
		  (110) cat_1_102 
		  (111) linear_25_103 
		  (112) dropout_9_104 
		  (113) add_2_105:1  (1/2 passes)
		  (114) layernorm_2_106 
		  (115) linear_26_107 
		  (116) relu_1_108 
		  (117) linear_27_109 
		  (118) dropout_10_110 
		  (119) add_2_105:2  (2/2 passes)
		  (120) layernorm_3_111 
		  (121) linear_28_112 
		  (122) linear_29_113 
		  (123) transpose_9_114 
		  (124) matmul_17_115 
		  (125) mul_9_116 
		  (126) buffer_9 
		  (127) getitem_9_117 
		  (128) eq_9_118 
		  (129) maskedfill_9_119 
		  (130) softmax_9_120 
		  (131) dropout_11_121 
		  (132) linear_30_122 
		  (133) matmul_18_123 
		  (134) linear_31_124 
		  (135) linear_32_125 
		  (136) transpose_10_126 
		  (137) matmul_19_127 
		  (138) mul_10_128 
		  (139) buffer_10 
		  (140) getitem_10_129 
		  (141) eq_10_130 
		  (142) maskedfill_10_131 
		  (143) softmax_10_132 
		  (144) dropout_12_133 
		  (145) linear_33_134 
		  (146) matmul_20_135 
		  (147) linear_34_136 
		  (148) linear_35_137 
		  (149) transpose_11_138 
		  (150) matmul_21_139 
		  (151) mul_11_140 
		  (152) buffer_11 
		  (153) getitem_11_141 
		  (154) eq_11_142 
		  (155) maskedfill_11_143 
		  (156) softmax_11_144 
		  (157) dropout_13_145 
		  (158) linear_36_146 
		  (159) matmul_22_147 
		  (160) linear_37_148 
		  (161) linear_38_149 
		  (162) transpose_12_150 
		  (163) matmul_23_151 
		  (164) mul_12_152 
		  (165) buffer_12 
		  (166) getitem_12_153 
		  (167) eq_12_154 
		  (168) maskedfill_12_155 
		  (169) softmax_12_156 
		  (170) dropout_14_157 
		  (171) linear_39_158 
		  (172) matmul_24_159 
		  (173) linear_40_160 
		  (174) linear_41_161 
		  (175) transpose_13_162 
		  (176) matmul_25_163 
		  (177) mul_13_164 
		  (178) buffer_13 
		  (179) getitem_13_165 
		  (180) eq_13_166 
		  (181) maskedfill_13_167 
		  (182) softmax_13_168 
		  (183) dropout_15_169 
		  (184) linear_42_170 
		  (185) matmul_26_171 
		  (186) linear_43_172 
		  (187) linear_44_173 
		  (188) transpose_14_174 
		  (189) matmul_27_175 
		  (190) mul_14_176 
		  (191) buffer_14 
		  (192) getitem_14_177 
		  (193) eq_14_178 
		  (194) maskedfill_14_179 
		  (195) softmax_14_180 
		  (196) dropout_16_181 
		  (197) linear_45_182 
		  (198) matmul_28_183 
		  (199) linear_46_184 
		  (200) linear_47_185 
		  (201) transpose_15_186 
		  (202) matmul_29_187 
		  (203) mul_15_188 
		  (204) buffer_15 
		  (205) getitem_15_189 
		  (206) eq_15_190 
		  (207) maskedfill_15_191 
		  (208) softmax_15_192 
		  (209) dropout_17_193 
		  (210) linear_48_194 
		  (211) matmul_30_195 
		  (212) linear_49_196 
		  (213) linear_50_197 
		  (214) transpose_16_198 
		  (215) matmul_31_199 
		  (216) mul_16_200 
		  (217) buffer_16 
		  (218) getitem_16_201 
		  (219) eq_16_202 
		  (220) maskedfill_16_203 
		  (221) softmax_16_204 
		  (222) dropout_18_205 
		  (223) linear_51_206 
		  (224) matmul_32_207 
		  (225) cat_2_208 
		  (226) linear_52_209 
		  (227) dropout_19_210 
		  (228) add_3_211:1  (1/2 passes)
		  (229) layernorm_4_212 
		  (230) linear_53_213 
		  (231) relu_2_214 
		  (232) linear_54_215 
		  (233) dropout_20_216 
		  (234) add_3_211:2  (2/2 passes)
		  (235) layernorm_5_217 
		  (236) linear_55_218 
		  (237) linear_56_219 
		  (238) transpose_17_220 
		  (239) matmul_33_221 
		  (240) mul_17_222 
		  (241) buffer_17 
		  (242) getitem_17_223 
		  (243) eq_17_224 
		  (244) maskedfill_17_225 
		  (245) softmax_17_226 
		  (246) dropout_21_227 
		  (247) linear_57_228 
		  (248) matmul_34_229 
		  (249) linear_58_230 
		  (250) linear_59_231 
		  (251) transpose_18_232 
		  (252) matmul_35_233 
		  (253) mul_18_234 
		  (254) buffer_18 
		  (255) getitem_18_235 
		  (256) eq_18_236 
		  (257) maskedfill_18_237 
		  (258) softmax_18_238 
		  (259) dropout_22_239 
		  (260) linear_60_240 
		  (261) matmul_36_241 
		  (262) linear_61_242 
		  (263) linear_62_243 
		  (264) transpose_19_244 
		  (265) matmul_37_245 
		  (266) mul_19_246 
		  (267) buffer_19 
		  (268) getitem_19_247 
		  (269) eq_19_248 
		  (270) maskedfill_19_249 
		  (271) softmax_19_250 
		  (272) dropout_23_251 
		  (273) linear_63_252 
		  (274) matmul_38_253 
		  (275) linear_64_254 
		  (276) linear_65_255 
		  (277) transpose_20_256 
		  (278) matmul_39_257 
		  (279) mul_20_258 
		  (280) buffer_20 
		  (281) getitem_20_259 
		  (282) eq_20_260 
		  (283) maskedfill_20_261 
		  (284) softmax_20_262 
		  (285) dropout_24_263 
		  (286) linear_66_264 
		  (287) matmul_40_265 
		  (288) linear_67_266 
		  (289) linear_68_267 
		  (290) transpose_21_268 
		  (291) matmul_41_269 
		  (292) mul_21_270 
		  (293) buffer_21 
		  (294) getitem_21_271 
		  (295) eq_21_272 
		  (296) maskedfill_21_273 
		  (297) softmax_21_274 
		  (298) dropout_25_275 
		  (299) linear_69_276 
		  (300) matmul_42_277 
		  (301) linear_70_278 
		  (302) linear_71_279 
		  (303) transpose_22_280 
		  (304) matmul_43_281 
		  (305) mul_22_282 
		  (306) buffer_22 
		  (307) getitem_22_283 
		  (308) eq_22_284 
		  (309) maskedfill_22_285 
		  (310) softmax_22_286 
		  (311) dropout_26_287 
		  (312) linear_72_288 
		  (313) matmul_44_289 
		  (314) linear_73_290 
		  (315) linear_74_291 
		  (316) transpose_23_292 
		  (317) matmul_45_293 
		  (318) mul_23_294 
		  (319) buffer_23 
		  (320) getitem_23_295 
		  (321) eq_23_296 
		  (322) maskedfill_23_297 
		  (323) softmax_23_298 
		  (324) dropout_27_299 
		  (325) linear_75_300 
		  (326) matmul_46_301 
		  (327) linear_76_302 
		  (328) linear_77_303 
		  (329) transpose_24_304 
		  (330) matmul_47_305 
		  (331) mul_24_306 
		  (332) buffer_24 
		  (333) getitem_24_307 
		  (334) eq_24_308 
		  (335) maskedfill_24_309 
		  (336) softmax_24_310 
		  (337) dropout_28_311 
		  (338) linear_78_312 
		  (339) matmul_48_313 
		  (340) cat_3_314 
		  (341) linear_79_315 
		  (342) dropout_29_316 
		  (343) add_4_317:1  (1/2 passes)
		  (344) layernorm_6_318 
		  (345) linear_80_319 
		  (346) relu_3_320 
		  (347) linear_81_321 
		  (348) dropout_30_322 
		  (349) add_4_317:2  (2/2 passes)
		  (350) layernorm_7_323 
		  (351) linear_82_324 
		  (352) linear_83_325 
		  (353) transpose_25_326 
		  (354) matmul_49_327 
		  (355) mul_25_328 
		  (356) buffer_25 
		  (357) getitem_25_329 
		  (358) eq_25_330 
		  (359) maskedfill_25_331 
		  (360) softmax_25_332 
		  (361) dropout_31_333 
		  (362) linear_84_334 
		  (363) matmul_50_335 
		  (364) linear_85_336 
		  (365) linear_86_337 
		  (366) transpose_26_338 
		  (367) matmul_51_339 
		  (368) mul_26_340 
		  (369) buffer_26 
		  (370) getitem_26_341 
		  (371) eq_26_342 
		  (372) maskedfill_26_343 
		  (373) softmax_26_344 
		  (374) dropout_32_345 
		  (375) linear_87_346 
		  (376) matmul_52_347 
		  (377) linear_88_348 
		  (378) linear_89_349 
		  (379) transpose_27_350 
		  (380) matmul_53_351 
		  (381) mul_27_352 
		  (382) buffer_27 
		  (383) getitem_27_353 
		  (384) eq_27_354 
		  (385) maskedfill_27_355 
		  (386) softmax_27_356 
		  (387) dropout_33_357 
		  (388) linear_90_358 
		  (389) matmul_54_359 
		  (390) linear_91_360 
		  (391) linear_92_361 
		  (392) transpose_28_362 
		  (393) matmul_55_363 
		  (394) mul_28_364 
		  (395) buffer_28 
		  (396) getitem_28_365 
		  (397) eq_28_366 
		  (398) maskedfill_28_367 
		  (399) softmax_28_368 
		  (400) dropout_34_369 
		  (401) linear_93_370 
		  (402) matmul_56_371 
		  (403) linear_94_372 
		  (404) linear_95_373 
		  (405) transpose_29_374 
		  (406) matmul_57_375 
		  (407) mul_29_376 
		  (408) buffer_29 
		  (409) getitem_29_377 
		  (410) eq_29_378 
		  (411) maskedfill_29_379 
		  (412) softmax_29_380 
		  (413) dropout_35_381 
		  (414) linear_96_382 
		  (415) matmul_58_383 
		  (416) linear_97_384 
		  (417) linear_98_385 
		  (418) transpose_30_386 
		  (419) matmul_59_387 
		  (420) mul_30_388 
		  (421) buffer_30 
		  (422) getitem_30_389 
		  (423) eq_30_390 
		  (424) maskedfill_30_391 
		  (425) softmax_30_392 
		  (426) dropout_36_393 
		  (427) linear_99_394 
		  (428) matmul_60_395 
		  (429) linear_100_396 
		  (430) linear_101_397 
		  (431) transpose_31_398 
		  (432) matmul_61_399 
		  (433) mul_31_400 
		  (434) buffer_31 
		  (435) getitem_31_401 
		  (436) eq_31_402 
		  (437) maskedfill_31_403 
		  (438) softmax_31_404 
		  (439) dropout_37_405 
		  (440) linear_102_406 
		  (441) matmul_62_407 
		  (442) linear_103_408 
		  (443) linear_104_409 
		  (444) transpose_32_410 
		  (445) matmul_63_411 
		  (446) mul_32_412 
		  (447) buffer_32 
		  (448) getitem_32_413 
		  (449) eq_32_414 
		  (450) maskedfill_32_415 
		  (451) softmax_32_416 
		  (452) dropout_38_417 
		  (453) linear_105_418 
		  (454) matmul_64_419 
		  (455) cat_4_420 
		  (456) linear_106_421 
		  (457) dropout_39_422 
		  (458) add_5_423:1  (1/2 passes)
		  (459) layernorm_8_424 
		  (460) linear_107_425 
		  (461) relu_4_426 
		  (462) linear_108_427 
		  (463) dropout_40_428 
		  (464) add_5_423:2  (2/2 passes)
		  (465) layernorm_9_429 
		  (466) linear_109_430 
		  (467) linear_110_431 
		  (468) transpose_33_432 
		  (469) matmul_65_433 
		  (470) mul_33_434 
		  (471) buffer_33 
		  (472) getitem_33_435 
		  (473) eq_33_436 
		  (474) maskedfill_33_437 
		  (475) softmax_33_438 
		  (476) dropout_41_439 
		  (477) linear_111_440 
		  (478) matmul_66_441 
		  (479) linear_112_442 
		  (480) linear_113_443 
		  (481) transpose_34_444 
		  (482) matmul_67_445 
		  (483) mul_34_446 
		  (484) buffer_34 
		  (485) getitem_34_447 
		  (486) eq_34_448 
		  (487) maskedfill_34_449 
		  (488) softmax_34_450 
		  (489) dropout_42_451 
		  (490) linear_114_452 
		  (491) matmul_68_453 
		  (492) linear_115_454 
		  (493) linear_116_455 
		  (494) transpose_35_456 
		  (495) matmul_69_457 
		  (496) mul_35_458 
		  (497) buffer_35 
		  (498) getitem_35_459 
		  (499) eq_35_460 
		  (500) maskedfill_35_461 
		  (501) softmax_35_462 
		  (502) dropout_43_463 
		  (503) linear_117_464 
		  (504) matmul_70_465 
		  (505) linear_118_466 
		  (506) linear_119_467 
		  (507) transpose_36_468 
		  (508) matmul_71_469 
		  (509) mul_36_470 
		  (510) buffer_36 
		  (511) getitem_36_471 
		  (512) eq_36_472 
		  (513) maskedfill_36_473 
		  (514) softmax_36_474 
		  (515) dropout_44_475 
		  (516) linear_120_476 
		  (517) matmul_72_477 
		  (518) linear_121_478 
		  (519) linear_122_479 
		  (520) transpose_37_480 
		  (521) matmul_73_481 
		  (522) mul_37_482 
		  (523) buffer_37 
		  (524) getitem_37_483 
		  (525) eq_37_484 
		  (526) maskedfill_37_485 
		  (527) softmax_37_486 
		  (528) dropout_45_487 
		  (529) linear_123_488 
		  (530) matmul_74_489 
		  (531) linear_124_490 
		  (532) linear_125_491 
		  (533) transpose_38_492 
		  (534) matmul_75_493 
		  (535) mul_38_494 
		  (536) buffer_38 
		  (537) getitem_38_495 
		  (538) eq_38_496 
		  (539) maskedfill_38_497 
		  (540) softmax_38_498 
		  (541) dropout_46_499 
		  (542) linear_126_500 
		  (543) matmul_76_501 
		  (544) linear_127_502 
		  (545) linear_128_503 
		  (546) transpose_39_504 
		  (547) matmul_77_505 
		  (548) mul_39_506 
		  (549) buffer_39 
		  (550) getitem_39_507 
		  (551) eq_39_508 
		  (552) maskedfill_39_509 
		  (553) softmax_39_510 
		  (554) dropout_47_511 
		  (555) linear_129_512 
		  (556) matmul_78_513 
		  (557) linear_130_514 
		  (558) linear_131_515 
		  (559) transpose_40_516 
		  (560) matmul_79_517 
		  (561) mul_40_518 
		  (562) buffer_40 
		  (563) getitem_40_519 
		  (564) eq_40_520 
		  (565) maskedfill_40_521 
		  (566) softmax_40_522 
		  (567) dropout_48_523 
		  (568) linear_132_524 
		  (569) matmul_80_525 
		  (570) cat_5_526 
		  (571) linear_133_527 
		  (572) dropout_49_528 
		  (573) add_6_529:1  (1/2 passes)
		  (574) layernorm_10_530 
		  (575) linear_134_531 
		  (576) relu_5_532 
		  (577) linear_135_533 
		  (578) dropout_50_534 
		  (579) add_6_529:2  (2/2 passes)
		  (580) layernorm_11_535 
		  (581) linear_136_536 
		  (582) linear_137_537 
		  (583) transpose_41_538 
		  (584) matmul_81_539 
		  (585) mul_41_540 
		  (586) buffer_41 
		  (587) getitem_41_541 
		  (588) eq_41_542 
		  (589) maskedfill_41_543 
		  (590) softmax_41_544 
		  (591) dropout_51_545 
		  (592) linear_138_546 
		  (593) matmul_82_547 
		  (594) linear_139_548 
		  (595) linear_140_549 
		  (596) transpose_42_550 
		  (597) matmul_83_551 
		  (598) mul_42_552 
		  (599) buffer_42 
		  (600) getitem_42_553 
		  (601) eq_42_554 
		  (602) maskedfill_42_555 
		  (603) softmax_42_556 
		  (604) dropout_52_557 
		  (605) linear_141_558 
		  (606) matmul_84_559 
		  (607) linear_142_560 
		  (608) linear_143_561 
		  (609) transpose_43_562 
		  (610) matmul_85_563 
		  (611) mul_43_564 
		  (612) buffer_43 
		  (613) getitem_43_565 
		  (614) eq_43_566 
		  (615) maskedfill_43_567 
		  (616) softmax_43_568 
		  (617) dropout_53_569 
		  (618) linear_144_570 
		  (619) matmul_86_571 
		  (620) linear_145_572 
		  (621) linear_146_573 
		  (622) transpose_44_574 
		  (623) matmul_87_575 
		  (624) mul_44_576 
		  (625) buffer_44 
		  (626) getitem_44_577 
		  (627) eq_44_578 
		  (628) maskedfill_44_579 
		  (629) softmax_44_580 
		  (630) dropout_54_581 
		  (631) linear_147_582 
		  (632) matmul_88_583 
		  (633) linear_148_584 
		  (634) linear_149_585 
		  (635) transpose_45_586 
		  (636) matmul_89_587 
		  (637) mul_45_588 
		  (638) buffer_45 
		  (639) getitem_45_589 
		  (640) eq_45_590 
		  (641) maskedfill_45_591 
		  (642) softmax_45_592 
		  (643) dropout_55_593 
		  (644) linear_150_594 
		  (645) matmul_90_595 
		  (646) linear_151_596 
		  (647) linear_152_597 
		  (648) transpose_46_598 
		  (649) matmul_91_599 
		  (650) mul_46_600 
		  (651) buffer_46 
		  (652) getitem_46_601 
		  (653) eq_46_602 
		  (654) maskedfill_46_603 
		  (655) softmax_46_604 
		  (656) dropout_56_605 
		  (657) linear_153_606 
		  (658) matmul_92_607 
		  (659) linear_154_608 
		  (660) linear_155_609 
		  (661) transpose_47_610 
		  (662) matmul_93_611 
		  (663) mul_47_612 
		  (664) buffer_47 
		  (665) getitem_47_613 
		  (666) eq_47_614 
		  (667) maskedfill_47_615 
		  (668) softmax_47_616 
		  (669) dropout_57_617 
		  (670) linear_156_618 
		  (671) matmul_94_619 
		  (672) linear_157_620 
		  (673) linear_158_621 
		  (674) transpose_48_622 
		  (675) matmul_95_623 
		  (676) mul_48_624 
		  (677) buffer_48 
		  (678) getitem_48_625 
		  (679) eq_48_626 
		  (680) maskedfill_48_627 
		  (681) softmax_48_628 
		  (682) dropout_58_629 
		  (683) linear_159_630 
		  (684) matmul_96_631 
		  (685) cat_6_632 
		  (686) linear_160_633 
		  (687) dropout_59_634 
		  (688) add_7_635:1  (1/2 passes)
		  (689) layernorm_12_636 
		  (690) linear_161_637 
		  (691) relu_6_638 
		  (692) linear_162_639 
		  (693) dropout_60_640 
		  (694) add_7_635:2  (2/2 passes)
		  (695) layernorm_13_641 
		  (696) linear_163_642 
		  (697) output_1 
